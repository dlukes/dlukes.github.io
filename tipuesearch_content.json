{"pages":[{"title":"about me","text":"Contact David Lukeš, Personal I'm a phonetician/linguist with side interests in statistics, programming and NLP. In fact, I'm terminally curious about computer-related stuff in general, which has led me down more rabbit holes than I'd care to count. I hail from Prague, Czech Republic, a precious little gem of a city, but I've also lived in France and Belgium for a while when I was a kid. I play badminton and football (of the soccer persuasion) and enjoy reading American Jewish literature, as well as watching (in no particular order) Monty Python, Bill Bailey, Jon Stewart, Ricky Gervais, Stephen Fry and other assorted smart and funny people who, along with Frank Zappa and The Beatles, are collectively responsible for my lifelong crush on the English language. I used to play the saxophone reasonably well, but nowadays mostly enjoy playing the guitar atrociously bad and singing along. Academic and professional credentials Current position I'm a junior researcher at the Institute of the Czech National Corpus , Faculty of Arts , Charles University (Prague) . My duties/interests include: coordinating phonetic transcription for the ORTOFON corpus project data processing and functionality prototyping (mainly using Python and R) research into the scope of language variability in Czech using multi-dimensional analysis For further details and older history, see my CV (updated spring 2017). Code When coding, I feel most comfortable in Python, and I love spending time with Rust (though I'm nowhere near as proficient, and occasionally lost and frustrated). I find R useful (though convoluted on occasion), write JavaScript when forced to and avoid Perl as much as I can. I know my way around the Unix command line and enjoy using it. And even after years of casual acquaintance, I'm still not really sure how I feel about Lisp, though I'll take Emacs Lisp over Vimscript any day. Feel free to check out my GitHub profile . MA thesis If you're interested in the affinities between people's abilities to process language and music, my MA thesis was about Perceptual sensitivity to music and speech stimuli in the frequency and temporal domains and I'm moderately proud of it. Unfortunately, it's in Czech (with an English abstract). Here's a quick summary: I was curious to know whether having a good musical ear correlates with having a good \"speech perception ear\" (spoiler alert: it does), so I put together a listening test (using Praat ) in which I made participants listen to short utterances and melodies to see how well they could detect manipulations in pitch and timing. There's also a companion repository on GitHub which contains the listening test (so that anyone can run it in Praat) and raw results as they were collected (for reproducibility). BA thesis Way back when, my academic interests lay in literary theory, so I wrote a BA thesis on Philip Roth . I still like Philip Roth very much, but I mostly tend to steer away from literary theory nowadays because I realized I'd rather be wrapping my head around logically coherent complex abstract notions.","tags":"pages","url":"pages/about.html"},{"title":"How to mount a Flask app under a URL prefix (or really, any WSGI app)","text":"Intro In a hurry? Skip to the minimal working example below! So this is probably something that seasoned Flask webapp devs will find trivial, especially if they're also well-acquainted with WSGI itself (the Python web server standard that Flask and the other popular synchronous Python web frameworks comply with). But my background is not like that. I've only occasionally used Flask for a project here and there over the years (and I've been really happy with it, since it delivers on its promise of being \"micro\" and keeps out of the way most of the time). I learned it mostly through its own docs. I've been aware that WSGI is a thing, but it never occurred to me I should learn more about it, it always felt like the Flask docs (should) cover everything I need. Which they did, except for the issue mentioned in the title of the article: transparently mounting the app under a URL prefix. Up until yesterday (when I found out basically by sheer luck that this is addressed by the WSGI standard that Flask conforms to), I had no idea what the proper way to do it in Flask was. I never could find any guidance in the Flask docs; as far as I know, I don't think the idiomatic solution I'll be discussing below is covered in there (and I've tried searching them again now that I know what I'm looking for). So I always resorted to clunky workarounds in order to achieve this. I'm fairly sure I'm not the only person like this -- a happy occasional user of Flask whose one major frustration has been having to manually smuggle URL prefixes into his routes. If you have a similar background, then may Google be more clement than it was to me and rank this article on the first page of its results when you hit this particular roadblock ;) If you're a Flask / WSGI whiz, then your reaction will probably be \"But of course this is how it's done!\" So in order to get something out of this article, you can instead muse upon the relative merits and drawbacks of splitting up technical documentation in a way that's well-organized / clearly delineated / methodical / logical (= Flask info in Flask docs, WSGI info in WSGI docs) vs. in a way that's useful and accessible even to occasional users, beginners etc. (= Flask docs might want to cover the parts of WSGI that a budding Flask dev might want to care about, even though that means duplicating information). The problem As you're prototyping and developing a Flask app, routing is pretty easy and straightforward: @app . route ( \"/login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... But when the time comes to deploy it, you often realize you need to mount it under a URL prefix , e.g. /my-app . For the route above, that means that you want to trigger it when users navigate to example.com/my-app/login , not example.com/login . And similarly, you want this prefix added when generating internal redirects or links in your templates. So you think to yourself, this is surely such a common use case that there's definitely an example how to do this directly in Flask's Quickstart . Or failing that, definitely somewhere in the docs. So you search the docs for a reasonable query like \"url prefix\", which sounds like what you're interested in, and you learn about blueprints , which is where stuff starts to become confusing. Blueprints are for making reusable application components and they support registering at a URL prefix and/or a subdomain. They're also a fairly advanced feature useful in more complicated apps, so they sort of go against the grain of your expectations with Flask. You likely picked up Flask because you liked that a simple app can consist of just a single Python module and a few lines of code, but now it looks like if you want to run it in production in a flexible way, you'll need to learn about blueprints and rewrite it as one? Because it sure doesn't look like vanilla Flask apps support URL prefixes, the docs don't mention any promising leads... So you end up deciding to roll your own homegrown solution, which might be configurable but still boilerplate-y if you still have some mental energy left to invest into the problem: PREFIX = \"/my-app\" @app . route ( f \" { PREFIX } /login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... Or it might just be hardcoded to show you really don't care anymore, if you've just lost an hour searching for a Flask feature that would handle this in a civilized manner, and/or reading up on blueprints (before giving up): @app . route ( \"/my-app/login\" , methods = [ \"GET\" , \"POST\" ]) def login_func (): ... If that sounds like you (and it certainly sounds like me), then boy do I have good news for you :) Turns out there actually is a right way to do it! The idiomatic solution The title of the article hints at the reason why the right way to do it is so hard to figure out: it turns out that deploying a Flask app under a URL prefix isn't actually a feature of Flask per se, it's a feature of the Python WSGI standard that Flask conforms to. So that's probably why the way to achieve this isn't described in the Flask docs themselves, because I suppose you're expected to read up on the available WSGI knobs and handles separately...? Frankly, I don't think that's a realistic expectation :) I think this information would make a great addition to the Flask docs, maybe even directly in the Quickstart section. So what's the trick? The trick is to set the rather quirkily/quaintly named SCRIPT_NAME environment variable prior to starting the web server running the app. If you set SCRIPT_NAME=/my-app , WSGI guarantees that the web server running your app will strip this prefix from incoming URLs, and add it to outgoing URLs (redirects, in-app links in templates). That's it, no need to modify your app at all, whether to add a configurable or hardcoded prefix to all the routes, or to rewrite it as a blueprint. Nice. EDIT: As u/james_pic points out over on Reddit , reading SCRIPT_NAME from an env var is actually not required by the WSGI spec, it's a convenience feature provided by some WSGI servers, e.g. Gunicorn . Caveats Because of course, there are a few issues you might run into. Flask's builtin web server ignores SCRIPT_NAME EDIT: As should be clear from the previous edit, Flask's builtin web server only ignores the SCRIPT_NAME env var. It works perfectly well with the SCRIPT_NAME WSGI variable, which however requires quite a bit more work to set up. You need to add WSGI routing middleware to your application, e.g. like this: from werkzeug.middleware.dispatcher import DispatcherMiddleware from werkzeug.wrappers import Response app . wsgi_app = DispatcherMiddleware ( Response ( 'Not Found' , status = 404 ), { '/my-app' : app . wsgi_app } ) Presumably, you'd make the actual value of the prefix part of the app's config. (Code sample courtesy of u/james_pic over on Reddit , thank you!) When you find out about SCRIPT_NAME , your first intuition is to test if it indeed does what you need with Flask's builtin dev server, because, well, that's what you use for development. Unfortunately, it turns out that Flask's server couldn't care less about this env var, which might lead you to (wrongly) conclude that you're on the wrong path. I actually think this happened to me a few years back because I feel like I've tinkered with SCRIPT_NAME before, only to conclude it doesn't seem to do anything. The fix: just use a fully-featured production WSGI server that supports reading this configuration from an environment variable , e.g. Gunicorn . I promise SCRIPT_NAME works there. Alternatively, add some routing middleware of the kind sketched above to your app. Is this one of the reasons why Flask's builtin server warns \"This is a development server. Do not use it in a production deployment\"? I'd always thought it was for performance / stability / security reasons, but it looks like it also just isn't (fully) WSGI-compliant. Use url_for to generate internal links I.e. instead of writing href=\"/login\" in your templates or redirect(\"/login\") in your view functions, write href=\"{{ url_for('login_func') }}\" and redirect(url_for(\"login_func\")) . This will make sure the URLs are correctly prefixed with SCRIPT_NAME , if applicable. This is recommended as best practice by the Flask docs anyway, so it's not like it's additional hoops to jump through. Interestingly, this section in the Flask docs is the closest they brush with discussing mounting the app under a URL prefix. They tell you: If your application is placed outside the URL root, for example, in /myapplication instead of / , url_for() properly handles that for you. But they don't tell you how to place your application outside the URL root in the first place, which is kind of maddening. If you open the link to the url_for() docs , you are further teased with mentions of APPLICATION_ROOT and SERVER_NAME configuration values, which sound a lot like the kind of setting you were looking for, except the docs immediately dash your hopes and remain silent about what you really want, i.e. the SCRIPT_NAME env var: Configuration values APPLICATION_ROOT and SERVER_NAME are only used when generating URLs outside of a request context. I remember tearing my hair out in despair at this point of digging through the Flask docs and trying in vain (of course) to tinker with those configuration values a few years back. EDIT: Again, after reading u/james_pic 's very helpful and insightful comment on Reddit , I've realized the Flask docs technically do contain information which can help you figure this out, in the Application Dispatching section, specifically under Dispatch by Path . The trouble is that the section is introduced by the following sentence: Application dispatching is the process of combining multiple Flask applications on the WSGI level. Which sounds like it's once again a more advanced topic (kind of like blueprints) meant for people who are trying to run multiple apps under the same WSGI server at the same time (which you're not). The code examples are fairly complex to match, so at first sight, it doesn't look like this is what you should be spending your time reading if you just need to figure out something as basic as running your app under a URL prefix in production. Don't strip the prefix in your reverse proxy config If you're configuring your Flask app under a URL prefix, it's probably running behind a reverse proxy of some sort (Nginx, Apache, etc.). Reverse proxies can be configured to strip these prefixes when relaying requests to individual apps, but don't do this ! You want to leave the handling of the prefix to the WSGI server running your app, because the WSGI server has intimate knowledge about the app (it knows it can expect the app to talk WSGI), so it can be smart about how exactly prefix handling should be done. By contrast, the reverse proxy has very little information about your app -- it can rewrite incoming URLs in request headers fairly easily, but that's about it, it can hardly rewrite outgoing URLs in response bodies, that would be non-trivial. That's what sprinkling url_for() s where appropriate throughout your app is for (see previous section). So make sure your reverse proxy leaves the prefix alone. As an example, for Nginx, you don't want this, which would get rid of the /my-app/ prefix: location /my-app/ { proxy_pass http://127.0.0.1:8000/ ; # ... } Instead, you want this: location /my-app/ { proxy_pass http://127.0.0.1:8000/my-app/ ; # ... &#94;&#94;&#94;&#94;&#94;&#94;&#94; } Minimal working example Let's wrap up with a runnable demo. Put this in app.py : from flask import Flask , url_for app = Flask ( __name__ ) @app . route ( \"/\" ) def index (): return url_for ( \"index\" ) Run the app with a WSGI-compliant web server, e.g. Gunicorn (again, not Flask's builtin dev server, see above), while setting the SCRIPT_NAME env var: $ SCRIPT_NAME = /my-app gunicorn app:app Now try accessing / with curl : $ curl localhost:8000/ You should get an internal server error. If you inspect the traceback in the window where the app is running, you'll see this was due to the fact that the expected /my-app prefix wasn't found, so try adding it: $ curl localhost:8000/my-app/ /my-app/ Now everything works, even though your app's source code is clean and blissfully unaware of any prefix. Gunicorn strips away the prefix from the request before passing it on to your app's router, and url_for takes care of adding the prefix back into any internal link generated by your app, as you can see in the output ( /my-app/ instead of / ). Just to confirm Flask's builtin development server doesn't properly handle read SCRIPT_NAME from the environment -- run the app with it: $ FLASK_APP = app SCRIPT_NAME = /my-app flask run The prefix is ignored, which means that accessing / works (which we don't want): $ curl localhost:5000/ / And /my-app/ unfortunately doesn't: $ curl localhost:5000/my-app/ # outputs Flask's default 404 message","tags":"floss","url":"flask-wsgi-url-prefix.html"},{"title":"Vše, co jste kdy chtěli vědět o Pythonu...","text":"... ale báli jste se zeptat na hodině ;) In [1]: from utils import TableOfContents TableOfContents ( \"python_crash_course.ipynb\" ) Out[1]: Objekty Proměnné Typy a třídy Importování knihoven Funkce Lokální vs. globální proměnné print vs. return Pokročilé způsoby předávání argumentů Metody Čísla Řetězce Kolekce Seznamy n-tice Množiny Slovníky Vnořené kolekce Nezabudované kolekce Opakování operací: for-cyklus Logika Podmínky: if, elif, else (a while-cyklus) Konvertory kolekcí Drobnosti Práce se soubory Výrazy vs. příkazy Dokumentace Objekty Programování v Pythonu spočívá v manipulaci s různými objekty -- čísli, řetězci (písmen), seznamy, funkcemi... Zastřešující pojem \"objekt\" naznačuje analogii s fyzickými předměty. Každý předmět slouží k něčemu jinému, něco s ním lze udělat jednoduše, něco hůř, něco vůbec; taky případně může obsahovat další předměty. Podobně v Pythonu: čísla se hodí na sčítání, ale nemůžeme změřit jejich délku; a seznam jakožto objekt je vlastně kolekcí dalších objektů, jeho prvků. Když Python spustíte, načte se poměrně velké množství zabudovaných objektů (funkcí a konstant), se kterými můžete rovnou začít pracovat. Např. funkce sorted vytvoří z libovolného iterovatelného objektu (= objektu, ze kterého lze jeden po druhém tahat nějaké jeho dílčí prvky) seřazený seznam: In [2]: # cokoli za znakem # na jakémkoli řádku Python ignoruje, # jsou to tzv. komentáře, které slouží jen lidem sorted Out[2]: <function sorted(iterable, /, *, key=None, reverse=False)> ( Seznam všech zabudovaných funkcí -- built-in functions -- v Pythonu ) Konstanta True zas označuje \"pravdu\" (v logickém smyslu)... In [3]: True Out[3]: True ... takže ji Python vrátí jako výslednou hodnotu při vyhodnocení pravdivého výroku: In [4]: 2 + 2 == 4 Out[4]: True Ale další objekty si můžeme libovolně vytvářet. In [5]: # čísla 42 Out[5]: 42 In [6]: # řetězce \"Hello, world!\" Out[6]: 'Hello, world!' Proměnné Abychom s objekty mohli dál pracovat, je potřeba je nějak pojmenovat, jinak na ně nemůžeme odkazovat. Štítek se jménem můžeme na objekt pověsit pomocí přiřazovacího operátoru = . In [7]: vzdálenost = 10 In [8]: čas = 20 In [9]: rychlost = vzdálenost / čas In [10]: rychlost Out[10]: 0.5 Těm štítkům se jmény se říká proměnné , protože je lze libovolně kdykoli pověsit na jiný objekt. In [11]: vzdálenost = 5 In [12]: rychlost = vzdálenost / čas In [13]: rychlost Out[13]: 0.25 Jsou i jiné způsoby, jak pověsit štítek se jménem na nějaký objekt. Např. když teď vytvoříme funkci a pojmenujeme ji rychlost , tak štítek rychlost strhneme z čísla 0.25, na které jsme ho naposledy navěsili, a nalepíme ho na tu funkci (která je z hlediska Pythonu zase jen dalším typem objektu, jako čísla, řetězce atp.). In [14]: def rychlost ( vzdálenost , čas ): return vzdálenost / čas In [15]: rychlost Out[15]: <function __main__.rychlost(vzdálenost, čas)> Nyní tedy už proměnná rychlost neodkazuje na číslo 0.25, ale na funkci, kterou jsme právě vytvořili. Funkci můžeme zavolat , tj. spustit \"recept\" (sérii kroků, malý dílčí program), který je v ní uložený: In [16]: rychlost ( vzdálenost , čas ) Out[16]: 0.25 Nic nebrání tomu, abychom na existující objekty navěsili štítků víc, a můžeme k tomu použít stávající štítky: In [17]: distance = vzdálenost time = čas speed = rychlost In [18]: speed ( distance , time ) Out[18]: 0.25 Jak to funguje? Python nejprve vyhodnotí výraz napravo od operátoru = , tj. v tomto případě jen dohledá objekty, na které odkazují původní proměnné, a pak na ty samé objekty navěsí další nové štítky (uvedené nalevo od operátoru = ). Všimněte si, že jsme tímto způsobem jednoduše přiřadili další jméno i funkci rychlost : In [19]: speed Out[19]: <function __main__.rychlost(vzdálenost, čas)> To, zda dvě jména (dvě proměnné) odkazují na ten samý objekt, jednoduše zjistíme pomocí operátoru is : In [20]: distance is vzdálenost Out[20]: True In [21]: distance is čas Out[21]: False In [22]: distance = čas In [23]: distance is čas Out[23]: True In [24]: speed is rychlost Out[24]: True POZN.: Rovnost ( x == y ) a identita ( x is y ) jsou dvě různé věci. Dva různé objekty (z hlediska identity) jsou si rovné, pokud reprezentují tu samou hodnotu. Např. dva různé seznamy čísel od jedné do tří: In [25]: x = [ 1 , 2 , 3 ] y = [ 1 , 2 , 3 ] In [26]: x == y Out[26]: True In [27]: x is y Out[27]: False Je to podobně jako s jednovaječnými dvojčaty: vypadají stejně, takže třeba pro účel focení jsou zaměnitelná, ale to neznamená, že je to ten samý člověk. Ještě pár slov ke konvencím pojmenovávání proměnných v Pythonu: u drtivé většiny proměnných (vč. funkcí) je zvykem používat jen malá písmena, např. proměnná občas se používají i číslice, např. x1 , ale jméno proměnné číslicí nesmí začínat když pojmenování sestává z více slov, oddělují se pomocí podtržítka, např. víceslovná_pojmenování (tzv. snake case ) proměnné, které se používají napříč celým programem (často je definujeme na začátku programu a pak na ně odkazujeme v růzých jeho částech), se někdy odlišují pomocí velkých písmen, např. ABECEDA nebo RYCHLOST_SVĚTLA jména tříd (viz níže) jsou většinou v tzv. camel case s velkým počátečním písmenem, např. Třída či JináTřída Jméno proměnné by nemělo být moc dlouhé, ale zároveň by mělo být deskriptivní -- mělo by srozumitelně naznačovat, k čemu proměnná slouží, jak se používá. Hezky a trefně pojmenovat proměnnou je často těžší, než by se na první pohled mohlo zdát . Když píšete program na jedno použití, je to jedno, ale pokud po sobě budete program ještě někdy číst (nebo někdo jiný), budete za každou smysluplně pojmenovanou proměnnou (a každý vhodně umístěný komentář) vděční. Typy a třídy Každý objekt v Pythonu spadá do nějaké kategorie objektů -- např. číslo, řetězec, seznam apod. -- podobně jako předměty v reálném světě spadají do různých kategorií (všechny kočky do kategorie \"kočka\", všechny tužky do kategorie \"tužka\" apod.). Těmto kategoriím s v Pythonu říká typy a u libovolného objektu lze zjistit jeho typ pomocí funkce type : In [28]: obj = 1 type ( obj ) Out[28]: int In [29]: obj = \"ahoj\" type ( obj ) Out[29]: str In [30]: obj = [ 1 , \"ahoj\" ] type ( obj ) Out[30]: list In [31]: type ( 1 ) == type ( 2 ) Out[31]: True In [32]: type ( \"a\" ) == type ( \"b\" ) Out[32]: True In [33]: # ovšem pozor type ( \"1\" ) Out[33]: str In [34]: type ( 1 ) == type ( \"1\" ) Out[34]: False Typy jako int , str nebo list jsou v Pythonu přímo zabudované, jsou k dispozici vždycky. Ale každý programátor si může definovat vlastní typy a dále s nimi pracovat. Těmto uživatelsky definovaným typům se říká třídy , podle klíčového slovíčka class , pomocí něhož se definují: In [35]: class FooBar : # klíčové slovíčko pass je prázdný příkaz (Python na jeho základě nic # nevykoná) pass Instanci třídy, tj. konkrétní objekt, jehož typem bude daná třída, vytvoříme tak, že zavoláme tzv. konstruktor , tj. funkci, která nám instanci \"postaví\", \"zkonstruuje\". Počáteční sestavení a nastavení objektu někdy nazýváme inicializací . Jako konstruktor většinou slouží třída samotná, kterou zavoláme jako funkci: In [36]: obj = FooBar () type ( obj ) Out[36]: __main__.FooBar In [37]: type ( obj ) is FooBar Out[37]: True Oproti tomu instance zabudovaných typů můžeme vytvářet i pomocí speciálních zápisů, tzv. literálů , nepotřebujeme k tomu nutně funkce. V konstruktoru je proces vytváření objektu svým způsobem skrytý, není na první pohled jasné, co se v něm odehrává (obecně může konstruktorem být libovolná funkce), kdežto v literálu doslova (angl. literally ) prostě jen vypíšeme, jak má nějaký objekt vypadat, a Python nám ho podle toho poskládá: In [38]: # číselný literál 42 Out[38]: 42 In [39]: # řetězcový literál \"Hello, world!\" Out[39]: 'Hello, world!' Literály jsou součástí syntaxe Pythonu, tj. pravidel pro skládání programů tak, aby jim Python rozuměl a mohl je vykonávat. Důležité jsou ještě literály pro vytváření kolekcí (viz odd. Kolekce). Chceme-li ověřit, zda je nějaký objekt instancí nějaké dané třídy či obecněji typu, můžeme použít funkci isinstance . In [40]: isinstance ( obj , FooBar ) Out[40]: True In [41]: isinstance ( 1 , int ) Out[41]: True Třídy se hodí pro některé pokročilejší programátorské účely (viz např. notebook object_oriented.ipynb ). V tomto semestru se jim blíže nevěnujeme. Importování knihoven Knihovna (též balíček , angl. package ) je v programování soubor funkcí, tříd atp., které jsou často užitečné, takže nemá smysl, aby si je každý programátor implementoval znovu a znovu, když je potřebuje. Zároveň ale nejsou potřeba úplně pořád, takže taky nemá smysl, aby byly do Pythonu přímo zabudované (tj. aby byly k dispozici pokaždé, když Python spustíme, podobně třeba jako funkce sorted ). Vždy jsou ale na dosah ruky, stačí je importovat . Např. knihovna random obsahuje funkcionalitu související s generováním náhodných čísel: In [42]: import random # následujícímu příkazu není potřeba věnovat přílišnou # pozornost, jen zajistí, abychom pokaždé dostali stejnou # sérii pseudonáhodných čísel; viz: # https://docs.python.org/3/library/random.html#random.seed random . seed ( 42 ) K objektům, které knihovna definuje, se pak dostaneme přes tečku. Např. pokud chceme pomocí funkce randint vygenerovat náhodné číslo mezi jednou a deseti: In [43]: random . randint ( 1 , 10 ) Out[43]: 2 Při importu si lze knihovnu přejmenovat pomocí klíčového slovíčka as , např. pokud jméno random už v programu používáme pro něco jiného. In [44]: random = \"random. angl. náhodný\" import random as rnd rnd . randint ( 1 , 10 ), random Out[44]: (1, 'random. angl. náhodný') Taky je možné si z knihovny naimportovat přímo jeden konkrétní objekt (funkci, třídu atp.): In [45]: from random import randint randint ( 1 , 10 ) Out[45]: 5 To se hodí zejm. v případě, kdy objekt budeme používat často a nechce se nám pokaždé zdlouhavě psát jméno_knihovny.jméno_objektu . Pokud chceme, můžeme si jich rovnou naimportovat i víc zároveň: In [46]: from random import random , randint random (), randint ( 1 , 10 ) Out[46]: (0.24489185380347622, 3) Větší knihovny sestávají z dílčích modulů , které při importu oddělujeme tečkami. In [47]: import os.path this_file = \"python_crash_course.ipynb\" os . path . isfile ( this_file ) Out[47]: True In [48]: import os.path as osp osp . realpath ( this_file ) Out[48]: '/home/david/src/dlukes.github.io/content/notebooks/python_crash_course.ipynb' In [49]: from os.path import splitext splitext ( this_file ) Out[49]: ('python_crash_course', '.ipynb') Funkce Funkce je jako recept -- série příkazů, malý dílčí program, který má jméno, pomocí něhož ho můžeme kdykoli spusit, tzv. zavolat . Funkci definujeme pomocí klíčového slovíčka def ... In [50]: def hoď_kostkou (): # hlavička return randint ( 1 , 6 ) # tělo ... a voláme tak, že napíšeme její jméno a za ně kulaté závorky: In [51]: hoď_kostkou () Out[51]: 6 Recept uložený ve funkci se vykonává krok po kroku (řádek po řádku), dokud Python nenarazí na klíčové slovíčko return . V tu chvíli funkci ukončí a hodnotu výrazu, který se nachází za return , vrátí jako výsledek celé funkce. Tento výsledek můžeme uložit do proměnné: In [52]: výsledek_hodu_kostkou = hoď_kostkou () In [53]: výsledek_hodu_kostkou Out[53]: 1 Do těla funkce patří vše, co následuje po hlavičce a je odsazené alespoň o jednu úroveň dál než samotná hlavička: In [54]: def function (): print ( \"Tohle patří do těla funkce.\" ) print ( \"Tohle taky.\" ) print ( \"Tohle už ne.\" ) Tohle už ne. In [55]: function () Tohle patří do těla funkce. Tohle taky. Funkce často mají jeden nebo více parametrů či argumentů , tj. proměnných, které deklarujeme v hlavičce funkce. Např. následující funkce má parametry počet_stěn a násobek . In [56]: def hoď_kostkou_a_vynásob ( počet_stěn , násobek ): return randint ( 1 , počet_stěn ) * násobek Když funkci voláme, je potřeba za parametry dosadit konkrétní objekty, které funkce pak v rámci receptu použije: In [57]: hoď_kostkou_a_vynásob ( 20 , 100 ) Out[57]: 1800 POZN.: Přísně vzato bychom měli takto rozlišovat mezi (formálními) parametry (= proměnnými, se kterými pracujeme při definici funkce a které v tu chvíli nereprezentují žádnou konkrétní hodnotu) a (konkrétnimi) argumenty (= konkrétními objekty, které funkci předáme ve chvíli, kdy ji voláme). Nicméně často tento rozdíl není příliš důležitý, takže v praxi se oba termíny používají zaměnitelně. Parametry mohou mít defaultní hodnotu: In [58]: def hoď_kostkou_a_vynásob ( počet_stěn , násobek = 100 ): return randint ( 1 , počet_stěn ) * násobek Pokud nám defaultní hodnota daného parametru vyhovuje, nemusíme ho pak při volání specifikovat: In [59]: hoď_kostkou_a_vynásob ( 20 ) Out[59]: 300 Pokud nám naopak nevyhouje, nic nám v tom nebrání: In [60]: hoď_kostkou_a_vynásob ( 20 , 0.01 ) Out[60]: 0.19 Parametrům bez defaultní hodnoty se říká povinné (je potřeba je specifikovat vždy), parametrům s defaultní hodnotou pak volitelné . Parametry můžeme funkci předávat buď na základě pořadí (tzv. pozičně )... In [61]: # počet_stěn = 4, násobek = 10 hoď_kostkou_a_vynásob ( 4 , 10 ) Out[61]: 40 In [62]: # počet_stěn = 10, násobek = 4 hoď_kostkou_a_vynásob ( 10 , 4 ) Out[62]: 4 ... nebo pomocí jejich jmen (pak hovoříme o pojmenovaných argumentech , angl. named nebo též keyword arguments ): In [63]: hoď_kostkou_a_vynásob ( počet_stěn = 4 , násobek = 10 ) Out[63]: 10 In [64]: # v tomto případě na pořadí nezáleží hoď_kostkou_a_vynásob ( násobek = 10 , počet_stěn = 4 ) Out[64]: 10 Oba způsoby můžeme i kombinovat, dokonce je to velmi časté. V Pythonu je většinou zvykem, že se povinné parametry předávají pozičně (bývá jich málo a je potřeba je zadávat pokaždé, takže si člověk jejich pořadí zapamatuje) a volitelné parametry jsou pojmenované (může jich být mnoho a používají se příležitostně, takže si člověk jejich pořadí nezapamatuje; navíc často chceme specifikovat jen jeden a ostatním nechat defaultní hodnotu). V případě naší funkce by to vypadalo takto: In [65]: hoď_kostkou_a_vynásob ( 4 , násobek = 10 ) Out[65]: 20 Lokální vs. globální proměnné Parametry plus jakékoli další proměnné, které v rámci funkce vytvoříme, jsou soukromé, dostupné čistě jen funkci -- jsou to tzv. lokální proměnné . Též hovoříme o tom, že tyto proměnné mají lokální dosah (angl. local scope ). To znamená, že s nimi můžeme pracovat pouze v rámci těla funkce. In [66]: def function ( parameter ): local_variable = 1 print ( \"My parameter:\" , parameter ) print ( \"My local variable:\" , local_variable ) In [67]: function ( 0 ) My parameter: 0 My local variable: 1 Mimo tělo funkce function nejsou proměnné parameter a local_variable definované: In [68]: parameter --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-68-cfbbb8ed4864> in <module> ----> 1 parameter NameError : name 'parameter' is not defined In [69]: local_variable --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-69-ffb5b39c1381> in <module> ----> 1 local_variable NameError : name 'local_variable' is not defined TIP: Naučit se číst chybové hlášky je k nezaplacení, Python se vám s jejich pomocí snaží ze všech sil poradit, v čem je problém. Zkuste si každou chybovou hláškou pečlivě přečíst, zamyslet se nad ní, pochopit, v čem je problém a kde přesně nastal. Funkce mohou taky pracovat s globálními proměnnými , tj. s proměnnými definovanými samostatně, mimo nějaké funkce či třídy. In [70]: global_variable = 2 In [71]: def function ( parameter ): local_variable = 1 print ( \"My parameter:\" , parameter ) print ( \"My local variable:\" , local_variable ) print ( \"My global variable:\" , global_variable ) In [72]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 2 Taková funkce se ale snadno může rozbít -- stačí dotyčnou globální proměnnou smazat... In [73]: del global_variable ... a když funkci příště zavoláme, tak zkolabuje: In [74]: function ( 0 ) My parameter: 0 My local variable: 1 --------------------------------------------------------------------------- NameError Traceback (most recent call last) <ipython-input-74-e85ea32b6f77> in <module> ----> 1 function ( 0 ) <ipython-input-71-051e3887233a> in function (parameter) 3 print ( \"My parameter:\" , parameter ) 4 print ( \"My local variable:\" , local_variable ) ----> 5 print ( \"My global variable:\" , global_variable ) NameError : name 'global_variable' is not defined Snad ještě horší je to, že u takové funkce není možné jen na základě jejího volání odhadnout, co přesně udělá: In [75]: global_variable = 512 In [76]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 512 In [77]: global_variable = 1024 In [78]: function ( 0 ) My parameter: 0 My local variable: 1 My global variable: 1024 V obou případech funkci voláme jako function(0) , ale výsledek je pokaždé trochu jiný. Mnohem lepší je tedy psát funkce tak, aby používaly jen lokální proměnné a nebyly závislé na těch globálních. Nejde to vždycky, ale většinu času ano. Místo přímého odkazování na globální proměnné je lepší dát funkci více parametrů, přes něž jí můžeme globální proměnné zprostředkovaně předávat, což je mnohem bezpečnější a přehlednější. In [79]: def function ( parameter1 , parameter2 ): local_variable = 1 print ( \"My first parameter:\" , parameter1 ) print ( \"My local variable:\" , local_variable ) print ( \"My other parameter:\" , parameter2 ) In [80]: global_variable = 2 function ( 0 , global_variable ) My first parameter: 0 My local variable: 1 My other parameter: 2 print vs. return Jaký je rozdíl v tom, když zavolám následující dvě funkce? In [81]: def funkce1 (): print ( 1 ) In [82]: def funkce2 (): return 1 In [83]: funkce1 () 1 In [84]: funkce2 () Out[84]: 1 V obou případech se mi zobrazí číslo 1, v druhém je navíc vedle něj číslo v hranatých závorkách a dvojtečka. V čem se to liší? Rozdíl se lépe projeví, když se pokusíme výsledek funkce uložit do proměnné: In [85]: výsledek1 = funkce1 () 1 In [86]: výsledek1 In [87]: výsledek2 = funkce2 () In [88]: výsledek2 Out[88]: 1 funkce1 číslo 1 jen v rámci svého běhu vytiskne na obrazovku, kdežto funkce2 ho vrátí jako svůj výsledek, takže ho můžeme uložit do proměnné a dál s ním pracovat. Možná pomůže, když se podíváme na funkci, která jedno čísle tiskne a jiné vrací: In [89]: def funkce3 (): print ( 1 ) return 2 In [90]: výsledek3 = funkce3 () 1 In [91]: výsledek3 Out[91]: 2 Zbývá tedy jen otázka: co vrací funkce1 ? Nic, když vůbec ani neobsahuje klíčové slovíčko return ? Svým způsobem ano, ale i nic je v Pythonu něco ;) Což zní krypticky, ale je to jednoduché: Python má speciální objekt (konstantu) None , která reprezentuje \"nic\". Protože None je nic, tak se vám po vyhodnocení ani neukáže: In [92]: None A kdykoli nějaká funkce doběhne na konec svého receptu, aniž by potkala return , tak implicitně automaticky vrátí jako svůj výsledek None . Vím, že je trochu záludné, že ono None není \"vidět\", ale v praxi si můžeme to, že funkce1 skutečně vrátila None , lehce ověřit: In [93]: výsledek1 is None Out[93]: True Pokročilé způsoby předávání argumentů V Pythonu lze také vytvořit funkce, které pracují s libovolným počtem pozičních nebo pojmenovaných argumentů. Slouží k tomu speciální operátory * a ** : In [94]: def flexibilní_funkce ( poziční_argument , * zbytek_pozičních_argumentů , pojmenovaný_argument , ** zbytek_pojmenovaných_argumentů ): print ( \"poziční argument:\" , poziční_argument ) print ( \"zbytek pozičních argumentů:\" , zbytek_pozičních_argumentů ) print ( \"pojmenovaný argument:\" , pojmenovaný_argument ) print ( \"zbytek pojmenovaných argumentů:\" , zbytek_pojmenovaných_argumentů ) In [95]: flexibilní_funkce ( 1 , 2 , 3 , 4 , a = 5 , b = 6 , pojmenovaný_argument = 7 , c = 8 ) poziční argument: 1 zbytek pozičních argumentů: (2, 3, 4) pojmenovaný argument: 7 zbytek pojmenovaných argumentů: {'a': 5, 'b': 6, 'c': 8} Parametr *zbytek_pozičních_argumentů sesbírá zbylé poziční argumenty, sestaví z nich n-tici a tu uloží do proměnné zbytek_pozičních_argumentů ; podobně parametr **zbytek_pojmenovaných_argumentů sesbírá zbylé pojmenované argumenty, sestaví z nich slovník a uloží ho do proměnné zbytek_pojmenovaných_argumentů (víc o n-ticích a slovnících viz odd. Kolekce). Jak vidno, můžeme tyto speciální parametry pojmenovat libovolně, speciální chování jim propůjčuje operátor * , resp. ** . Nicméně konvenčně se v Pythonu používají jména *args a **kwargs (jako keyword arguments ). A aby toho nebylo málo, funguje to i \"naopak\": máme-li seznam/n-tici (nebo slovník) a chceme jeho položky předat funkci jako poziční (nebo pojmenované) argumenty, můžeme taky využít operátoru * (nebo ** ): In [96]: def nudná_funkce ( a , b ): return a + b In [97]: poziční_argumenty = [ 1 , 2 ] In [98]: nudná_funkce ( * poziční_argumenty ) Out[98]: 3 In [99]: pojmenované_argumenty = { \"a\" : 1 , \"b\" : 2 } In [100]: nudná_funkce ( ** pojmenované_argumenty ) Out[100]: 3 U pozičních argumentů musí pochopitelně sedět počet... In [101]: poziční_argumenty = [ 1 , 2 , 3 ] In [102]: nudná_funkce ( * poziční_argumenty ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-102-1d551cc00f6d> in <module> ----> 1 nudná_funkce ( * poziční_argumenty ) TypeError : nudná_funkce() takes 2 positional arguments but 3 were given ... a u pojmenovaných jména: In [103]: pojmenované_argumenty = { \"a\" : 1 , \"c\" : 2 } In [104]: nudná_funkce ( ** pojmenované_argumenty ) --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-104-75894a48cf5c> in <module> ----> 1 nudná_funkce ( ** pojmenované_argumenty ) TypeError : nudná_funkce() got an unexpected keyword argument 'c' Metody Metody jsou funkce úzce spjaté s objekty. Seznam metod, které daný objekt podporuje, získáme tak, že napíšeme jméno proměnné, která objekt obsahuje, za ně tečku a stiskneme tabulátor: obj .< TAB > In [105]: obj = \"ahoj\" obj . upper () Out[105]: 'AHOJ' Formálně je metoda jen funkce \"navěšená\" na třídě: In [106]: class FooBar : def get_my_type ( self ): return type ( self ) In [107]: obj = FooBar () obj . get_my_type () Out[107]: __main__.FooBar První argument metody ( self ) odkazuje na instanci třídy, na které jsme metodu zavolali, tj. v příkladu výše na ten samý objekt, na který odkazuje proměnná obj . Na rozdíl od případných dalších argumentů se metodě předává pomocí propojení přes tečku, ne v závorkách za metodou, nepíšeme tedy obj.get_my_type(obj) . Čísla V Pythonu jsou dva základní typy čísel: celá čísla (angl. integer ) -- typ int reálná čísla, reprezentovaná pomocí tzv. pohyblivé řádové čárky (angl. floating point ) -- typ float Literály celých čísel vypadají např. takhle: In [108]: 3 Out[108]: 3 In [109]: type ( 3 ) Out[109]: int In [110]: - 5 Out[110]: -5 In [111]: 9543761 Out[111]: 9543761 Pro lepší čitelnost mohou obsahovat i podtržítka: In [112]: 9_543_761 Out[112]: 9543761 Literály reálných čísel se většinou poznají podle toho, že obsahují desetinnou čárku... tedy tečku protože je to podle angličtiny: In [113]: 3.14 Out[113]: 3.14 In [114]: type ( 3.14 ) Out[114]: float In [115]: - 2.72 Out[115]: -2.72 In [116]: 0.1 Out[116]: 0.1 In [117]: # nulu můžeme vynechat . 1 Out[117]: 0.1 Ovšem ne nutně -- Python má speciální syntax pro tzv. vědecký zápis čísel , a čísla zapsaná tímto způsobem jsou vždycky typu float , i když desetinnou tečku neobsahují. In [118]: # 1 × 10³ 1e3 Out[118]: 1000.0 In [119]: # 1 × 10⁻³ 1e-3 Out[119]: 0.001 In [120]: # 2.34 × 10² 2.34e2 Out[120]: 234.0 Kromě celých a reálných čísel disponuje Python i komplexními čísly: In [121]: 3 + 4 j Out[121]: (3+4j) In [122]: type ( 3 + 4 j ) Out[122]: complex Jestli jste o nich nikdy neslyšeli, tak je zase pusťte z hlavy :) Kromě desítkové (decimální) soustavy, na kterou jsme všichni zvyklí, mají celá čísla v Pythonu literály i v soustavách jiných. Rozpoznáme je pomocí prefixů: 0b → binární (dvojková) soustava -- používá číslice 0 a 1 0o → oktální (osmičková) soustava -- používá číslice 0–7 0x → hexadecimální (šestnáctková) soustava -- používá číslice 0–9 a a–f In [123]: 0b11010 Out[123]: 26 In [124]: 0o32 Out[124]: 26 In [125]: 0x1a Out[125]: 26 Jak vidíme, převod na desítkovou soustavu je jednoduchý -- stačí vyhodnotit literál v soustavě jiné a Python nám ho v odpovědi převede do desítkové. K převodu opačným směrem existují funkce, které nám vrátí řetězec obsahující zápis čísla v požadované soustavě: In [126]: bin ( 26 ) Out[126]: '0b11010' In [127]: oct ( 26 ) Out[127]: '0o32' In [128]: hex ( 26 ) Out[128]: '0x1a' S čísly jdou provádět různé výpočty pomocí následujících operátorů , které snad díky zkušenosti s kalkulačkou budou působit převážně povědomě. In [129]: # sčítání 3 + 4 Out[129]: 7 In [130]: # odečítání 3 - 4 Out[130]: -1 In [131]: # násobení 3 * 4 Out[131]: 12 In [132]: # exponenciace 2 ** 3 Out[132]: 8 In [133]: # dělení 5 / 3 Out[133]: 1.6666666666666667 U dělení si všimněte, že výsledek je vždy float , i když dělíme dva int y a výsledkem je něco, co my lidé chápeme jako celé číslo: In [134]: 4 / 2 Out[134]: 2.0 In [135]: # celočíselné dělení 4 // 2 Out[135]: 2 In [136]: 5 // 3 Out[136]: 1 In [137]: # modulo (= zbytek po celočíselném dělení) 5 % 3 Out[137]: 2 Když chceme zároveň celočíselné dělení i zbytek po něm, můžeme použít zabudovanou funkci divmod . In [138]: divmod ( 5 , 3 ) Out[138]: (1, 2) In [139]: divmod ( 5 , 3 ) == ( 5 // 3 , 5 % 3 ) Out[139]: True To se hodí např. při časových převodech -- kolik je 143 vteřin minut? In [140]: divmod ( 143 , 60 ) Out[140]: (2, 23) → 2 minuty a 23 vteřin. Zabudovaná funkce abs vrátí absolutní hodnotu čísla: In [141]: abs ( - 4.1 ) Out[141]: 4.1 Pokud máme nějakou kolekci čísel (viz odd. Kolekce), můžeme identifikovat nejmenší, resp. největší z nich pomocí zabudovaných funkcí min a max . In [142]: from math import inf # inf reprezentuje nekonečno inf Out[142]: inf In [143]: čísla = [ 0 , - inf , 3.14 , - 2.72 , inf ] In [144]: min ( čísla ) Out[144]: -inf In [145]: max ( čísla ) Out[145]: inf Operátory mají různou prioritu , stejně jako v matematice: In [146]: 2 ** 3 + 4 * 5 Out[146]: 28 Pokud si nejsme pořadím operací jisti, můžeme ho pro jistotu specifikovat pomocí kulatých závorek: In [147]: ( 2 ** 3 ) + ( 4 * 5 ) Out[147]: 28 Nebo pochopitelně i změnit: In [148]: 2 ** ( 3 + 4 ) * 5 Out[148]: 640 Všechny výše uvedené typy i zápisy čísel můžeme při výpočtech libovolně kombinovat, jen je potřeba mít na paměti, že jakmile se nám jako dílčí výsledek objeví float , i celkový výsledek bude float . In [149]: - 0x1f + 165 * - 0b1001 + 1.0 Out[149]: -1515.0 Ke konverzi z int u na float slouží funkce float : In [150]: i = 3 In [151]: float ( i ) Out[151]: 3.0 Dokáže též převést textový zápis čísla (řetězec) na reálné číslo: In [152]: float ( \"3.14\" ) Out[152]: 3.14 In [153]: float ( \"3\" ) Out[153]: 3.0 Analogicky funguje funkce int : In [154]: int ( 2.0 ) Out[154]: 2 In [155]: int ( \"2\" ) Out[155]: 2 In [156]: int ( 2.72 ) Out[156]: 2 Všimněte si, že funkce int prostě jen uřízne desetinná místa. Pokud chceme zaokrouhlit podle matematických zvyklostí, je potřeba použít funkci round : In [157]: round ( 2.72 ) Out[157]: 3 Při počítání s float y je nutná jistá obezřetnost. Kvůli tomu, jak jsou v paměti počítače reprezentovány, není jejich zachycení úplně přesné, a některé výsledky tak můžou být překvapivé... In [158]: . 1 + . 2 Out[158]: 0.30000000000000004 ... což vede až k tomu, že některé očekávané rovnosti neplatí: In [159]: . 1 + . 2 == . 3 Out[159]: False Při porovnávání float ů je bezpečnější místo operátoru == (striktní rovnosti) používat funkci isclose z knihovny math (přibližná rovnost). In [160]: from math import isclose isclose ( . 1 + . 2 , . 3 ) Out[160]: True S číselnými proměnnými často narážíme na následující situaci: vytvoříme proměnnou... In [161]: i = 0 i Out[161]: 0 ... a následně ji pak pomocí nějaké aritmetické operace potřebujeme aktualizovat na základě staré hodnoty -- např. přičíst k původnímu číslu nějaké nové a výsledek znovu uložit do té samé proměnné: In [162]: i = i + 2 i Out[162]: 2 Python umožňuje zápis i = i + x zkrátit na i += x , abychom nemuseli psát i dvakrát: In [163]: i += 7 i Out[163]: 9 Tento zkrácený zápis funguje s libovolným operátorem. In [164]: i **= 2 i Out[164]: 81 Řetězce Řetězce (angl. strings , v Pythonu typ str ) nám umožňují reprezentovat text jako uspořádanou sérii (řetězec) znaků. Literály řetězců mají různé varianty, vždy jsou ale ohraničené jednoduchými nebo dvojitými uvozovkami. In [165]: \"hello\" Out[165]: 'hello' In [166]: 'hello' Out[166]: 'hello' In [167]: type ( \"hello\" ) Out[167]: str Chceme-li do řetězce uvozeného jednoduchými (dvojitými) uvozovkami vložit jednoduchou (dvojitou) uvozovku, je potřeba zrušit její speciální význam \"ukončovač řetězce\". K tomu slouží zpětné lomítko. Angl. se říká the backslash escapes the next character . In [168]: \" \\\" Hello, \\\" I said.\" Out[168]: '\"Hello,\" I said.' In [169]: ' \\' Hello, \\' I said.' Out[169]: \"'Hello,' I said.\" Někdy je ale jednodušší prostě jen prostřídat druh uvozovek, které slouží k delimitaci řetězce, jak nám to ve svých odpovědích naznačuje sám Python. Zpětné lomítko ale neslouží jen k rušení speciálního významu uvozovek -- jiným znakům speciální význam naopak dodává. Slouží tak spíš jako přepínač mezi doslovným a speciálním významem. Speciálním sekvencím znaků , které tvoří zpětné lomítko + jeden či více dalších znaků a mají jiný než doslovný význam, se angl. říká escape sequences . Např. sekvence \\n se promění ve znak nového řádku, \\t pak ve znak tabulátoru: In [170]: \"a \\t 1 \\n aa \\t 11\" Out[170]: 'a\\t1\\naa\\t11' Na první pohled to tak nevypadá, ale to je jen kvůli tomu, že nás Python chce na tyto speciální znaky upozornit (což je dobře), takže když řetězec jen zobrazuje , reprezentuje tyto znaky pomocí dobře čitelných speciálních sekvencí. Aby se nové řádky a tabulátory projevily, musíme řetězec vytisknout : In [171]: print ( \"a \\t 1 \\n aa \\t 11\" ) a 1 aa 11 Což odpovídá zamýšlené podobě, ale je mnohem těžší poznat, co je v řetězci skutečně za znaky. Úplně jiný řetězec může totiž vytištěný vypadat zcela identicky: In [172]: print ( \"a 1 \\n aa 11 \" ) a 1 aa 11 Speciální sekvence existují též např. pro vložení libovolného unicodového znaku, ve formátu \\uXXXX nebo \\UXXXXXXXX , kde XXX... je hexadecimální zápis pořadového čísla znaku v unicodové tabulce: In [173]: \"asi nějaký čínský znak...? \\u4e3e \" Out[173]: 'asi nějaký čínský znak...? 举' Pořadové číslo znaku lze v Pythonu získat pomocí funkce ord ... In [174]: ord ( \"č\" ) Out[174]: 269 ... takto získáme jeho hexadecimální zápis... In [175]: hex ( 269 ) Out[175]: '0x10d' ... a ten pak můžeme použít ve speciální sekvenci: In [176]: \"tohle by mělo být č: \\u010d \" Out[176]: 'tohle by mělo být č: č' Inverzní funkcí k funkci ord je funkce chr -- dáme jí pořadové číslo znaku a ona nám vrátí odpovídající znak. In [177]: chr ( 269 ) Out[177]: 'č' In [178]: chr ( 0x10d ) Out[178]: 'č' Více o znacích, Unicodu a kódování textu obecně viz samostatný notebook unicode.ipynb . Python pozná, když zpětné lomítko nepředchází znaku či znakům, s nímž/nimiž by tvořilo speciální sekvenci, a vloží na takovém místě doslovné zpětné lomítko, ale zároveň nám jemně naznačí, že by byl radši, kdybychom speciální přepínací význam zpětného lomítka v literálu explicitně vypnuli... pomocí zpětného lomítka: In [179]: \"a \\ b\" Out[179]: 'a \\\\ b' In [180]: print ( \"a \\ b\" ) a \\ b In [181]: print ( \"a \\\\ b\" ) a \\ b In [182]: \"a \\ b\" == \"a \\\\ b\" Out[182]: True Z toho mj. plyne, že když chceme zpětných lomítek napsat víc za sebou, je jich potřeba vždy dvojnásobek: In [183]: \"a \\\\\\\\ b\" Out[183]: 'a \\\\\\\\ b' In [184]: print ( \"a \\\\\\\\ b\" ) a \\\\ b Podobné literály začnou brzy vypadat nepřehledně. Člověk pak zatouží všechny speciální sekvence zrušit, jen aby všechny znaky fungovaly jednoduše doslovně. I to je možné, pomocí tzv. surových řetězců (angl. raw strings ). V nich je zpětné lomítko jen další znak: In [185]: r \"\\t \\\\\\\\ \\n\" Out[185]: '\\\\t \\\\\\\\\\\\\\\\ \\\\n' In [186]: print ( r \"\\t \\\\\\\\ \\n\" ) \\t \\\\\\\\ \\n Surové řetězce se velmi dobře hodí pro práci s regulárními výrazy (viz notebook regex.ipynb ), kde se lomítka často používají a hodí se nemuset přemýšlet o tom, jestl nám je Python nějak nepomíchá. Formátovací řetězce (angl. format strings ) umožňují pomocí složených závorek vkládat do řetězců libovolné výrazy: In [187]: x = 1 y = 2 f \" { x } + { y } = { x + y } \" Out[187]: '1 + 2 = 3' Místo pár jednoduchých/dvojitých uvozovek můžeme řetězce též delimitovat párem trojic jednoduchých/dvojitých uvozovek. Tyto literály můžou obsahovat doslovné znaky nového řádku (ne jen speciální sekvenci \\n ) jednoduché/dvojité uvozovky bez lomítek (jen ne tři za sebou). In [188]: \"\"\"a b c\"\"\" Out[188]: 'a\\nb\\nc' In [189]: \"\"\" a b c \"\"\" Out[189]: '\\na\\nb\\nc\\n' In [190]: s = \"\"\"\"Hello,\" I said. \"Hi,\" she replied.\"\"\" In [191]: print ( s ) \"Hello,\" I said. \"Hi,\" she replied. Jinak v nich platí stejná pravidla jako v běžných řetězcích, můžou být taktéž surové nebo formátovací nebo obojí zároveň atp. Řetězce mají mnoho užitečných metod. Několik jich začíná na is... a informují nás o obsahu řetězce: In [192]: \"hello\" . islower () Out[192]: True In [193]: \"HELLO\" . isupper () Out[193]: True In [194]: \"Hello\" . istitle () Out[194]: True In [195]: \"12\" . isnumeric () Out[195]: True Jiné vytvoří nový pozměněný řetězec: In [196]: \"hello\" . upper () Out[196]: 'HELLO' In [197]: \"hello\" . title () Out[197]: 'Hello' Metoda strip oseká z okrajů řetězce prázdné znaky (angl. whitespace )... In [198]: \" \\n ZZZ \\n \\t \\n \" . strip () Out[198]: 'ZZZ' ... popřípadě libovolné znaky, které jí zadáme: In [199]: \"bbabZZZaabaaa\" . strip ( \"ab\" ) Out[199]: 'ZZZ' Metoda split naseká řetězec na dílčí řetězce na prázdných znacích... In [200]: \"a b c\" . split () Out[200]: ['a', 'b', 'c'] ... nebo na každém výskytu zadaného podřetězce: In [201]: \"a, b, c\" . split ( \", \" ) Out[201]: ['a', 'b', 'c'] Prázdných znaků může být libovolné množství a pokud dělením vznikne prázdný řetězec, tak je z výsledku odebrán... In [202]: \" a \\n b \\t c \\n \" . split () Out[202]: ['a', 'b', 'c'] ... ale pokud zadáme podřetězec, hledá metoda doslova a do písmene přesně jeho výskyty a případné prázdné řetězce ve výstupu zachovává: In [203]: \" a, b, , c\" . split ( \", \" ) Out[203]: [' a', 'b', '', ' c'] Opakem metody split je metoda join , která pospojuje kolekci řetězců, kterou jí předáme jako argument: In [204]: \"-\" . join ([ \"a\" , \"b\" , \"c\" ]) Out[204]: 'a-b-c' In [205]: # i řetězec lze chápat jako kolekci jednoznakových řetězců \"-\" . join ( \"abc\" ) Out[205]: 'a-b-c' Důležité je, že řetězce patří k tzv. nemodifikovatelným typům, tj. ať s nimi děláme co děláme... In [206]: s = \" a,b,c \" s Out[206]: ' a,b,c ' In [207]: s . strip () . split ( \",\" ) Out[207]: ['a', 'b', 'c'] ... původní řetězec vždy zůstane nedotčený: In [208]: s Out[208]: ' a,b,c ' Více o (ne)modifikovatelných typech viz odd. Kolekce. Ještě pár slov k funkci print . Tato funkce vytiskne všechny svoje poziční argumenty oddělené mezerami a na závěr přilepí znak nového řádku. In [209]: print ( 1 , \"a\" , []) 1 a [] Oddělovač (mezeru) i ukončovač (nový řádek) si můžeme upravit pomocí pojmenovaných argumentů sep a end -- může to být libovolný jiný řetězec. In [210]: print ( 1 , \"a\" , [], sep = \"__ODDĚLOVAČ__\" , end = \"__UKONČOVAČ__\" ) 1__ODDĚLOVAČ__a__ODDĚLOVAČ__[]__UKONČOVAČ__ Jak to, že print umí vytisknout i jiné objekty než jen řetězce? Ve skutečnosti neumí, jen na každý poziční argument zavolá funkci str , která ho právě na řetězec převede. In [211]: str ( 1 ) Out[211]: '1' In [212]: str ([]) Out[212]: '[]' U objektů, které již řetězci jsou, žádný převod pochopitelně není potřeba, str jako výsledek vrátí nezměněný argument: In [213]: str ( \"a\" ) Out[213]: 'a' Kolekce Kolekce je objekt, který obsahuje další objekty, které z něj lze jeden po druhém vytáhnout. Mezi základní zabudované typy kolekcí patří: seznamy -- typ list n-tice -- typ tuple množiny -- typ set slovníky -- typ dict Svým způsobem můžeme i na řetězce nahlížet jako na kolekce znaků -- jak uvidíme níže, jde s nimi provádět stejné typy operací. Stejně jako ostatní kolekce např. reagují na funkci len , která vrací počet prvků v kolekci... In [214]: len ( \"abc\" ) Out[214]: 3 ... nebo na operátor in , který testuje přítomnost prvku v kolekci... In [215]: \"b\" in \"abc\" Out[215]: True ... nebo na funkci sorted , která umí na základě kolekce vyrobit seřazený seznam jejích prvků: In [216]: sorted ( \"bca\" ) Out[216]: ['a', 'b', 'c'] Seznamy Klíčové vlastnosti: uspořádanost (angl. se uspořádaná kolekce řekne ordered collection ): zachovává pořadí prvků prvky se mohou opakovat modifikovatelnost (angl. mutability ): seznam lze kdykoli upravovat (přidávat/ubírat prvky) Literál seznamu sestává z hranatých závorek, mezi něž vypíšeme počáteční prvky seznamu oddělené čárkami: In [217]: [ 1 , \"a\" ] Out[217]: [1, 'a'] In [218]: # opakované prvky [ 1 , 1 , 1 ] Out[218]: [1, 1, 1] In [219]: # prázdný seznam [] Out[219]: [] Kromě literálu můžeme seznam vytvořit ještě pomocí funkce list , např. proměnit řetězec na seznam znaků: In [220]: list ( \"abc\" ) Out[220]: ['a', 'b', 'c'] Argument pro funkci list může být jakýkoli iterovatelný objekt, tj. objekt, ze kterého lze opakovaně tahat další objekty jako králíky z klobouku (iterace = opakování). Většinou se jedná o různé typy kolekcí, ale ne nutně. Např. funkce range umí vytvořit objekt, z nějž lze postupně tahat čísla v zadaném rozpětí: In [221]: list ( range ( 3 )) Out[221]: [0, 1, 2] In [222]: # prázdný seznam list () Out[222]: [] Vytvořme si nyní seznam na hraní: In [223]: zvířata = \"kočka pes morče slon žirafa kočka\" . split () zvířata Out[223]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka'] Díky tomu, že je seznam uspořádaný, má každý jeho prvek pořadového číslo, tzv. index (číslováno od nuly). Pomocí indexů lze k jednotlivým prvkům seznamu přistupovat (\"ukázat\" si na ně -- index pochází z latinského slova pro ukazováček). Této operaci se říká indexace a vypadá tak, že za seznam napíšeme hranaté závorky s požadovaným indexem: In [224]: zvířata [ 0 ] Out[224]: 'kočka' Záporná čísla indexují odzadu: In [225]: zvířata [ - 1 ] Out[225]: 'kočka' Teoreticky lze indexaci přilepit přímo za literál seznamu, byť to v praxi není moc užitečné a zápis vypadá zvláštně: In [226]: [ \"kočka\" , \"pes\" , \"morče\" ][ 2 ] Out[226]: 'morče' Dohledat index nějakého prvku můžeme pomocí metody index : In [227]: zvířata . index ( \"morče\" ) Out[227]: 2 In [228]: zvířata . index ( \"orangutan\" ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-228-979eb2c01602> in <module> ----> 1 zvířata . index ( \"orangutan\" ) ValueError : 'orangutan' is not in list Spočítat počet výskytů nějakého prvku můžeme pomocí metody count : In [229]: zvířata . count ( \"kočka\" ) Out[229]: 2 Když chceme vytáhnout ze seznamu ne jeden prvek, ale podseznam, použijeme při indexaci tzv. výřez (angl. slice): In [230]: zvířata [ 2 : 4 ] Out[230]: ['morče', 'slon'] První číslo je index prvku, jímž podseznam začíná, druhé číslo je index prvního prvku, který již do podseznamu nepatří . To je na první pohled možná trochu zvláštní, ale v kombinaci s tím, že vypuštěním prvního/druhého čísla dosáhneme toho, že výřez bude od začátku/do konce, nám to umožňuje jednoduše rozříznout seznam na nepřekrývající se části: In [231]: hranice = 3 In [232]: zvířata [: hranice ] Out[232]: ['kočka', 'pes', 'morče'] In [233]: zvířata [ hranice :] Out[233]: ['slon', 'žirafa', 'kočka'] Když vynecháme obě čísla, vytvoříme podseznam odpovídající původnímu seznamu: In [234]: zvířata [:] Out[234]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka'] In [235]: # což je to samé jako toto zvířata [ 0 : len ( zvířata )] Out[235]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka'] Ve výřezu můžeme volitelně specifikovat ještě třetí číslo, které stanovuje, že do seznamu má být zahrnutý jen každý x-tý prvek (např. každý druhý): In [236]: zvířata [ 1 : 4 : 2 ] Out[236]: ['pes', 'slon'] Když je toto číslo záporné, můžeme podseznam vyříznou v opačném směru (zprava doleva): In [237]: zvířata [ 4 : 1 : - 2 ] Out[237]: ['žirafa', 'morče'] In [238]: # takto lze tedy převrátit pořadí seznamu zvířata [:: - 1 ] Out[238]: ['kočka', 'žirafa', 'slon', 'morče', 'pes', 'kočka'] In [239]: # ale srozumitelnější je asi tento ekvivalentní zápis # pomocí funkce reversed list ( reversed ( zvířata )) Out[239]: ['kočka', 'žirafa', 'slon', 'morče', 'pes', 'kočka'] Kromě vyřezávání lze nové seznamy vytvářet i spojováním seznamů pomocí operátoru + ... In [240]: zvířata + [ \"mastodont\" ] Out[240]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka', 'mastodont'] ... nebo klonováním pomocí operátoru * : In [241]: 2 * zvířata Out[241]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka', 'kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka'] Říkali jsme si, že seznamy jsou modifikovatelné, že lze prvky libovolně ubírat a přidávat. Nicméně všechno, co jsme zatím s naším seznamem zvířata prováděli, na něm nezkřivilo ani vlásek: In [242]: zvířata Out[242]: ['kočka', 'pes', 'morče', 'slon', 'žirafa', 'kočka'] Zatím jsme tedy tento seznam nijak nemodifikovali, všechny operace, které jsme na něj aplikovali, vedly k tomu, že na jeho základě vznikl nový, který z toho původního nějakým způsobem vycházel. K modifikaci seznamu slouží některé jeho metody. Ukažme si to na příkladu seřazování. Samostatná funkce sorted vytvoří na základě původního seznamu nový seznam , který obsahuje stejné prvky , ovšem seřazené , a vrátí ho jako výsledek : In [243]: seřazená_zvířata = sorted ( zvířata ) seřazená_zvířata Out[243]: ['kočka', 'kočka', 'morče', 'pes', 'slon', 'žirafa'] Můžeme si jednoduše ověřit, že zvířata a seřazená_zvířata jsou dva různé objekty... In [244]: zvířata is seřazená_zvířata Out[244]: False ... a ani si nejsou rovné (protože záleží na pořadí): In [245]: zvířata == seřazená_zvířata Out[245]: False Naopak metoda sort seřadí původní seznam , na kterém ji zavoláme, a vrátí None : In [246]: # v tuto chvíli poprvé modifikujeme seznam zvířata... zvířata . sort () In [247]: # ... což si můžeme jednoduše ověřit (změnilo se pořadí) zvířata Out[247]: ['kočka', 'kočka', 'morče', 'pes', 'slon', 'žirafa'] Seznamy zvířata a seřazená_zvířata jsou nadále různé objekty... In [248]: zvířata is seřazená_zvířata Out[248]: False ... ale v tuto chvíli už jsou si rovné (protože teď už obsahují stejné prvky ve stejném pořadí): In [249]: zvířata == seřazená_zvířata Out[249]: True Některé další užitečné modifikující metody objektů typu list : In [250]: # přidání jednoho prvku na konec zvířata . append ( \"užovka\" ) zvířata Out[250]: ['kočka', 'kočka', 'morče', 'pes', 'slon', 'žirafa', 'užovka'] In [251]: # pomocí indexace lze nahradit jeden prvek... zvířata [ 4 ] = \"slonice\" zvířata Out[251]: ['kočka', 'kočka', 'morče', 'pes', 'slonice', 'žirafa', 'užovka'] In [252]: # ... nebo celý podseznam: zvířata [ 4 : 6 ] = [ \"slon\" , \"žirafáč\" ] zvířata Out[252]: ['kočka', 'kočka', 'morče', 'pes', 'slon', 'žirafáč', 'užovka'] In [253]: # přidání jednoho prvku na libovolný index zvířata . insert ( 1 , \"kocour\" ) zvířata Out[253]: ['kočka', 'kocour', 'kočka', 'morče', 'pes', 'slon', 'žirafáč', 'užovka'] In [254]: # přidání celé kolekce prvků na konec zvířata . extend ([ \"kapr\" , \"štika\" , \"candát\" ]) zvířata Out[254]: ['kočka', 'kocour', 'kočka', 'morče', 'pes', 'slon', 'žirafáč', 'užovka', 'kapr', 'štika', 'candát'] In [255]: # odebrání posledního prvku zvířata . pop () Out[255]: 'candát' In [256]: zvířata Out[256]: ['kočka', 'kocour', 'kočka', 'morče', 'pes', 'slon', 'žirafáč', 'užovka', 'kapr', 'štika'] In [257]: # odebrání prvku na libovolném indexu zvířata . pop ( 0 ) Out[257]: 'kočka' In [258]: zvířata Out[258]: ['kocour', 'kočka', 'morče', 'pes', 'slon', 'žirafáč', 'užovka', 'kapr', 'štika'] In [259]: # odebrání prvního výskytu konkrétního prvku zvířata . remove ( \"pes\" ) zvířata Out[259]: ['kocour', 'kočka', 'morče', 'slon', 'žirafáč', 'užovka', 'kapr', 'štika'] In [260]: # převrácení pořadí prvků zvířata . reverse () zvířata Out[260]: ['štika', 'kapr', 'užovka', 'žirafáč', 'slon', 'morče', 'kočka', 'kocour'] Důležitým důsledkem modifikovatelnosti je, že pokud na ten samý seznam odkazuje více proměnných (= má více jmen), jakákoli modifikace se projeví pod všemi jmény . Představte si analogii: pokud mám bratra, který se jmenuje Jan, tak když se Jan nechá ostříhat, bude ostříhaný i můj bratr, protože je to jedna a tatáž osoba. Podobně se seznamy: In [261]: jan = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] bratr = jan In [262]: jan is bratr Out[262]: True In [263]: # teď Jana \"ostříháme\" jan . pop () Out[263]: 'vlasy' In [264]: jan Out[264]: ['nohy', 'trup', 'hlava'] In [265]: bratr Out[265]: ['nohy', 'trup', 'hlava'] Modifikace je ošemetná zejména v případě, kdy je skrytá v nějaké funkci a týká se argumentů, které funkci předáváme zvenčí. Pak nám vůbec nemusí dojít, že argumenty vlastně modifikujeme: In [266]: def zkrášlit ( osoba ): # osobu zkrášlíme tak, že ji ostříháme osoba . pop () josef = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] In [267]: zkrášlit ( josef ) Nic není vidět, zafungovala to vůbec, podařilo se nám Josefa zkrášlit? No pro jistotu to zkusíme ještě jednou, tím přece nemůžeme nic zkazit, že... In [268]: zkrášlit ( josef ) Tak co teď? Podívejme se na Josefa... In [269]: josef Out[269]: ['nohy', 'trup'] Ježišmarja chudák Josef, kde má hlavu?! (Je to trochu fórek pórek, ale snad si rozumíme, v čem tkví nebezpečí :) ) Závěrem -- s modifikací je potřeba být opatrný a raději s ní šetřit: pokud možno přímo nemodifikovat argumenty funkcí kde se to hodí, používat místo seznamů nemodifikovatelné n-tice n-tice Klíčové vlastnosti jsou stejné jako u seznamů jen s tím rozdílem, že jsou nemodifikovatelné . Lze s nimi tedy provádět ty samé operace, s výjimkou těch modifikujících. Literály n-tic v zásadě mohou sestávat jen z prvků oddělených čárkami: In [270]: 1 , \"a\" Out[270]: (1, 'a') Nicméně v praxi je často potřeba obalit n-tici do kulatých závorek, protože čárka má jako operátor velmi nízkou prioritu (viz odd. Čísla). Je to podobné, jako když musíme použít závorky, aby sčítání proběhlo před dělením -- např. 2 + 3 × 4 = 14 vs. (2 + 3) × 4 = 20. Kdo se tím nechce moc zabývat, může závorky okolo n-tic používat pořád. In [271]: ( 1 , \"a\" ) Out[271]: (1, 'a') Specifickou syntax má prázdná n-tice... In [272]: () Out[272]: () In [273]: type (()) Out[273]: tuple In [274]: len (()) Out[274]: 0 ... a jednoprvková n-tice (čárka před uzavírací závorkou je povinná): In [275]: ( \"a\" ,) Out[275]: ('a',) Jiný iterovatelný objekt na n-tici proměníme pomocí funkce tuple : In [276]: tuple ([ 1 , \"a\" ]) Out[276]: (1, 'a') In [277]: tuple ( \"abc\" ) Out[277]: ('a', 'b', 'c') In [278]: tuple ( range ( 3 )) Out[278]: (0, 1, 2) Jedním z důsledků nemodifikovatelnosti n-tic je, že do nich lze sice indexovat... In [279]: ntice = tuple ( \"abc\" ) ntice [ 1 ] Out[279]: 'b' ... ale už nelze pomocí indexace prvky nahrazovat jinými: In [280]: ntice [ 1 ] = \"Z\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-280-53c8f0165f7f> in <module> ----> 1 ntice [ 1 ] = \"Z\" TypeError : 'tuple' object does not support item assignment Podobně řetězce, které jsou také nemodifikovatelné: In [281]: řetězec = \"abc\" řetězec [ 1 ] Out[281]: 'b' In [282]: řetězec [ 1 ] = \"Z\" --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-282-a215aaf9569e> in <module> ----> 1 řetězec [ 1 ] = \"Z\" TypeError : 'str' object does not support item assignment Množiny Klíčové vlastnosti: neuspořádanost : prvky nemají dané pořadí prvky se nesmí opakovat modifikovatelnost Literály vypadají následovně: In [283]: { 1 , \"a\" } Out[283]: {1, 'a'} Ovšem pozor, {} není prázdná množina, ale prázdný slovník (viz níže)! Prázdnou množinu získáme pomocí funkce set ... In [284]: set () Out[284]: set() ... která nám poslouží i k převodu jiné kolekce na množinu: In [285]: set ( \"kočička\" ) Out[285]: {'a', 'i', 'k', 'o', 'č'} Prvky v množině se nesmějí opakovat v tom smyslu, že do množiny nelze vložit dva prvky a a b , o nichž platí, že a == b . Nicméně není třeba se bát, když se o to pokusíme, nenastane chyba, množina prostě druhý pokus jednoduše ignoruje: In [286]: { 1 , 2 , 1 } Out[286]: {1, 2} In [287]: set ([ 1 , 2 , 1 ]) Out[287]: {1, 2} Vytvořme si dvě množiny na hraní: In [288]: set1 = { 1 , 2 , 3 } set2 = { 2 , 3 , 4 } Důležitým důsledkem neuspořádanosti je, že do množiny nelze indexovat -- ptát se po \"prvním\" prvku množiny nedává smysl: In [289]: set1 [ 0 ] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-289-c38563f1af7a> in <module> ----> 1 set1 [ 0 ] TypeError : 'set' object is not subscriptable Množiny podporují různé množinové operace. Některé jsou nemodifikující... In [290]: # sjednocení set1 . union ( set2 ) Out[290]: {1, 2, 3, 4} In [291]: # též možno pomocí operátoru | set1 | set2 Out[291]: {1, 2, 3, 4} In [292]: # průnik set1 . intersection ( set2 ) Out[292]: {2, 3} In [293]: # též možno pomocí operátoru & set1 & set2 Out[293]: {2, 3} In [294]: # rozdíl set1 . difference ( set2 ) Out[294]: {1} In [295]: # též možno pomocí operátoru - set1 - set2 Out[295]: {1} In [296]: # vztahy mezi množinami set1 . issubset ( set2 ) Out[296]: False ... jiné jsou modifikující: In [297]: # přidání prvku set1 . add ( 4 ) set1 Out[297]: {1, 2, 3, 4} In [298]: # zde k modifikaci nedojde, protože set1 už prvek 4 obsahuje set1 . add ( 4 ) set1 Out[298]: {1, 2, 3, 4} In [299]: # odebrání prvku set1 . remove ( 4 ) set1 Out[299]: {1, 2, 3} In [300]: # přidání více prvků, ovšem znovu pochopitelně s vyřazením duplicit set1 . update ([ 0 , 1 , 2 ]) set1 Out[300]: {0, 1, 2, 3} In [301]: # průnik, přičemž výsledek operace se uloží do původní množiny set1 set1 . intersection_update ( set2 ) set1 Out[301]: {2, 3} Teď už tedy platí: In [302]: set1 . issubset ( set2 ) Out[302]: True In [303]: # též možno pomocí operátoru < (\"menší než\") set1 < set2 Out[303]: True In [304]: set2 . issuperset ( set1 ) Out[304]: True In [305]: # též možno pomocí operátoru > (\"větší než\") set2 > set1 Out[305]: True Kromě toho, že nám množiny dobře poslouží, když chceme kolekci s duplicitami zredukovat na kolekci unikátních objektů (např. když chceme seznam tokenů v textu převést na množinu typů ), mají ještě jednu výhodu: díky tomu, že prvky nemusí respektovat pořadí zadané uživatelem, můžou být interně uspořádané tak, aby umožňovaly velmi rychle zjistit, zda množina daný prvek obsahuje či ne. V případě seznamu je potřeba ho procházet prvek po prvku, od začátku do konce, abychom zjistili, zda daný prvek obsahuje či ne. Čím je seznam delší, a čím dál v něm prvek je (nebo pokud v něm prvek vůbec není), tím déle to trvá: In [306]: # seznam prvního milionu čísel seznam = list ( range ( 1_000_000 )) In [307]: # Python musí seznam projít celý, prvek po prvku, aby zjistil, # že řetězec \"a\" v našem seznamu prvního milionu čísel není \"a\" in seznam Out[307]: False In [308]: %% timeit # speciální direktiva %%timeit není přímo součástí Pythonu, # poskytuje ji prostředí Jupyter; slouží k tomu, že spustí # danou buňku opakovaně, pokaždé změří, jak dlouho trvá # buňku vykonat, a pak nám ukáže průměrný čas \"a\" in seznam 8.28 ms ± 26.2 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) Oproti tomu v množině lze díky jejímu internímu uspořádání dohledat prvek velmi rychle: In [309]: množina = set ( seznam ) In [310]: \"a\" in množina Out[310]: False In [311]: %% timeit \"a\" in množina 26.3 ns ± 0.0176 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) U seznamu trvá naše operace v řádu desítek milisekund (10⁻³ s), kdežto u množiny jsou to desítky nanosekund (10⁻⁹ s) -- je tedy milionkrát rychlejší! Nemodifikovatelným protějškem typu set je typ frozenset . Slovníky Slovníky se od ostatních kolekcí, které jsme doposud potkali, liší v tom, že obsahují dva typy prvků: klíče (angl. keys ), podle nichž se ve slovníku hledá a hodnoty (angl. values ), které pod klíči dohledáme Je to podobně jako s jazykovými slovníky (proto se tak v Pythonu jmenují): když ve slovníku hledám slovo \"kočka\", tak \"kočka\" je klíč, a definice, kterou najdu, je jeho odpovídající hodnota. Stěžejní vlastnosti slovníků: neuspořádanost : prvky nemají dané pořadí klíče se nesmí opakovat , ale hodnoty klidně můžou modifikovatelnost Literály vypadají následovně: In [312]: { \"a\" : 1 , \"b\" : 2 } Out[312]: {'a': 1, 'b': 2} Prvek před dvojtečkou je vždy klíč, za dvojtečkou pak jeho odpovídající hodnota. Když stejný klíč specifikujeme víckrát, nenastane chyba, jen poslední asociovaná hodnota přemaže ty předchozí: In [313]: { \"a\" : 1 , \"a\" : 2 , \"a\" : 3 } Out[313]: {'a': 3} In [314]: # prázdný slovník {} Out[314]: {} In [315]: type ({}) Out[315]: dict Slovníky lze vytvářet i pomocí funkce dict , a to v zásadě dvěma způsoby. První možnost je, že do slovníku \"nasypeme\" kolekci sestávající z dvojic objektů, z nichž první vždy bude brán jako klíč a druhý jako jeho asociovaná hodnota: In [316]: # např. seznam n-tic o dvou položkách... dict ([( \"a\" , 1 ), ( \"b\" , 2 )]) Out[316]: {'a': 1, 'b': 2} In [317]: # ... nebo klidně seznam seznamů o dvou položkách... dict ([[ \"a\" , 1 ], [ \"b\" , 2 ]]) Out[317]: {'a': 1, 'b': 2} In [318]: # ... nebo třeba n-tice řetězců o dvou znacích atp. dict (( \"ab\" , \"cd\" )) Out[318]: {'a': 'b', 'c': 'd'} Druhá možnost je, že využijeme pojmenované argumenty -- jména argumentů pak skončí jako klíče, argumenty samotné jako hodnoty: In [319]: dict ( a = 1 , b = 2 ) Out[319]: {'a': 1, 'b': 2} Vytvořme si slovník na hraní: In [320]: slovník = { \"pes\" : \"nejlepší přítel člověka\" , \"kočka\" : \"vypočítavá potvora\" } Slovníky jsou sice stejně jako množiny neuspořádané, ale indexaci na rozdíl od nich podporují . Jen se jako index nepoužívá pořadové číslo (protože žádné pořadí neexistuje), ale klíč : In [321]: slovník [ \"kočka\" ] Out[321]: 'vypočítavá potvora' Přes metodu keys se dostaneme ke všem klíčům ve slovníku... In [322]: slovník . keys () Out[322]: dict_keys(['pes', 'kočka']) ... přes metodu values k hodnotám... In [323]: slovník . values () Out[323]: dict_values(['nejlepší přítel člověka', 'vypočítavá potvora']) ... a přes metodu items k uspořádaným dvojicím (klíč, hodnota) : In [324]: slovník . items () Out[324]: dict_items([('pes', 'nejlepší přítel člověka'), ('kočka', 'vypočítavá potvora')]) Když se pokusíme indexovat podle klíče, který ve slovníku není, Python zkolabuje: In [325]: slovník [ \"morče\" ] --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-325-21eeb4f972e5> in <module> ----> 1 slovník [ \"morče\" ] KeyError : 'morče' Abychom se kolapsu (a tím i zastavení běhu programu) vyhnuli, můžeme použít metodu get , která v případě absence klíče vrátí místo hodnoty ve slovníku nějaký jiný objekt (defaultně je to None , ale můžeme specifikovat libovolný objekt). In [326]: slovník . get ( \"pes\" ) Out[326]: 'nejlepší přítel člověka' In [327]: slovník . get ( \"morče\" ) In [328]: slovník . get ( \"morče\" , \"MORČE VE SLOVNÍKU NENÍ!\" ) Out[328]: 'MORČE VE SLOVNÍKU NENÍ!' Díky tomu, že jsou slovníky modifikovatelné, můžeme hodnoty odpovídající jednotlivým klíčům libovolně měnit... In [329]: slovník [ \"kočka\" ] = \"chlupatý mazlíček\" slovník Out[329]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček'} ... nebo přidávat nové: In [330]: slovník [ \"rybička\" ] = \"němá slizká tvář\" slovník Out[330]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář'} V prvním případě jsme přepsali původní hodnotu odpovídající klíči \"kočka\" , ve druhém jsme přidali nový klíč \"rybička\" a k němu odpovídající hodnotu. Syntax (zápis) ale vypadá v obou případech stejně. Občas potřebujeme zajistit, aby za žádnou cenu nedošlo k přepsání existující hodnoty (viz první případ výše). K tomu poslouží metoda setdefault -- pokud klíč už ve slovníku existuje, původní hodnotu nezmění, jen ji vrátí... In [331]: slovník . setdefault ( \"kočka\" , \"tuhle novou hodnotu do slovníku neprocpu :(\" ) Out[331]: 'chlupatý mazlíček' In [332]: slovník Out[332]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář'} ... ale pokud neexistuje, klíč a odpovídající hodnotu do slovníku přidá (a hodnotu taky vrátí): In [333]: slovník . setdefault ( \"morče\" , \"ale tuhle procpu :)\" ) Out[333]: 'ale tuhle procpu :)' In [334]: slovník Out[334]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)'} Metoda setdefault tedy funguje podobně jako metoda get s tím rozdílem, že v případě nepřítomnosti klíče ve slovníku defaultní hodnotu nejen vrátí, ale i uloží do slovníku. Metoda update , která přidává nové prvky do slovníku / upravuje stávající, nabízí co do argumentů, které zvládne zpracovat, podobné možnosti jako samotná funkce dict : In [335]: # libovolná kolekce dvojic prvků... slovník . update ([( \"a\" , 1 ), ( \"b\" , 2 )]) slovník Out[335]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'a': 1, 'b': 2} In [336]: slovník . update (( \"ab\" , \"cd\" )) slovník Out[336]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'a': 'b', 'b': 2, 'c': 'd'} In [337]: # ... jiný slovník... slovník . update ({ \"b\" : \"z\" }) slovník Out[337]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'a': 'b', 'b': 'z', 'c': 'd'} In [338]: # ... nebo pojmenované argumenty: slovník . update ( žralok = \"dravá mořská paryba\" , kůň = \"tam někde v pastvinách\" ) slovník Out[338]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'a': 'b', 'b': 'z', 'c': 'd', 'žralok': 'dravá mořská paryba', 'kůň': 'tam někde v pastvinách'} Odebírat prvky ze slovníku lze různými způsoby, podle toho, zda chceme odebrat konkrétní klíč a chceme ještě pracovat s jeho hodnotou... In [339]: # metoda pop smaže prvek podle klíče a vrátí odpovídající hodnotu slovník . pop ( \"a\" ) Out[339]: 'b' In [340]: slovník Out[340]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'b': 'z', 'c': 'd', 'žralok': 'dravá mořská paryba', 'kůň': 'tam někde v pastvinách'} ... nebo zda hodnotu k ničemu nepotřebujeme... In [341]: # operátor del smaže prvek podle klíče a nevrátí nic del slovník [ \"b\" ] In [342]: slovník Out[342]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'c': 'd', 'žralok': 'dravá mořská paryba', 'kůň': 'tam někde v pastvinách'} ... nebo zda je nám jedno, který prvek odebereme (necháme Python vybrat za nás): In [343]: # metoda popitem odebere nějaký prvek (nevíme předem jaký) # a vrátí klíč a hodnotu jako n-tici slovník . popitem () Out[343]: ('kůň', 'tam někde v pastvinách') In [344]: slovník Out[344]: {'pes': 'nejlepší přítel člověka', 'kočka': 'chlupatý mazlíček', 'rybička': 'němá slizká tvář', 'morče': 'ale tuhle procpu :)', 'c': 'd', 'žralok': 'dravá mořská paryba'} Díky neuspořádanosti a zákazu opakování je dohledávání klíčů ve slovníku velmi rychlé (stejně jako u množin) a zapisuje se velmi jednoduše: In [345]: \"kočka\" in slovník Out[345]: True Hodnoty ale ve slovníku takhle jednoduše nedohledáme: In [346]: \"nejlepší přítel člověka\" in slovník Out[346]: False Je potřeba je projít jednu po druhé pomocí metody values , což na rozdíl od klíčů trvá stejně dlouho jako u seznamu (asi jako kdybychom v jazykovém slovníku hledali tak, že bychom pročítali jen definice): In [347]: \"nejlepší přítel člověka\" in slovník . values () Out[347]: True Můžeme si to ověřit na libovolném velkém slovníku. K vytvoření takového velkého testovacího slovníku můžeme použít zabudovanou funkci zip , která \"sezipuje\" dohromady prvky několika iterovatelných objektů. Zní to složitě, ale když se na ni podíváme v akci, je to myslím jednoduché: In [348]: # vše obalíme do funkce list, aby se výsledné \"sezipované\" prvky # uložily do seznamu list ( zip ( \"abc\" , [ 1 , 2 , 3 ])) Out[348]: [('a', 1), ('b', 2), ('c', 3)] Pomocí funkce zip jednoduše můžeme vytvořit slovník, v němž namapujeme první milion čísel na sebe sama, ve stylu... In [349]: dict ( zip ( range ( 3 ), range ( 3 ))) Out[349]: {0: 0, 1: 1, 2: 2} ... akorát větší. Takový slovník samozřejmě není k ničemu užitečný, jen je velký, takže dobře poslouží k ilustraci rozdílu rychlosti v hledání mezi klíči a hodnotami. In [350]: libovolný_velký_slovník = dict ( zip ( range ( 1_000_000 ), range ( 1_000_000 ))) Hledání v hodnotách: In [351]: \"a\" in libovolný_velký_slovník . values () Out[351]: False In [352]: %% timeit \"a\" in libovolný_velký_slovník.values() 11.4 ms ± 243 µs per loop (mean ± std. dev. of 7 runs, 100 loops each) Hledání v klíčích: In [353]: \"a\" in libovolný_velký_slovník Out[353]: False In [354]: %% timeit \"a\" in libovolný_velký_slovník 23.6 ns ± 0.00474 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) Znovu pozorujeme rozdíl šesti řádů: hledání v klíčích je milionkrát rychlejší než hledání v hodnotách. A když už máme slovníku plné zuby, můžeme jeho obsah vymazat a začít nanovo: In [355]: libovolný_velký_slovník . clear () In [356]: libovolný_velký_slovník Out[356]: {} Vnořené kolekce Jak už jsme ostatně v některých příkladech výše naznačili, kolekce lze do sebe libovolně zanořovat, čímž můžeme reprezentovat různé složité strukturní vztahy. Literální zápis vnořených kolekcí je celkem intuitivní: In [357]: slovník = { \"hesla\" : { \"kočka\" : { \"definice\" : \"chlupatý mazlíček\" , \"příklad\" : \"Na okně seděla kočka...\" , }, \"pes\" : { \"definice\" : \"nejlepší přítel člověka\" , \"příklad\" : \"... a venku štěkal pes.\" , }, }, \"autoři\" : [ { \"jméno\" : \"John\" , \"příjmení\" : \"Doe\" }, { \"jméno\" : \"Jane\" , \"příjmení\" : \"Doe\" }, ], \"datum\" : ( 2018 , 11 , 7 ), } Naproti tomu indexace do vnořených kolekcí občas lidem činí při prvním setkání potíže. Chceme-li např. vytáhnout ze slovníku definici kočky, někdo má tendenci zkoušet následující zápis, který svým vnořením zrcadlí vnoření literálu: In [358]: slovník [ \"hesla\" [ \"kočka\" ]] <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <ipython-input-358-c2f5af94dca9>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? slovník[\"hesla\"[\"kočka\"]] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-358-c2f5af94dca9> in <module> ----> 1 slovník [ \"hesla\" [ \"kočka\" ] ] TypeError : string indices must be integers Chybová hláška zní trochu krypticky, ale můžeme z ní odvodit, že se Python zřejmě snaží indexovat do nějakého řetězce (tj. vytáhnout znak z řetězce), což jsme rozhodně nezamýšleli, takže zápis musí být špatně. Co se tedy děje? Pojďme si výraz rozebrat tak, jak ho vidí Python. Nejdřív vidí proměnnou slovník , za ní hranaté závorky. Usoudí tedy (správně), že se snažíme přistoupit k nějakému klíči ve slovníku. K jakému klíči? To je potřeba zjistit na základě obsahu hranatých závorek. Jinými slovy, Python v tuto chvíli vidí slovník[X] , kde X je zástupný znak pro klíč, který chceme, aby ve slovníku dohledal. Dobře, tak dál. Abych mohl X dohledat, uvažuje Python, musím nejdřív zjistit, co je zač. Hm, podle všeho mám za X dosadit výraz \"hesla\"[Y] . Ten musím tedy vyhodnotit jako první, abych mohl posléze vyhodnotit výraz slovník[X] . Co je \"hesla\"[Y] za výraz? Je to výraz, v němž máme řetězec \"hesla\" , a pomocí indexace se z něj snažíme vytáhnout znak, který odpovídá indexu Y . Skvěle, tetelí se Python, už tedy stačí jen zjistit, jaká je hodnota indexu Y , a můžu začít celý výraz vyhodnocovat, pěkně zvnitřka k vnějšku. Jenže ouha, Y by mělo být pořadové číslo požadovaného znaku, ale místo toho je to řetězec \"kočka\" . Chudák Python neví, co to znamená, vytáhnout \"kočka\" -tý znak z řetězce \"hesla\" , a tak to vzdá a jen si postěžuje, že indexy do řetězců by měly být celá čísla (\"string indices must be integers\"). Jinými slovy, vyhodnocování celého výrazu selže na tomto podvýrazu: In [359]: \"hesla\" [ \"kočka\" ] <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? <ipython-input-359-acc1bf557eb4>:1: SyntaxWarning: str indices must be integers or slices, not str; perhaps you missed a comma? \"hesla\"[\"kočka\"] --------------------------------------------------------------------------- TypeError Traceback (most recent call last) <ipython-input-359-acc1bf557eb4> in <module> ----> 1 \"hesla\" [ \"kočka\" ] TypeError : string indices must be integers Jak vidno, chyba je stejná. Jak tedy na to? Vždycky je možné rozdělit indexaci do vnořené kolekce na víc jednoduchých indexací s použitím pomocných proměnných: In [360]: hesla = slovník [ \"hesla\" ] kočka = hesla [ \"kočka\" ] kočka Out[360]: {'definice': 'chlupatý mazlíček', 'příklad': 'Na okně seděla kočka...'} Z výše uvedeného zápisu ale plyne, že se meziproměnné hesla můžeme zbavit pomocí dosazování . Začněme od proměnné kočka , která obsahuje zamýšlený výsledek (slovníkové heslo pro kočku; mezery slouží jen ke zvýraznění toho, jak probíhá dosazování): In [361]: kočka Out[361]: {'definice': 'chlupatý mazlíček', 'příklad': 'Na okně seděla kočka...'} Za výraz kočka lze dosadit výraz hesla[\"kočka\"] (protože kočka = hesla[\"kočka\"] ): In [362]: hesla [ \"kočka\" ] Out[362]: {'definice': 'chlupatý mazlíček', 'příklad': 'Na okně seděla kočka...'} Výborně, výsledek zůstal stejný, takže dosazení platí. Dále můžeme za výraz hesla dosadit výraz slovník[\"hesla\"] (protože hesla = slovník[\"hesla] ), přičemž útržek kódu [\"kočka\"] zůstane tam, kde byl: In [363]: slovník [ \"hesla\" ] [ \"kočka\" ] Out[363]: {'definice': 'chlupatý mazlíček', 'příklad': 'Na okně seděla kočka...'} Skvěle, výsledek je stále stejný! Teď už se jen zbavíme mezer, aby byl zápis konvenčnější, a vymysleli jsme způsob, jak indexovat do vnořených kolekcí. In [364]: slovník [ \"hesla\" ][ \"kočka\" ] Out[364]: {'definice': 'chlupatý mazlíček', 'příklad': 'Na okně seděla kočka...'} In [365]: # rok slovník [ \"datum\" ][ 0 ] Out[365]: 2018 In [366]: # příkladová věta pro psa slovník [ \"hesla\" ][ \"pes\" ][ \"příklad\" ] Out[366]: '... a venku štěkal pes.' In [367]: # jméno druhého autora slovník [ \"autoři\" ][ 1 ][ \"jméno\" ] Out[367]: 'Jane' A tak podobně. Nezabudované kolekce Existuje samozřejmě mnoho nezabudovaných kolekcí -- např. třída FreqDist z knihovny nltk je taky kolekce (obsahuje dílčí prvky). Nezabudované kolekce pochopitelně nemají literály, je potřeba je inicializovat pomocí příslušného konstruktoru třídy (viz odd. Typy a třídy): In [368]: from nltk import FreqDist In [369]: FreqDist ( \"aababc\" ) Out[369]: FreqDist({'a': 3, 'b': 2, 'c': 1}) Opakování operací: for-cyklus Často chceme nějakou operaci provést opakovaně, ale pokaždé s trochu jinými daty. Např. chceme vytisknout čísla od 0 do 2. Mohli bychom samozřejmě použít copy-paste, tj. řádek třikrát zkopírovat a pokaždé jen nahradit číslo... In [370]: print ( 0 ) print ( 1 ) print ( 2 ) 0 1 2 ... jenže pak je problém, že jakmile chceme akci trochu upravit, třeba vytisknout \"Číslo: 0\" místo jen \"0\", a podobně pro ostatní čísla, musíme provést úpravu na každém řádku: In [371]: print ( \"Číslo:\" , 0 ) print ( \"Číslo:\" , 1 ) print ( \"Čísloo:\" , 2 ) Číslo: 0 Číslo: 1 Čísloo: 2 Při takovém opisování / kopírování je snadné udělat na jednom místě chybu (viz výše \"Čísloo\" místo \"Číslo\") a najednou místo toho, abychom provedli třikrát tu samou operaci, provedeme dvakrát jednu a potřetí trochu jinou. Proto je lepší se zamyslet, co se při každém opakování mění a co naopak zůstává stejné, a zaznamenat to pomocí tzv. for-cyklu (angl. for-loop ): In [372]: for i in [ 0 , 1 , 2 ]: # hlavička print ( \"Číslo:\" , i ) # tělo Číslo: 0 Číslo: 1 Číslo: 2 Takový for-cyklus můžeme parafrázovat následovně: každé číslo ze seznamu [0, 1, 2] postupně ulož do proměnné i a proveď sérii operací popsaných v těle cyklu. Zkráceně: pro každé (angl for each ) číslo ze seznamu proveď následující operaci/operace. Hlavička for-cyklu popisuje proměnlivou část akce, kterou provádíme (zde: proměnná i postupně nabývá různých hodnot, které čerpáme ze seznamu). Tělo for-cyklu popisuje repetitivní část (zde: pokaždé proměnnou i vytiskneme, spolu s prefixem \"Číslo: \"). Podobně jako u funkcí poznáme to, co ještě patří do těla for-cyklu, a to, co už ne, podle odsazení: In [373]: for i in [ 0 , 1 , 2 ]: print ( \"Číslo:\" , i ) print ( \"Já ještě do těla cyklu patřím!\" ) print ( \"Já už ne :(\" ) Číslo: 0 Já ještě do těla cyklu patřím! Číslo: 1 Já ještě do těla cyklu patřím! Číslo: 2 Já ještě do těla cyklu patřím! Já už ne :( Obecně můžeme for-cyklus charakterizovat takto: for item in iterable : # do something with item Proměnná iterable může obsahovat jakýkoli iterovatelný objekt (= objekt, ze kterého lze tahat další objekty, viz odd. Kolekce). Seznamy už jsme v roli iterable viděli v akci výše, n-tice fungují stejně. Řetězce taky, přičemž ve for-cyklu je procházíme znak po znaku: In [374]: řetězec = \"abc\" for znak in řetězec : print ( znak ) a b c Množiny fungují podobně, jen není zaručené pořadí, v němž budeme prvky vytahovat: In [375]: množina = { 2 , 3 , 1 } for prvek in množina : print ( prvek ) 1 2 3 U slovníků je to trochu složitější -- záleží, zda chceme procházet klíče... In [376]: slovník = { \"a\" : 1 , \"b\" : 2 , \"c\" : 3 } for klíč in slovník : print ( klíč ) a b c In [377]: # nebo též explicitněji for klíč in slovník . keys (): print ( klíč ) a b c ... hodnoty... In [378]: for hodnota in slovník . values (): print ( hodnota ) 1 2 3 ... nebo obojí zároveň: In [379]: # jak přesně tato syntax funguje si vysvětlíme o kousek níž for klíč , hodnota in slovník . items (): print ( f \"Pod klíčem { klíč !r} je uložená hodnota { hodnota } .\" ) Pod klíčem 'a' je uložená hodnota 1. Pod klíčem 'b' je uložená hodnota 2. Pod klíčem 'c' je uložená hodnota 3. Ale iterovatelný objekt nerovná se nutně jen kolekce. V kombinaci s for-cyklem se často hodí funkce range : In [380]: for i in range ( 3 ): print ( \"Číslo:\" , i ) Číslo: 0 Číslo: 1 Číslo: 2 Funkce range vytvoří objekt, který v hlavičce for-cyklu postupně generuje čísla v zadaném rozpětí (v našem případě od 0 do 3, horní hranici vyjímaje). Proč je to užitečné? Představte si, že bychom chtěli operaci print(\"Číslo:\", i) provést pro prvních milion nezáporných čísel. Oproti seznamu má funkce range dvě výhody: pro nás je výhoda, že nemusíme ručně vypisovat seznam s milionem čísel pro počítač je výhoda, že nemusí vytvářet celý seznam najednou a pak ho uchovávat v paměti (milion čísel sice dnešní počítače zvládnou hravě, ale obecně platí, že paměti nikdy není neomezeně) -- čísla tiskne jedno po druhém a jakmile jedno zpracuje, tak ho může zapomenout Když proměnnou z hlavičky for-cyklu nikde v těle nepoužijeme, bývá zvykem jí dát speciální jméno _ , čímž ostatním programátorům naznačíme, že se nemají divit, že není použitá. In [381]: # tři hody kostkou for _ in range ( 3 ): print ( randint ( 1 , 6 )) 2 5 5 Občas se stane, že potřebujeme for-cyklem projít kolekci, jejímiž prvky jsou n-tice nebo seznamy, které v rámci for-cyklu potřebujeme dále rozebrat na dílčí prvky. Můžeme samozřejmě použít indexaci: In [382]: # Větu \"Prší.\" máme reprezentovanou jako seznam dvou tokenů. Každý token # je trojice řetězců: první označuje slovní tvar, druhý lemma (= # slovníkovou podobu), třetí slovní druh (V = sloveso, Z = interpunkce). věta = [( \"Prší\" , \"pršet\" , \"V\" ), ( \".\" , \".\" , \"Z\" )] for token in věta : word = token [ 0 ] lemma = token [ 1 ] tag = token [ 2 ] print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Prší', LEMMA: 'pršet', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' Práci nám ale může ušetřit tzv. destrukturace (angl. destructuring , též se tomu někdy říká unpacking nebo pattern matching ): máme-li uspořádanou kolekci (seznam nebo n-tici) o X prvcích, můžeme prvky rovnou namapovat na X proměnných: In [383]: a , b , c = [ 1 , 2 , 3 ] In [384]: a Out[384]: 1 In [385]: b Out[385]: 2 In [386]: c Out[386]: 3 Jak to funguje? Strukturní vzorec ( pattern ) nalevo od = obsahuje tři volné \"sloty\" (odpovídající třem proměnným). Python se tento vzorec pokusí přiložit na datovou strukturu napravo od = (představte si průhledný pauzovací papír), namapovat ( match ) objekty, které datová struktura obsahuje, na volné sloty, a strukturu tak rozebrat na dílčí prvky (proto hovoříme o destrukturaci nebo unpacking -- rozbalení). Ne vždycky se to samozřejmě povede -- např. když je slotů víc než prvků... In [387]: a , b , c = ( 1 , 2 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-387-24077ba23852> in <module> ----> 1 a , b , c = ( 1 , 2 ) ValueError : not enough values to unpack (expected 3, got 2) ... nebo naopak: In [388]: a , b = ( 1 , 2 , 3 ) --------------------------------------------------------------------------- ValueError Traceback (most recent call last) <ipython-input-388-faee755feda6> in <module> ----> 1 a , b = ( 1 , 2 , 3 ) ValueError : too many values to unpack (expected 2) Strukturní vzorec může obsahovat proměnnou s hvězdičkou (viz odd. Funkce), pak na sebe tato proměnná naváže zbývající prvky kolekce: In [389]: a , * rest = ( 1 , 2 , 3 , 4 , 5 , 6 ) In [390]: a Out[390]: 1 In [391]: rest Out[391]: [2, 3, 4, 5, 6] Když je kolekce vnořená, můžeme strukturní vzorec namapovat jen na nejvyšší úroveň... In [392]: a , b = ( 1 , ( 2 , 3 )) In [393]: a Out[393]: 1 In [394]: b Out[394]: (2, 3) ... nebo může vzorec být taktéž vnořený: In [395]: a , ( b , c ) = ( 1 , ( 2 , 3 )) In [396]: a Out[396]: 1 In [397]: b Out[397]: 2 In [398]: c Out[398]: 3 Když dáme destrukturaci dohromady s for-cyklem, zkrátí a zpřehlední se nám zápis: In [399]: for token in věta : word , lemma , tag = token print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Prší', LEMMA: 'pršet', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' A dokonce můžeme ubrat ještě jeden řádek, protože destrukturaci lze provést přímo v rámci hlavičky for-cyklu: In [400]: for word , lemma , tag in věta : print ( f \"WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) WORD: 'Prší', LEMMA: 'pršet', TAG: 'V' WORD: '.', LEMMA: '.', TAG: 'Z' Varianta s vnořeným strukturním vzorcem může nastat např. v případě, že si jednotlivé tokeny očíslujeme pomocí funkce enumerate , která ke každému prvku ve zdrojové kolekci přidá pořadové číslo. Takhle to vypadá, když si výsledek uložíme do seznamu: In [401]: list ( enumerate ( věta )) Out[401]: [(0, ('Prší', 'pršet', 'V')), (1, ('.', '.', 'Z'))] A takhle bychom ji spolu s vnořenou destrukturací mohli použít ve for-cyklu: In [402]: for i , ( word , lemma , tag ) in enumerate ( věta ): print ( f \" { i + 1 } . WORD: { word !r} , LEMMA: { lemma !r} , TAG: { tag !r} \" ) 1. WORD: 'Prší', LEMMA: 'pršet', TAG: 'V' 2. WORD: '.', LEMMA: '.', TAG: 'Z' V prvním opakování for-cyklu provede Python tuto destrukturaci... In [403]: i , ( word , lemma , tag ) = ( 0 , ( \"Prší\" , \"pršet\" , \"V\" )) ... v druhém pak tuto: In [404]: i , ( word , lemma , tag ) = ( 1 , ( \".\" , \".\" , \"Z\" )) Na závěr příklad použití destrukturace s hvězdičkou -- může se hodit, když jsou jednotlivé dílčí kolekce, které postupně ve for-cyklu zpracováváme, různě dlouhé: In [405]: for i , * rest in [( 1 ,), ( 2 , 3 ), ( 4 , 5 , 6 )]: print ( i ) print ( rest ) 1 [] 2 [3] 4 [5, 6] Občas se v rámci for-cyklů můžou hodit klíčová slovíčka break a continue . break okamžitě ukončí for-cyklus. Následující for-cyklus by měl sice teoreticky projít čísla od 0 do 4, ale zastaví se u čísla 3: In [406]: for i in range ( 5 ): # syntax podmínek viz odd. Podmínky if i == 3 : break print ( i ) 0 1 2 continue ukončí současnou iteraci for-cyklu a zahájí další. Následující for-cyklus tedy projde čísla od 0 do 4, ale vytiskne jen ta lichá, protože u sudých se k řádku s funkcí print vůbec nedostane: In [407]: for i in range ( 5 ): if i % 2 == 0 : continue print ( i ) 1 3 POZN.: V tělech for-cyklů jsme všude použili funkci print , abychom mohli nahlédnout do jejich průběhu, tj. do toho, jak se jednotlivé operace opakují. Bez funkce print to jde samozřejmě taky, jen se nám nedostane žádné vizuální zpětné vazby, která by nám pomohla pochopit, co se po spuštění for-cyklu odehrává: In [408]: num = 0 for x in range ( 101 ): num += x In [409]: num Out[409]: 5050 Jen ještě doplním, že elegantněji můžeme čísla od 0 do 100 sečíst pomocí funkce sum : In [410]: sum ( range ( 101 )) Out[410]: 5050 Logika V Pythonu lze pracovat i s logickými výroky. Základem pro to jsou konstanty True a False , které označují pravdu, resp. nepravdu, a které Python vrací, když posuzuje pravdivost nějakého logického výroku: In [411]: # rovnost 2 + 2 == 4 Out[411]: True In [412]: list ( \"abc\" ) == sorted ( \"cab\" ) Out[412]: True In [413]: # negace rovnosti 2 + 2 != 5 Out[413]: True In [414]: # přítomnost prvku v kolekci \"a\" in \"abc\" Out[414]: True In [415]: # přítomnost celého čísla v rozpětí celých čísel 7 in range ( 5 , 10 ) Out[415]: True In [416]: # identita objektu jan = bratr = [ \"nohy\" , \"trup\" , \"hlava\" , \"vlasy\" ] jan is bratr Out[416]: True In [417]: # různé typy nerovností -- menší než 2 < 3 Out[417]: True In [418]: # menší nebo rovno 3 <= 3 Out[418]: True In [419]: # větší 2 > 1 Out[419]: True In [420]: # větší nebo rovno 1 >= 1 Out[420]: True In [421]: 1 < 3 > 2 Out[421]: True In [422]: # výroky o řetězcích \"abc\" . islower () Out[422]: True A tak podobně, viz příklady operací, které vrací True nebo False , v předchozích oddílech. Pravdivostní hodnotu ale mají všechny objekty v Pythonu. Můžeme ji zjistit pomocí funkce bool : In [423]: bool ( \"abc\" ) Out[423]: True Pravdivé jsou skoro všechny objekty kromě: In [424]: # čísla 0 bool ( 0 ) Out[424]: False In [425]: # prázdných kolekcí bool ([]) Out[425]: False In [426]: bool ({}) Out[426]: False In [427]: bool ( \"\" ) Out[427]: False In [428]: # None bool ( None ) Out[428]: False In [429]: # a pochopitelně samotné konstanty False bool ( False ) Out[429]: False Sestavovat z jednoduchých výroků složitější lze pomocí operátorů and , or a not : In [430]: # x and y platí, když jsou oba výroky x a y pravdivé True and True Out[430]: True In [431]: True and False Out[431]: False In [432]: # x or y platí, když je aspoň jeden z výroků x a y pravdivý True or True Out[432]: True In [433]: True or False Out[433]: True In [434]: False or False Out[434]: False In [435]: # not invertuje pravdivostní hodnotu výroku not True Out[435]: False In [436]: not False Out[436]: True V praxi použijeme třeba následovně: In [437]: řetězec = \"kočka\" len ( řetězec ) > 3 and \"č\" in řetězec Out[437]: True Jak jsme psali výše, pravdivostní hodnotu mají všechny objekty v Pythonu, můžeme tedy klidně napsat: In [438]: not \"abc\" Out[438]: False In [439]: True or 0 Out[439]: True Díky tomu můžeme trochu upřesnit chování operátorů and a or . and začne vyhodnocovat výrazy (zleva doprava) a vrátí buď první nepravdivý, nebo poslední zbývající: In [440]: # poslední zbývající (pravdivý) True and 1 and \"abc\" and sorted Out[440]: <function sorted(iterable, /, *, key=None, reverse=False)> In [441]: # první nepravdivý True and 1 and \"abc\" and sorted and [] and 0 Out[441]: [] or taky začne vyhodnocovat výrazy (zleva doprava) a vrátí buď první pravdivý, nebo poslední zbývající: In [442]: # první pravdivý 0 or [] or False or 42 or \"abc\" Out[442]: 42 In [443]: # poslední zbývající (nepravdivý) 0 or [] or False or {} Out[443]: {} Jakmile and nebo or narazí na výraz, podle kterého se může rozhodnout, další už nevyhodnocuje. Můžeme si to ukázat, když jeden z výrazu bude volání funkce, jejímž vedlejším efektem je, že něco vytiskne na obrazovku. In [444]: def funkce (): print ( \"volám funkci...\" ) return \"výsledek funkce\" In [445]: # zde může and vrátit hned první výraz, takže se funkce nezavolá 0 and funkce () Out[445]: 0 In [446]: # zde je potřeba po prvním výrazu pokračovat v ověřování pravdivostních # hodnot, takže se funkce zavolá 1 and funkce () volám funkci... Out[446]: 'výsledek funkce' Pomocí or tedy můžeme např. nahradit metodu setdefault (viz odd. Kolekce -- Slovníky) -- úpravu slovníku provedeme pouze v případě, že daný klíč ještě neobsahuje: In [447]: slovník = dict ( a = 1 , b = 2 ) In [448]: # \"a\" už ve slovníku je, první výraz tedy platí a druhý se ani # nevyhodnotí \"a\" in slovník or slovník . update ( a = 42 ) Out[448]: True In [449]: slovník Out[449]: {'a': 1, 'b': 2} In [450]: # \"c\" ve slovníku není, první výraz tedy neplatí a druhý se # vyhodnotí \"c\" in slovník or slovník . update ( c = 42 ) In [451]: slovník Out[451]: {'a': 1, 'b': 2, 'c': 42} Máme-li kolekci objektů a chceme ověřit, zda je aspoň jeden z nich pravdivý , můžeme použít zabudovanou funkci any : In [452]: any ([ True , False , False ]) Out[452]: True Chceme-li ověřit, zda jsou všechny pravdivé , použijeme zabudovanou funkci all : In [453]: all ([ True , True , True ]) Out[453]: True any a all jsou velmi užitečné v kombinaci s konvertory kolekcí (viz odd. Konvertory kolekcí). Varování na závěr -- složitější logické výroky jsou zrádné, jejich úpravy jsou netriviální a řídí se přesně danými pravidly. Kdo si je podobně jako já ze střední školy pamatuje spíš matně, měl by si dát extra pozor ;) Např. výrok not (x and y) je ekvivalentní výroku not x or not y , ne výroku not x and not y , jak bychom možná mohli mít tendenci naivně \"roznásobit\". Složitější případy si naštěstí vždy lze pro jistotu ověřit pomocí pravdivostní tabulky , ať už si ji nakreslíme ručně nebo si rovnost výroků pro všechny možné kombinace hodnot x a y zkontrolujeme pomocí Pythonu: In [454]: for x in [ True , False ]: for y in [ True , False ]: # ↓ výroky, jejichž rovnost ověřujeme ↓ is_equal = ( not ( x and y )) == ( not x or not y ) print ( f \"x is { x } , y is { y } , the equality is { is_equal } \" ) x is True, y is True, the equality is True x is True, y is False, the equality is True x is False, y is True, the equality is True x is False, y is False, the equality is True Aby rovnost dvou výrazů platila obecně, musí platit pro každou variantu dosazení konkrétních hodnot za proměnné (zde x a y ). O tom se můžeme přesvědčit buď ověřením výstupů funkce print (viz předchozí buňka), nebo můžeme kód taky upravit jako konvertor kolekce v kombinaci s funkcí all a dostaneme tak celkový výsledek rovnou: In [455]: all ( ( not ( x and y )) == ( not x or not y ) for x in [ True , False ] for y in [ True , False ] ) Out[455]: True A ještě pro srovnání, jak by vypadaly výstupy v případě, že si porovnávané výrazy obecně vzato rovné nejsou (všimněte si, že pro některé kombinace hodnot x a y rovnost platí, ale ne pro všechny)... In [456]: for x in [ True , False ]: for y in [ True , False ]: is_equal = ( not ( x and y )) == ( not x and not y ) print ( f \"x is { x } , y is { y } , the equality is { is_equal } \" ) x is True, y is True, the equality is True x is True, y is False, the equality is False x is False, y is True, the equality is False x is False, y is False, the equality is True ... takže celkový verdikt je... In [457]: all ( ( not ( x and y )) == ( not x and not y ) for x in [ True , False ] for y in [ True , False ] ) Out[457]: False Podmínky: if, elif, else (a while-cyklus) Na logických výrocích lze dál stavět větvení programu pomocí podmínek: In [458]: if 2 + 2 == 4 : # hlavička print ( \"matematika funguje\" ) # tělo matematika funguje Syntax podmínek je znovu postavená na principu hlavičky a odsazeného těla, které se vykoná, pokud podmínka uvedená v hlavičce platí. if je jako výhybka: platí-li podmínka, vydá se program trochu jinou cestou, než když neplatí. Alternativních podmíněných cest může být více, napojíme je pomocí klíčového slovíčka elif : In [459]: num = 2 if num == 0 : print ( \"nula\" ) elif num == 1 : print ( \"jedna\" ) elif num == 2 : print ( \"dva\" ) dva Na závěr můžeme přidat ještě jednu nepodmíněnou alternativní cestu pomocí klíčového slovíčka else . Tou se programu vydá, když se ukáže, že ani jedna z předchozích podmínek neplatí: In [460]: num = 3 if num == 0 : print ( \"nula\" ) elif num == 1 : print ( \"jedna\" ) elif num == 2 : print ( \"dva\" ) else : print ( \"něco jiného\" ) něco jiného Je důležité si uvědomit, že větvení skutečně funguje jako výhybka: program se vydá první cestou, kde narazí na pravdivou podmínku, a zbývající ignoruje, i kdyby byly nakrásně pravdivé taky: In [461]: if True : print ( \"podmínka u téhle větve je vždy pravdivá\" ) elif True : print ( \"u téhle taky, ale není to nic platné, je až druhá\" ) podmínka u téhle větve je vždy pravdivá Vizuálně si to můžeme představit takto: .---> if / /-----> elif / --------> elif \\ \\-----{ ... \\ `---> else Důležitým důsledkem je, že záleží na pořadí podmínek . Pravidlo pravé ruky zní, že specifičtější podmínky by měly pro jistotu být na začátku , jinak se k nim program nemusí dostat, protože ho odchytí dřívější méně specifické podmínky. Např. podmínka x >= 0 je méně specifická než podmínka x == 2 , takže když bude před ní, sebere jí všechny potenciální zákazníky: In [462]: for x in range ( 4 ): if x >= 0 : print ( f \" { x } je větší než 0\" ) elif x == 2 : print ( \"HA! DVOJKA.\" ) 0 je větší než 0 1 je větší než 0 2 je větší než 0 3 je větší než 0 Srovnejte s výstupem, když pořadí podmínek prohodíme: In [463]: for x in range ( 4 ): if x == 2 : print ( \"HA! DVOJKA.\" ) elif x >= 0 : print ( f \" { x } je větší než 0\" ) 0 je větší než 0 1 je větší než 0 HA! DVOJKA. 3 je větší než 0 Každé klíčové slovíčko if odstartuje nové větvení, pod něž spadají případná následující elif a else na stejné úrovni odsazení: In [464]: num = 2 if num > 1 : print ( \"1. větvení: num je větší než jedna\" ) elif num < 1 : print ( \"1. větvení: num je menší než jedna\" ) else : print ( \"1. větvení: num je rovno jedné\" ) if num == 2 : print ( \"2. větvení: num je rovno dvěma\" ) 1. větvení: num je větší než jedna 2. větvení: num je rovno dvěma Vizuálně si to můžeme představit takto: .---> if / /-----> elif / --------> elif \\ \\-----{ ... \\ `---> else --------> if Existuje též speciální dvoučlenný operátor if ... else , který jako výsledek vrátí jednu ze dvou možností podle toho, jak dopadne pravdivostní test: výsledek_je_li_test_pravdivý if test else výsledek_je_li_test_nepravdivý Konkrétní příklad bude asi srozumitelnější: In [465]: \"A\" if True else \"B\" Out[465]: 'A' In [466]: \"A\" if False else \"B\" Out[466]: 'B' Díky tomuto operátoru máme k dispozici kompaktnější zápis, když chceme hodnotu nějaké proměnné stanovit na základě pravdivostního testu. Bez operátoru if ... else bychom např. museli psát: In [467]: lednička = { \"puding\" , \"sýr\" , \"salát\" } if \"puding\" in lednička : reakce = \"hurá, puding!\" else : reakce = \"není puding :(\" reakce Out[467]: 'hurá, puding!' Ale s pomocí operátoru if ... else stačí napsat: In [468]: reakce = \"hurá, puding!\" if \"puding\" in lednička else \"není puding :(\" reakce Out[468]: 'hurá, puding!' S podmínkami souvisí i jiná podoba cyklu, tzv. while-cyklus. While-cyklus se opakuje tak dlouho, dokud je podmínka v hlavičce pravdivá: In [469]: i = 0 while i < 3 : print ( \"Číslo\" , i , \"je menší než 3.\" ) i += 1 Číslo 0 je menší než 3. Číslo 1 je menší než 3. Číslo 2 je menší než 3. Konvertory kolekcí Mnoho funkcí pracuje s kolekcemi, např. zabudovaná funkce sorted nebo konstruktor FreqDist z knihovny nltk . Někdy je užitečné mít možnost kolekci trochu upravit ve chvíli, kdy ji takové funkci předáváme. K tomu přesně slouží konvertory kolekce . Kamkoli můžeme dát normální kolekci... In [470]: věta = \"Bylo nás pět .\" . split () věta Out[470]: ['Bylo', 'nás', 'pět', '.'] In [471]: sorted ( věta ) Out[471]: ['.', 'Bylo', 'nás', 'pět'] ... můžeme propašovat místo ní i konvertor: In [472]: sorted ( slovo for slovo in věta if slovo . islower ()) Out[472]: ['nás', 'pět'] Jejich syntax připomíná syntax for-cyklu, jen jsou jednotlivé prvky přeskládané a možnosti jsou omezenější než v plném for-cyklu. Abychom se v jejich zápisu lépe zorientovali, využijeme toho, že uvnitř jakýchkoli závorek můžeme kód v Pythonu libovolně nasekat na řádky a přidat odsazení , aby se nám lépe četl: In [473]: sorted ( slovo for slovo in věta if slovo . islower () ) Out[473]: ['nás', 'pět'] Teď už je lépe vidět, že konvertor kolekce má tři části. Čtou se odprostředka: convert ( item ) # 3. for item in collection # 1. if test ( item ) # 2. Popis jednotlivých fází vypadá následovně: Procházíme kolekci prvek po prvku. Nepovinně můžeme provést nějaký test; pokud ho aktuální prvek nesplní, bude z výsledku vyřazen. Tím můžeme zdrojovou kolekci profiltrovat . Nakonec spočítáme hodnotu, kterou za daný prvek ze zdrojové kolekce zařadíme do výsledku. Tato hodnota může být původní prvek samotný, může být vypočítaná na základě prvku, nebo s ním taky vůbec nemusí souviset. Nejjednodušší konvertor kolekce, kterým kolekce projede nezměněná, vypadá takto: item # 3. for item in collection # 1. # 2. nic In [474]: sorted ( slovo # 3. for slovo in věta # 1. # 2. ) Out[474]: ['.', 'Bylo', 'nás', 'pět'] Chceme-li místo slov samotných do výsledku zařadit n-tici (slovo, délka_slova) , musíme upravit fázi 3: In [475]: sorted ( ( slovo , len ( slovo )) # 3. úprava zde for slovo in věta # 1. # 2. ) Out[475]: [('.', 1), ('Bylo', 4), ('nás', 3), ('pět', 3)] Chceme-li navíc zahodit všechna slova, která nesestávají z malých písmen, musíme doplnit fázi 2: In [476]: sorted ( ( slovo , len ( slovo )) # 3. for slovo in věta # 1. if slovo . islower () # 2. úprava zde ) Out[476]: [('nás', 3), ('pět', 3)] Konvertor kolekce lze vždy přeskládat na normální for-cyklus, např. ten bezprostředně předcházející: In [477]: pomocný_seznam = [] for slovo in věta : if slovo . islower (): pomocný_seznam . append (( slovo , len ( slovo ))) sorted ( pomocný_seznam ) Out[477]: [('nás', 3), ('pět', 3)] Naopak přeskládat for-cyklus na konvertor kolekce pokaždé nejde, protože for-cyklus poskytuje mnohem větší volnost. Výměnou za toto omezení poskytují konvertory kolekce oproti for-cyklům několik výhod: úspornější syntax (nesnaží se pokrýt plnou flexibilitu for-cyklů) při běhu programu zabírají méně času a paměti (např. není potřeba vytvářet žádné pomocné kolekce typu pomocný_seznam ) ve for-cyklu se kvůli jeho flexibilitě snadno může schovat mnohem větší množství chyb Vztah konvertorů kolekcí k for-cyklům je tedy podobný jako vztah (nemodifikovatelných) n-tic k (modifikovatelným) seznamům: konvertory (a n-tice) jsou mnohem méně flexibilní, ale ve chvíli, kdy tu flexibilitu nepotřebujete, je dobré mít možnost se jí explicitně vzdát a uchránit se tak možných chyb, které by z ní mohly plynout. Když není konvertor kolekce jediným argumentem funkce, je potřeba ho uzávorkovat... In [478]: sorted (( slovo for slovo in věta if slovo . islower ()), reverse = True ) Out[478]: ['pět', 'nás'] ... jinak nás Python vyplísní: In [479]: sorted ( slovo for slovo in věta if slovo . islower (), reverse = True ) File \"<ipython-input-479-bd2750ac97af>\" , line 1 sorted(slovo for slovo in věta if slovo.islower(), reverse=True) &#94; SyntaxError : Generator expression must be parenthesized Existuje speciální syntax na to, když chceme výsledky z konvertoru kolekce nasypat do seznamu (angl. se tomuto zápisu říká list comprehension )... In [480]: [ slovo for slovo in věta if slovo . islower ()] Out[480]: ['nás', 'pět'] ... do množiny (angl. set comprehension )... In [481]: { len ( slovo ) for slovo in věta } Out[481]: {1, 3, 4} ... nebo do slovníku (angl. dict comprehension ): In [482]: { slovo : len ( slovo ) for slovo in věta } Out[482]: {'Bylo': 4, 'nás': 3, 'pět': 3, '.': 1} U ostatních kolekcí speciální syntax neexistuje, ale není proč si zoufat, jejich konstruktory většinou podporují inicializaci na základě zdrojového iterovatelného objektu, takže stačí vepsat konvertor kolekce do konstruktoru: In [483]: tuple ( slovo . lower () for slovo in věta ) Out[483]: ('bylo', 'nás', 'pět', '.') In [484]: from nltk import FreqDist FreqDist ( slovo . lower () for slovo in věta ) Out[484]: FreqDist({'bylo': 1, 'nás': 1, 'pět': 1, '.': 1}) Konvertory kolekcí jsou velmi užitečné v kombinaci s logickými funkcemi all a any (viz výše odd. Logika): In [485]: all ( x < 10 for x in range ( 5 )) Out[485]: True In [486]: any ( x == 2 for x in range ( 5 )) Out[486]: True In [487]: all ( x == 2 for x in range ( 5 )) Out[487]: False Konvertorům kolekce též říkáme generátorové výrazy (angl. generator expression ), podle toho, že jejich výsledkem je generátor -- objekt, který generuje další objekty (na základě prvků původní kolekce). Když ho vytvoříme samostatně, můžeme si ho i prohlédnout: In [488]: gen = ( slovo for slovo in věta ) In [489]: gen Out[489]: <generator object <genexpr> at 0x7fea335dd7b0> In [490]: type ( gen ) Out[490]: generator Generátory poslouží kdekoli, kde je potřeba iterovatelný objekt (podobně jako kolekce nebo funkce range ). Navíc z nich lze prvky vytahovat po jednom pomocí zabudované funkce next : In [491]: next ( gen ) Out[491]: 'Bylo' Existují dvě široké kategorie iterovatelných objektů: kolekce a něco, co bychom mohli souhrnně nazvat potenciální či virtuální kolekce (sem patří generátory, výstupy funkcí range , reversed , enumerate apod.) Rozdíl mezi reálnou kolekcí o milionu prvků a potenciální kolekcí o milionu prvků je v tom, že v případě reálné kolekce musí celý milion prvků zároveň fyzicky existovat v paměti počítače. U potenciální kolekce stačí mít recept, jak ten milion prvků vytvořit... až budou potřeba. Často nepotřebujeme všechny prvky kolekce najednou, stačí nám je zpracovávat jeden po druhém. Pak jsou potenciální kolekce ideální -- zabírají mnohem méně paměti. Jindy ale všechny prvky najednou potřebujeme, typicky když si je chceme prohlédnout. Pak nám potenciální kolekce moc neposlouží: In [492]: ( slovo for slovo in věta ) Out[492]: <generator object <genexpr> at 0x7fea2bf89510> In [493]: enumerate ( slovo for slovo in věta ) Out[493]: <enumerate at 0x7fea2bf86c80> In [494]: reversed ([ 1 , 2 , 3 ]) Out[494]: <list_reverseiterator at 0x7fea2bfbfd30> In [495]: range ( - 2 , 2 ) Out[495]: range(-2, 2) Řešení je naštěstí jednoduché -- stačí potenciální kolekci donutit, aby vygenerovala všechny prvky, které v ní dřímají, a uložit je do reálné kolekce, např. do seznamu: In [496]: list ( slovo for slovo in věta ) Out[496]: ['Bylo', 'nás', 'pět', '.'] In [497]: list ( enumerate ( slovo for slovo in věta )) Out[497]: [(0, 'Bylo'), (1, 'nás'), (2, 'pět'), (3, '.')] In [498]: list ( reversed ([ 1 , 2 , 3 ])) Out[498]: [3, 2, 1] In [499]: list ( range ( - 2 , 2 )) Out[499]: [-2, -1, 0, 1] Drobnosti Práce se soubory Nejjednodušeji se v Pythonu pracuje se soubory v podobě tzv. čistého textu (angl. plain text ; často mívají příponu .txt ). K otevření souboru slouží zabudovaná funkce open . Chceme-li do souboru zapisovat, je potřeba specifikovat mód w (jako write ): In [500]: text = \"kočka leze dírou \\n pes oknem\" with open ( \"kočka.txt\" , \"w\" , encoding = \"utf-8\" ) as file : # hlavička file . write ( text ) # tělo Argument encoding je nepovinný, Python ho automaticky stanoví na základě nastavení vašeho operačního systému (v podstatě všude kromě Windows to bude UTF-8). Vzhledem k tomu, že vaše první volba kódování by vždy měla být UTF-8 (viz notebook unicode.ipynb ), je dobré si zvyknout ho vypisovat explicitně a nenechat operační systém rozhodovat za vás. Syntaxi s klíčovým slovíčkem with se říká context manager . V hlavičce zavoláme funkci open a její výsledek uložíme do proměnné file , která reprezentuje otevřený soubor. Tělo je pak kontext, v jehož rámci s tímto otevřeným souborem můžeme pracovat (v našem případě do něj zapisovat). Jakmile kontext skončí (= zrušíme odsazení), Python za nás soubor automaticky zase zavře, aniž bychom museli ručně volat metodu file.close() . Co víc, pokud v rámci těla nastane nějaký problém (chyba), tak Python soubor taky zavře, ještě než zkolabuje, abychom po sobě nenechali nepořádek. Když chceme soubor znovu načíst, můžeme při jeho otevírání specifikovat mód r (jako read ), ale nemusíme, protože je to default. Při načítání máme tři možnosti -- buď načteme celý soubor najednou jako jeden dlouhý řetězec... In [501]: with open ( \"kočka.txt\" , encoding = \"utf-8\" ) as file : text = file . read () text Out[501]: 'kočka leze dírou\\npes oknem' ... nebo celý soubor najednou jako seznam řádků... In [502]: with open ( \"kočka.txt\" , encoding = \"utf-8\" ) as file : lines = file . readlines () lines Out[502]: ['kočka leze dírou\\n', 'pes oknem'] ... nebo ho můžeme zpracovávat řádek po řádku pomocí for-cyklu: In [503]: with open ( \"kočka.txt\" , encoding = \"utf-8\" ) as file : for line in file : print ( line , end = \"\" ) kočka leze dírou pes oknem Poslední varianta se hodí zejména v případě, že máme velký textový soubor, který nepotřebujeme (a tím pádem ani nechceme) načítat celý najednou do paměti. Někdy jsou plaintextové soubory strukturované , takže načítat je řádek po řádku není úplně nejlepší způsob, jak s nimi pracovat. Např. jupyterovské notebooky jsou ve formátu JSON , který se snaží reprezentovat základní datové typy (čísla, řetězce, seznamy a slovníky) tak, aby šly vyměňovat mezi různými programovacími jazyky. Když načteme tento notebook jedním ze standardních způsobů popsaných výše, tahle struktura se nám nevyjeví: In [504]: with open ( \"python_crash_course.ipynb\" , encoding = \"utf-8\" ) as file : nb = file . readlines () # prvních deset řádků souboru s notebookem nb [: 10 ] Out[504]: ['{\\n', ' \"cells\": [\\n', ' {\\n', ' \"cell_type\": \"code\",\\n', ' \"execution_count\": 1,\\n', ' \"metadata\": {},\\n', ' \"outputs\": [\\n', ' {\\n', ' \"data\": {\\n', ' \"text/html\": [\\n'] Vypadne na nás jen seznam řetězců (= řádků). Podle jejich obsahu se zdá, že tam nějaká struktura bude (vidíme odsazení, uvozovky, hranaté a složené závorky), ale nemůžeme s ní nijak pracovat, jediné, co máme k dispozici, jsou řádky textu. Na rozpoznání struktury většiny běžných plaintextových formátů (tzv. parsování ) naštěstí existují knihovny: In [505]: import json with open ( \"python_crash_course.ipynb\" , encoding = \"utf-8\" ) as file : nb = json . load ( file ) Pomocí knihovny json za nás Python naparsuje strukturu textového souboru a převede ji do podoby, s níž umíme pracovat (vnořené kolekce): In [506]: type ( nb ) Out[506]: dict In [507]: nb . keys () Out[507]: dict_keys(['cells', 'metadata', 'nbformat', 'nbformat_minor']) In [508]: nb [ \"cells\" ][ 0 ][ \"source\" ] Out[508]: ['from utils import TableOfContents\\n', '\\n', 'TableOfContents(\"python_crash_course.ipynb\")'] A tak podobně. Pokud budete někdy potřebovat v Pythonu pracovat s jinými typy souborů než s čistým textem (např. Excel, Word atp.), na 99 % půjde vygooglit nějakou knihovnu, která vám s tím pomůže. Třeba s excelovými tabulkami se dobře pracuje pomocí knihovny pandas . Výrazy vs. příkazy Někdy je užitečné mít jemnější terminologické rozlišení pro různé části kódu. Výraz (angl. expression ) je jakýkoli kousek pythonovského kódu, který lze vyhodnotit a jeho výsledek uložit do proměnné. Jinými slovy, výraz je cokoli, co můžeme napsat napravo od operátoru = . Všechno ostatní v Pythonu jsou příkazy (angl. statement ). Příklady: Hlavička for-cyklu ( for x in y: ) je příkaz -- sama o sobě nemá žádný výsledek, který by šel uložit do proměnné. Klíčové slovíčko for ale může být i součástí výrazu, konkrétně generátorového výrazu, jehož výsledkem je generátor: In [509]: ( x for x in range ( 3 )) Out[509]: <generator object <genexpr> at 0x7fea2bfebf90> Podobný rozdíl existuje mezi hlavičkou podmínkového větvení ( if x: , elif y: i else: jsou všechno příkazy) a operátorem if ... else , který je součástí výrazu: In [510]: 42 if False else 0 Out[510]: 0 A do třetice všeho dobrého: přiřazování proměnné (např. x = 3 ) je příkaz tvořený: jménem proměnné (zde x ) přiřazovacím operátorem = a libovolným výrazem (zde číselný literál 3 ) Dokumentace Dokumentace programu slouží k tomu, aby byl čitelný nejen pro Python, ale i pro nás a další programátory. To je důležité, protože když je program nesrozumitelný, těžko se upravuje / opravuje. Složitější místa v kódu si zaslouží komentář . Je ale zbytečné komentovat každou operaci, zejména když je její záměr i výsledek naprosto jasný už z kódu: In [511]: # sečíst dvě jedničky a výsledek uložit do proměnná dva dva = 1 + 1 Funkce, které nejsou jen na jedno použití, by měly mít dokumentační řetězec (angl. docstring ). In [512]: def funkce (): \"\"\"Toto je dokumentační řetězec. První řádek by měl obsahovat stručný popis, k čemu funkce slouží. Další mohou obsahovat detaily, charakteristiku parametrů funkce atp. \"\"\" pass Dokumentační řetězec k libovolné funkci si lze zobrazit pomocí zabudované funkce help ... In [513]: help ( funkce ) Help on function funkce in module __main__: funkce() Toto je dokumentační řetězec. První řádek by měl obsahovat stručný popis, k čemu funkce slouží. Další mohou obsahovat detaily, charakteristiku parametrů funkce atp. ... nebo v prostředí Jupyter pomocí ? : In [514]: funkce ? Signature: funkce ( ) Docstring: Toto je dokumentační řetězec. První řádek by měl obsahovat stručný popis, k čemu funkce slouží. Další mohou obsahovat detaily, charakteristiku parametrů funkce atp. File: ~/src/dlukes.github.io/content/notebooks/<ipython-input-512-8724e3419bb6> Type: function Inspiraci, jak psát užitečné docstringy, doporučuju hledat u funkcí, které sami používáte :) In [515]: sorted ? Signature: sorted ( iterable , / , * , key = None , reverse = False ) Docstring: Return a new list containing all items from the iterable in ascending order. A custom key function can be supplied to customize the sort order, and the reverse flag can be set to request the result in descending order. Type: builtin_function_or_method","tags":"floss","url":"python-crash-course.html"},{"title":"Make the most of Python Jupyter notebooks","text":"Python-based Jupyter notebooks mostly consist of Python code, obviously, and some Markdown text. But they also offer some very handy functions and shortcuts which are not available in Python itself, and which are really helpful for interactive work. This is my personal best of / reference. The shortcuts fall into two groups: magics : special functions with special syntax whose names start with % (in which case they apply to the rest of the line → \"line magics\") or %% (in which case they apply to the entire cell → \"cell magics\") command line programs : if you know how to use command line programs, you can do so directly from the notebook by prefixing the command line invocation with ! If you want to follow along, the easiest way is to just click this link and have Binder launch a Jupyter environment with the notebook loaded for you. Or you can download this post in its original notebook format here and load it into your own Jupyter instance yourself. NB: Most of the following also applies to the IPython REPL . Magics The syntax of magic functions is modeled after the syntax of command line programs: to call them, just write their name and evaluate the cell, without any parentheses (unlike regular Python functions, which are called like this: function() ) arguments are separated just by whitespace (in Python, there are commas: function(arg1, arg) ) some have optional arguments ( options ) which tweak their behavior: these are formed by a hyphen and a letter, e.g. -r Getting help You can read more about the magic function system by calling the %magic magic: In [1]: % magic %quickref brings up a useful cheat sheet of special functionality: In [2]: % quickref If you want more information about an object, %pinfo and %pinfo2 are your friends: In [3]: def foo (): \"This foo function returns bar.\" return \"bar\" In [4]: # shows the object's docstring % pinfo foo In [5]: # shows the full source code % pinfo2 foo These are so handy that they have their own special syntax: ? and ?? , placed either before or after the object's name: In [6]: ? foo In [7]: foo ? In [8]: ?? foo In [9]: foo ?? Of course, this also works with magic functions: In [10]: ? %pylab You can also open a documentation popup by pressing Shift+Tab with your cursor placed in or after a variable name. Repeating the command cycles through different levels of detail. Manipulating objects The appeal of an interactive environment like Jupyter is that you can inspect any object you're working with by just evaluating it: In [11]: foo = 1 In [12]: foo Out[12]: 1 %who and %whos will show you all the objects you've defined: In [13]: % who foo In [14]: % whos Variable Type Data/Info ---------------------------- foo int 1 Sometimes though, these objects are large and you don't want to litter your notebook with tons of output you'll delete right afterwards. (Also, if you forget to delete it, your notebook might get too large to save .) That's when you need to use the Jupyter pager, which lets you inspect an object in a separate window. In [15]: foo = \"This is a line of text. \\n \" * 1000 In [16]: % page foo By default, the pager pretty-prints objects using the pprint() function from the pprint module. This is handy for collections, because it nicely shows the nesting hierarchy, but not so much for strings, because special characters like newlines \\n are shown as escape sequences. If you want the string to look like it would if it were a text file, pass the -r option (\"raw\") to page through the result of calling str() on the object instead: In [17]: % page -r foo If you want to inspect the source code of a module, use %pfile on the object representing that module, or an object imported from that module: In [18]: import os from random import choice In [19]: % pfile os In [20]: % pfile choice Sometimes, you create an object which you know you will want to reuse in a different session or maybe in a completely different notebook. A lightweight way to achieve this is using the %store magic: In [21]: % store foo Stored 'foo' (str) You can list the values stored in your database by invoking %store without arguments: In [22]: % store Stored variables and their in-db values: foo -> 'This is a line of text.\\nThis is a line of text.\\ To restore a variable from the database into your current Python process, use the -r option: In [23]: # restores only `foo` % store -r foo In [24]: # restores all variables in the database % store -r And this is how you clear no longer needed variables from storage: In [25]: # removes `foo` % store -d foo In [26]: # removes all variables % store -z In [27]: % store Stored variables and their in-db values: Working with the file system %ls lists files in the directory where your notebook is stored: In [28]: % ls 3foos.py command_line_intro.ipynb pos_tagging.ipynb zipf.ipynb classification.ipynb foo.py regex.ipynb cmudict.ipynb jupyter_magic.ipynb unicode.ipynb collocations.ipynb libraries.ipynb xcorr_vs_conv.ipynb If you provide a path as argument, it lists that directory instead: In [29]: % ls /etc/nginx conf.d / koi-utf nginx.conf sites-available / uwsgi_params fastcgi.conf koi-win proxy_params sites-enabled / win-utf fastcgi_params mime.types scgi_params snippets / If you provide a glob pattern , then only files that match it are listed: In [30]: % ls /etc/nginx/*.conf /etc/nginx/fastcgi.conf /etc/nginx/nginx.conf %ll (\"long listing\") formats the listing as one entry per line with columns providing additional information: In [31]: % ll ~/edu/ total 12805 drwxrwsrwt+ 3 lukes 2000147 Oct 4 18:52 exchange / drwxrwsr-x+ 21 lukes 2041640 Oct 24 14:44 lukes / drwxrwsr-x+ 9 lukes 2060567 Dec 18 21:54 mda / drwxrwsr-x+ 14 lukes 3001102 Oct 31 17:39 python / drwxrwsr-x+ 4 lukes 2004559 Feb 23 2017 r / drwxr-sr-x+ 4 lukes 2002420 Mar 9 2017 textlink / One of those columns indicates file size, which is great, but they're in bytes, which is less great (hard to read at a glance). The -h option makes the file sizes print in human-readable format: In [32]: % ll -h ~/edu/python/syn* -rw-rw-r--+ 1 lukes 1.5G Nov 1 2016 /home/lukes/edu/python/syn2015.gz -rw-rw-r--+ 1 lukes 112M Nov 2 2016 /home/lukes/edu/python/syn2015_sample %%writefile writes the contents of a cell to a file: In [33]: %% writefile foo.py def foo(): \"This foo function returns bar.\" return \"bar\" Writing foo.py %cat prints the contents of a file into the notebook: In [34]: % cat foo.py def foo(): \"This foo function returns bar.\" return \"bar\" %cat is called %cat because it can also con cat enate multiple files (or the same file, multiple times): In [35]: % cat foo.py foo.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" The output of %cat can be saved into a file with > (if the file exists, it's overwritten): In [36]: % cat foo.py foo.py >3foos.py In [37]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" Hey! Our 3foos.py is one foo short. Let's add it by appending to the file with >> : In [38]: % cat foo.py >>3foos.py In [39]: % cat 3foos.py def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" def foo(): \"This foo function returns bar.\" return \"bar\" There, much better. %less opens a file in the pager (with nice syntax highlighting if it's a Python source file): In [40]: % less foo.py %less is named after the program less , which is used to page through text files at the command line. Why is the original less called \"less\"? Because an earlier pager program was called more (as in \"show me more of this text file\"), and as the saying goes, \"less is more\". (Programmers are fond of dad jokes. I like how this one works on multiple levels -- the literal meaning that less -the-program is intended to replace more -the-program interacts with the figurative meaning that having less is better than having more, and both coalesce into \"use less because it's better than more \".) %cat and %ls are also named after corresponding command line programs. Finding out more about your code When developing, code often behaves differently from what you intended when you wrote it. The following tools might help you find out why. Timing the execution of a piece of code will help you determine if it's slowing you down. The %timeit magic has your back, it runs your code repeatedly and thus provides more reliable estimates. It comes in both line and cell variants. In [41]: % timeit sorted(range(1_000_000)) 61.4 ms ± 7.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) In [42]: %% timeit lst = list(range(1_000_000)) sorted(lst) 68.8 ms ± 7.57 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) The cell variant can include initialization code on the first line, which is run only once: In [43]: %% timeit lst = list(range(1_000_000)) sorted(lst) 28.8 ms ± 4.84 ms per loop (mean ± std. dev. of 7 runs, 10 loops each) If you have the memory_profiler library installed, you can load its magic extension and use %memit in the same way as %timeit to get a notion of how much memory your code is consuming. In [44]: % load_ext memory_profiler In [45]: % memit list(range(1_000_000)) peak memory: 82.43 MiB, increment: 35.71 MiB Peak memory is the highest total amount of memory the Python process used when your code ran. Increment is peak memory minus the amount of memory Python used before your code ran. In [46]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 127.82 MiB, increment: 73.29 MiB In [47]: %% memit lst = list(range(1_000_000)) even = [i for i in lst if i % 2 == 0] peak memory: 98.02 MiB, increment: -30.69 MiB If you have a more involved piece of code where multiple functions are called, you may need more granular information about running times than that provided by %timeit . In that case, you can resort to profiling using the %prun magic. Profiling tells you how fast different parts of your code run relative to each other, in other words, where your bottlenecks are. In [48]: import time def really_slow (): time . sleep ( 1 ) def fast (): pass def only_slow_because_it_calls_another_slow_function (): fast () really_slow () In [49]: % prun only_slow_because_it_calls_another_slow_function() The results show up in the pager, here's a copy: 7 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:9(only_slow_because_it_calls_another_slow_function) 1 0.000 0.000 1.001 1.001 <string>:1(<module>) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} %prun also has a cell variant: In [50]: %% prun really_slow() fast() 6 function calls in 1.001 seconds Ordered by: internal time ncalls tottime percall cumtime percall filename:lineno(function) 1 1.001 1.001 1.001 1.001 {built-in method time.sleep} 1 0.000 0.000 1.001 1.001 {built-in method builtins.exec} 1 0.000 0.000 1.001 1.001 <string>:2(<module>) 1 0.000 0.000 1.001 1.001 <ipython-input-81-8d3b1f67a0d9>:3(really_slow) 1 0.000 0.000 0.000 0.000 <ipython-input-81-8d3b1f67a0d9>:6(fast) 1 0.000 0.000 0.000 0.000 {method 'disable' of '_lsprof.Profiler' objects} Perhaps the most useful magic for development is %debug , which allows you to pause the execution of a piece of code, examine the variables which are defined at that moment in time, resume execution fully or step-by-step etc. You can either pass a statement that you want to debug as argument: In [51]: def foo (): for i in range ( 10 ): print ( \"printing\" , i ) In [52]: % debug foo() NOTE: Enter 'c' at the ipdb> prompt to continue execution. > <string> (1) <module> () ipdb> help Documented commands (type help <topic>): ======================================== EOF cl disable interact next psource rv unt a clear display j p q s until alias commands down jump pdef quit source up args condition enable l pdoc r step w b cont exit list pfile restart tbreak whatis break continue h ll pinfo return u where bt d help longlist pinfo2 retval unalias c debug ignore n pp run undisplay Miscellaneous help topics: ========================== exec pdb ipdb> step --Call-- > <ipython-input-51-e20c0aea6cfb> (1) foo () ----> 1 def foo ( ) : 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 0 ipdb> next printing 0 > <ipython-input-51-e20c0aea6cfb> (2) foo () 1 def foo ( ) : ----> 2 for i in range ( 10 ) : 3 print ( \"printing\" , i ) ipdb> next > <ipython-input-51-e20c0aea6cfb> (3) foo () 1 def foo ( ) : 2 for i in range ( 10 ) : ----> 3 print ( \"printing\" , i ) ipdb> i 1 ipdb> quit Or you can invoke plain %debug after an exception has been raised to jump directly to the place where the error occurred, so that you can figure out why things went wrong: In [53]: def foo (): dct = dict ( foo = 1 ) return dct [ \"bar\" ] In [54]: foo () --------------------------------------------------------------------------- KeyError Traceback (most recent call last) <ipython-input-54-c19b6d9633cf> in <module> () ----> 1 foo ( ) <ipython-input-53-29ed6ce0c4f1> in foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] KeyError : 'bar' In [55]: % debug > <ipython-input-53-29ed6ce0c4f1> (3) foo () 1 def foo ( ) : 2 dct = dict ( foo = 1 ) ----> 3 return dct [ \"bar\" ] ipdb> \"bar\" in dct False ipdb> dct.keys() dict_keys(['foo']) ipdb> quit If you want to pause one of your functions and explore its state at a particular point, set a breakpoint using the set_trace() function from the IPython.core.debugger module. The debugger will be automatically invoked when the call to set_trace() is reached during execution: In [56]: from IPython.core.debugger import set_trace def foo (): for i in range ( 2 ): set_trace () print ( \"printing\" , i ) In [57]: foo () > <ipython-input-56-805417d84ad8> (6) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : 5 set_trace ( ) ----> 6 print ( \"printing\" , i ) ipdb> i 0 ipdb> continue printing 0 > <ipython-input-56-805417d84ad8> (5) foo () 2 3 def foo ( ) : 4 for i in range ( 2 ) : ----> 5 set_trace ( ) 6 print ( \"printing\" , i ) ipdb> i 1 ipdb> continue printing 1 The Python debugger is called pdb and it has some special commands of its own which allow you to step through the execution. They can be listed by typing help at the debugger prompt (see above), or you can have a look at the documentation . The examples above also illustrate what a typical debugging session looks like (stepping through the program, inspecting variables). When you want to stop debugging, don't forget to quit the debugger with quit (or just q ) at the debugger prompt, or else your Python process will become unresponsive. Plotting Jupyter is tightly integrated with the matplotlib plotting library. Plotting is enabled by running the %matplotlib magic with an argument specifying how the notebook should handle graphical output. %matplotlib notebook will generate an interactive plot which you can resize, pan, zoom and more. A word of caution though: when using this variant, once you're done with the plot , don't forget to \"freeze\" it using the ⏻ symbol in the upper right corner, or else subsequent plotting commands from different cells will all draw into this same plot. In [58]: % matplotlib notebook In [59]: import matplotlib.pyplot as plt In [60]: plt . plot ( range ( 10 )) Out[60]: [<matplotlib.lines.Line2D at 0x7ff8e9b94518>] By contrast, %matplotlib inline will just show a basic plot with a default size: In [61]: % matplotlib inline In [62]: plt . plot ( range ( 10 )) Out[62]: [<matplotlib.lines.Line2D at 0x7ff8e9afa358>] For more information on plotting with matplotlib, see their usage guide . Command line programs The operations listed in the section on magics for working with the file system can of course also be achieved using the corresponding command line programs, so if you know those, no need to memorize the magics. In fact, the magics are often just thin wrappers around the command line programs, which is why they are named the same. In [63]: ! ls --color -hArtl /etc/nginx total 56K -rw-r--r-- 1 root root 3.0K Mar 30 2016 win-utf -rw-r--r-- 1 root root 664 Mar 30 2016 uwsgi_params -rw-r--r-- 1 root root 636 Mar 30 2016 scgi_params -rw-r--r-- 1 root root 180 Mar 30 2016 proxy_params -rw-r--r-- 1 root root 3.9K Mar 30 2016 mime.types -rw-r--r-- 1 root root 2.2K Mar 30 2016 koi-win -rw-r--r-- 1 root root 2.8K Mar 30 2016 koi-utf -rw-r--r-- 1 root root 1007 Mar 30 2016 fastcgi_params -rw-r--r-- 1 root root 1.1K Mar 30 2016 fastcgi.conf drwxr-xr-x 2 root root 4.0K Apr 26 2016 conf.d -rw-r--r-- 1 root root 1.5K Apr 27 2016 nginx.conf drwxr-xr-x 2 root root 4.0K Apr 27 2016 sites-enabled drwxr-xr-x 2 root root 4.0K Aug 15 09:21 sites-available drwxr-xr-x 2 root root 4.0K Aug 15 09:21 snippets The only functionality that I miss among the magics is the ability to take a quick look at part of a possibly very large text file. This can be done with the head command line program, which prints the beginning of a file: In [64]: ! head jupyter_magic.ipynb { \"cells\": [ { \"cell_type\": \"markdown\", \"metadata\": {}, \"source\": [ \"# Make the most of Python Jupyter notebooks\\n\", \"\\n\", \"[![Binder](https://mybinder.org/badge.svg)](https://mybinder.org/v2/gh/dlukes/dlukes.github.io/source?filepath=content%2Fnotebooks%2Fjupyter_magic.ipynb)\\n\", \"\\n\", The -n option controls how many lines from the beginning of the file should be printed: In [65]: ! head -n3 jupyter_magic.ipynb regex.ipynb ==> jupyter_magic.ipynb <== { \"cells\": [ { ==> regex.ipynb <== { \"cells\": [ { Similarly, the tail program prints endings of files: In [66]: ! tail -n5 jupyter_magic.ipynb } }, \"nbformat\": 4, \"nbformat_minor\": 2 } Another useful feature of command line execution is that instead of printing the result, you can have it returned as a list of strings corresponding to lines of output. Either by prepending two exclamation marks instead of one: In [67]: !! tail -n5 jupyter_magic.ipynb Out[67]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] Or by assigning the expression to a variable: In [68]: out = ! tail -n5 jupyter_magic.ipynb In [69]: out Out[69]: [' }', ' },', ' \"nbformat\": 4,', ' \"nbformat_minor\": 2', '}'] In summary These are just my favorite shortcuts, the ones I find most helpful. Obviously, there are many more, see %magic or %quickref . If you think I've missed a really neat one, let me know!","tags":"floss","url":"jupyter-magic.html"},{"title":"Text munching in R?","text":"R has been gaining traction as a language for data analysis. My feelings about the whole ecosystem are mixed -- it has some incredibly well-designed libraries and a top-of-the-game IDE , but the core language makes me cringe (it feels like \"Perl and Lisp: The Worse Parts\"). Be that as it may, it has undeniably become the go-to programming language for many people for whom programming is not their main breadwinner, many linguists among them. If you're one of these people and wondering whether it's worth undergoing the cognitive burden of learning another language and having to context-switch between them, read on! The main problem with R and large data is of course that R is fast as long as you can load everything into memory at once and use vectorized operations. The whole point of this post is that with the current size of a typical corpus, you often can't do that. You'll have to process the corpus line by line, which means using a for-loop, and these are notoriously slow in R. I'm not pretending this is some new discovery (it's not), I'm just trying to quantify how problematic this slowness is for processing large quantities of text (prohibitive, in my opinion), so that you don't have to figure it out for yourself and can get started learning Python 3 right away instead ;) (Another big problem is that R doesn't have an efficient and versatile hash table data structure.) tl;dr If you're planning to process corpora of hundreds of millions of tokens or more -- spoiler alert, you probably shouldn't do it in R. Task OK, I've specifically complained about R being bad at for-loops and hashes. Let's devise a task that'll show exactly how bad. At the same time, I don't mean to come up with anything particularly convoluted or far-fetched. So here goes: we'll be trying to build a per-part-of-speech frequency distribution of lemmas (dictionary headword forms) from a 120M-token corpus. If you've ever done any linguistic data analysis, I hope you'll agree it's a pretty basic and common task. The corpus data consists of lines with tab-separated word form, lemma and tag fields, plus some additional lines with metadata which do not contain tabs. We'll be skipping those. Here's a glimpse of the corpus format: <!-- WORD LEMMA TAG --> Hladina hladina NNFS1-----A----- jezera jezero NNNS2-----A----- , , Z:-------------- mrtvá mrtvý AAFS1----1A----- a a J&#94;-------------- černá černý AAFS1----1A----- , , Z:-------------- The first character of the tag indicates the part of speech. For each part of speech, we want to create a separate frequency distribution, i.e. we want to be able to say, for instance, the most frequent noun is X, followed by Y, whereas the most frequent adjective is Z etc. This should nicely exercise all of R's weak spots. Let's get to it! R: 1 day (!) / 15 hours (see EDIT below, but still !) After quite some exploration of various alternatives for the individual subtasks, this is the program that I came up with: library ( stringi ) library ( hash ) # read input from STDIN con <- file ( \"stdin\" , open = \"rt\" ) pos_sets <- hash () start <- Sys.time () while ( TRUE ) { line <- readLines ( con , n = 1 ) if ( length ( line ) == 0 ) { break } # lines with tokens (as opposed to metadata) contain tabs if ( stri_detect_fixed ( line , \"\\t\" )) { # individual token attributes are tab-separated attrs <- stri_split_fixed ( line , \"\\t\" ) # for each token, we're interested in the lemma (headword)... lemma <- attrs [[ 1 ]][ 2 ] # ... and the part-of-speech, which is the first character of the tag tag <- attrs [[ 1 ]][ 3 ] pos <- stri_split_boundaries ( tag , n = 2 , type = \"character\" )[[ 1 ]][ 1 ] # build a per-part-of-speech frequency distribution as a nested hash: # { # \"noun\": { # \"cat\": 5, # \"dog\": 3, # ... # }, # \"verb\": { # \"look\": 2, # ... # }, # ... # } if ( is.null ( pos_sets [[ pos ]])) { pos_sets [[ pos ]] <- hash () } if ( is.null ( pos_sets [[ pos ]][[ lemma ]])) { pos_sets [[ pos ]][[ lemma ]] <- 1 } else { pos_sets [[ pos ]][[ lemma ]] <- pos_sets [[ pos ]][[ lemma ]] + 1 } } } # report running time diff <- Sys.time () - start cat ( sprintf ( \"Done in %g %s.\\n\" , diff , units ( diff )), file = stderr ()) Given the input corpus mentioned above, this code takes 1.33 days to run. Compared to other languages one might conceivably use (see below), this is just ridiculous. Now, I'm certainly not an expert in R, so there may be better ways of doing some of this. But I doubt such improvements, if any, would be of any practical relevance, because even reducing the running time to a tenth of the original duration wouldn't be enough. And if there is a way to go even further, say to a hundredth, which would begin to make R competitive, then I would argue that a language which lets you shoot yourself so spectacularly in the foot performance-wise if you're not hip to some clever tricks should just be avoided for tasks where said performance matters. EDIT: Replacing stri_split_boundaries(tag, n=2, type=\"character\")[[1]][1] above with stri_sub(tag, from=1, to=1) , you can cut the running time down to 15 hours. That's still way too much in comparison with the competitors, and just reinforces one of the points made below: there's often no default and efficient way of doing some basic operations (like string manipulation) in R. This is in great part due to R's emphasis on vectorization, which leads to a proliferation of subtly different functions designed for doing subtly different kinds of vectorized passes over data. Good luck trying to remember them all. And if you pick the wrong one (cf. stri_split_boundaries() vs. stri_sub() ) -- because there are just too many similar ways of achieving the same result and too much documentation to read before you even begin to see what you should use -- you get penalized heavily. This is very programmer-unfriendly design. Contrast this with the Zen of Python : \"There should be one -- and preferably only one -- obvious way to do it.\" With these general considerations out of the way, let's look at some details of how to implement this task in R. In many cases, it's unclear how you should even approach the problem in R, due to missing or confusing built-in functionality . As a result, in addition to having a lousy running time on this task, R also puts a strain on the programmer's time. Reading the data This sounds so basic it should be obvious, right? Not so fast. First of all, the file(\"path/to/file\") function creates a file connection, which is however not open unless you also specify a mode in the open= argument, or alternatively, unless you call the open() function on the connection. Why you would want to create a connection that's not open is beyond me, but R adds insult to injury by allowing readLines() to work on a closed connection: it just opens the connection before doing the reading and closes it afterwards. This means that repeated calls to readLines(unopened_connection, n=1) will repeatedly read the first line of the file , which is most likely not what you want. This is API design level PHP. Second, the corpus is gzip compressed, so you'll need to uncompress it. There are basically two options: have an external program ( zcat ) do the decompression and pipe the data into R via STDIN handle the decompression within R itself As a general rule (for any language), it will always be faster to handle the decompression in a different process on a multi-core system, because the tasks can proceed in parallel. 1 On the other hand, it's more portable not to depend on external programs, and R does have a built-in function to open a connection to a gzipped file, namely gzfile() . Based on tests on shorter inputs, it's about 50% slower than external decompression, which is a somewhat worse performance deterioration than e.g. Python 3 (40% based on the full input). In light of the already dire running time, it's something we can't really afford. Third, having to do the line-by-line reading in a while (TRUE) loop, using a function called readLines() (note the plural) with an argument of 1 , checking the length of the resulting character vector in order to determine the end of the input -- that's just gross. String manipulation R has built-in functions for string matching ( grepl() et al.), not so much for string splitting. This is the point where I got suspicious of the performance of everything and started testing alternatives. I finally ended up using the stringi package, which is fast and has a fairly consistent API. stringr is a set of higher-level wrappers around it, which have however proven somewhat slower than the built-ins in my highly informal testing. O hash map, where art thou? Building a per-part-of-speech frequency distribution of headwords requires an appropriate data structure. As indicated in the comments in the R source, we want to build a nested collection that looks something like this: { \"noun\": { \"cat\": 5, \"dog\": 3, ... }, \"verb\": { \"look\": 2, ... }, ... } The requirements on the data structure we need are the following: it's a collection strings can be used as keys it can be arbitrarily nested key lookup is fast, i.e. constant time In other words, we need a hash (or a dict, in Python terminology). R doesn't have a hash (I'll qualify this statement in a bit). The workhorse data structure in R that satisfies points 1--3 is a list. Unfortunately, it has linear access time . That's not going to work. R also has environments, which it uses to store and access variables. Under the hood, environments are implemented as hashes, but using them as such is a massive pain, because their API isn't meant for it. Fortunately, there's a wrapper package which makes it more convenient. Unfortunately, environments weren't optimized with this use case in mind. They were designed to hold key--value (variable name--variable value) pairs explicitly defined by people as part of their programs, not millions of items extracted from data. As a result, they slow down dramatically once the number of items grows large. (The article in the previous link provides a survey of the state of the art of fast key lookup in R. The state of the art is... dismal. Your only option is basically indexing a data table, which is fine for a finalized data set, but useless when building the data set -- you can't afford to reindex after each new data point.) There's also the hashmap library , which is a wrapper around C++ Boost hashes. However, it doesn't do nesting, so it's of no use to us, and of very limited usefulness in general. Conclusion: technically, we have to concede that R has hashes, but for all practical intents and purposes, it doesn't . There's one last twist, though. Funnily enough, in our use case, it turns out it doesn't really matter anyway. Indeed, it seems the performance of for-loops in R is so egregiously bad that it dwarfs even the inefficiencies accrued by the linear lookup time of lists: if you reimplement the script with lists, it takes just a little longer than the version with hashes, about 1.36 days. (Or maybe it's just that the performance of environment-based hashes becomes so bad when they grow large as to be comparable with that of lists? Who knows, and frankly, I don't care enough to want to find out. If it's the for-loops though, then adding efficient hashes to R won't really solve anything.) EDIT: With stri_sub() substituted for stri_split_boundaries() as detailed above, the code using lists runs in about 1.28 days, which is a much smaller improvement than in the case of the code using hashes (1.33 days → 15 hours). Summary If you like R and your reaction to this is, \"That's not fair! R was never meant to do any of this, that's why everything feels so backhanded.\" -- then good, that's basically the gist of this post: don't use R for something it wasn't meant to do . What are the alternatives, then? Task The following details an informal test comparing the speed of R, Python 3, Rust and Perl at processing a large corpus file (~120M tokens, 1.5GB gzipped) and creating a frequency distribution of headwords per part-of-speech. The idea is to see whether R is a viable alternative in this domain, or whether the slowing down caused by the inability to use vectorized computations (because we can't load the entire thing into memory at once) will just be too much. Python 3: 5 minutes Yes, that's right. It takes Python 3 5 minutes to do the same task that took R over a day . The code feels a lot simpler too: import sys import time def main (): pos_sets = {} start = time . time () for line in sys . stdin : if \" \\t \" in line : _ , lemma , tag , _ = line . split ( \" \\t \" , maxsplit = 3 ) pos = tag [ 0 ] # this is an intentionally naive implementation which mimicks # the R code and something an inexperienced coder might do; # a more concise and probably better performing solution could # be achieved using dict.setdefault() or collections.defaultdict # / collections.Counter if pos not in pos_sets : pos_sets [ pos ] = {} if lemma not in pos_sets [ pos ]: pos_sets [ pos ][ lemma ] = 1 else : pos_sets [ pos ][ lemma ] += 1 diff = time . time () - start print ( f \"Done in { diff : .0f } seconds.\" , file = sys . stderr ) if __name__ == \"__main__\" : main () FYI, this was run using Python 3.6. As a rule, use always the most recent version of Python 3 you can (at least 3.5, 3.4 in a pinch; with earlier releases, you may encounter performance issues). In any case, do not use Python 2 for new projects and let it end-of-life in peace. Perl: 13 minutes Perl used to be a popular alternative for text processing. Like R, it has its fair share of nauseating language design and weird quirks, but since it was actually meant for use in this domain, it won't spectacularly let you down. (Unless your data is silently corrupted because you handled text encoding wrong. Perl's behavior in this respect is a relict of a pre-UTF-8-everywhere past, and it's the single biggest reason for why the language should be put out of its misery already.) Here's the code: use strict ; use utf8 ; use open qw(:std :encoding(utf8)) ; my $start = time (); my %pos_sets = (); while ( <> ) { if ( /\\t/ ) { my @attrs = split /\\t/ ; my $lemma = @attrs [ 1 ]; my $tag = @attrs [ 2 ]; my $pos = substr $tag , 0 , 1 ; # auto-vivification: ergonomic, but also made possible by the whole # \"implicit defaults that have a potential of screwing stuff up # without you even knowing about it\" culture of Perl $pos_sets { $pos }{ $lemma } += 1 ; } } my $diff = time () - $start ; print STDERR \"Done in $diff seconds.\\n\" ; Bottom line though, being more than twice as slow as Python 3 (which came as a surprise to me, I must admit) and definitely the worse language, it has little to recommend itself if you're considering to learn a new language for this type of task. Except maybe if you want to continuously log what the program is doing to a terminal -- like output the number of lines processed after each line. Perl is clearly very efficient at writing to a terminal, the running time is basically the same with continuous logging incorporated. By contrast, Python 3 takes about three times longer (~ 15 minutes). (I guess maybe Python flushes output after each print() call, whereas Perl does some smart buffering which results in it not being slowed down by the latency of the terminal...? Who knows, at any rate, it's hardly a \"killer\" feature.) Rust: 1.25 minutes As a compiled, systems-level language, Rust is in a different league compared to the previous contestants: of course it's going to be faster. I included it because it provides a frame of reference. The important takeaway is that we're in the same ballpark with Python 3 (roughly units of minutes), so there's no pressing need to turn to a compiled language for this task. Here's the code, for completeness sake: use std :: io ; use std :: io :: prelude :: * ; use std :: collections :: HashMap ; use std :: time ; type LemmaCount = HashMap < String , i32 > ; type PosSet = HashMap < char , LemmaCount > ; fn main () { let start = time :: SystemTime :: now (); let mut pos_sets = PosSet :: new (); let stdin = io :: stdin (); for line in stdin . lock (). lines () { let line = line . unwrap (); if line . contains ( \" \\t \" ) { let mut attrs = line . split ( \" \\t \" ). skip ( 1 ). take ( 2 ); let lemma = attrs . next (). unwrap (); let tag = attrs . next (). unwrap (); let pos = tag . chars (). take ( 1 ). next (). unwrap (); let pos_set = pos_sets . entry ( pos ). or_insert ( LemmaCount :: new ()); let count_for_lemma = pos_set . entry ( String :: from ( lemma )). or_insert ( 0 ); * count_for_lemma += 1 ; } } let diff = start . elapsed (). unwrap (). as_secs (); println! ( \"Done in {:.0} seconds.\" , diff ); } Note in passing how nicely the Rust code reads for a compiled language. Of course, since it's a much stricter (and safer) language than Python, it's more ceremonious to write and the APIs are more complicated, because they have to adhere to the various memory management guarantees Rust gives you (among other things). But once the code is written, it's very readable and clear. And all necessary functions and data structures are (a) available in the standard library, and (b) plenty efficient. Conclusion Just to be clear: the ultimate purpose of this post is not bashing R (not for being slow at text munching, at any rate); it's to give a convincing account of why it's just not the right tool for the job. And not in a small way, either -- in a way that requires to learn a different tool, there's no way around it. Let me reiterate that my recommendation would hands down be Python 3 . Once the data is extracted, go back to R by all means. Although Python does have a fairly nice high-level data analysis library , it's not my intention to discourage anyone from using R for what it is good at, especially if this is a skill they are already proficient in. The internet is full of people asking advice on which programming language to learn, and the answers are invariably evasive -- it depends on your tastes, what fits your brain better, what your use case is. In the hopes that some people might find opinionated guidance useful for a change (I know I personally often do, when flirting with a new language): if you're looking to process large quantities of text data, the answer is a big, resounding NOT R ! A vectorized postscript Since I ran these on a server with 64 GB of RAM, I figured I might as well try loading everything into memory in R and doing it the proper, vectorized way, while I'm at it. Here's the code, using dplyr : library ( dplyr ) library ( stringi ) start <- Sys.time () con <- file ( \"stdin\" , open = \"rt\" ) corpus <- readLines ( con ) diff <- Sys.time () - start cat ( sprintf ( \"Corpus read in after %g %s.\\n\" , diff , units ( diff ))) corpus <- stri_subset_fixed ( corpus , \"\\t\" ) corpus <- stri_split_fixed ( corpus , \"\\t\" , simplify = TRUE ) freq_dist <- tibble ( POS = stri_sub ( corpus [, 3 ], from = 1 , to = 1 ), LEMMA = corpus [, 2 ] ) %>% group_by ( POS , LEMMA ) %>% summarize ( FREQ = n ()) diff <- Sys.time () - start cat ( sprintf ( \"Finished processing corpus after %g %s.\\n\" , diff , units ( diff ))) Let me say at the outset that this code looks much nicer -- it's clean, modern R, made possible in great part by Hadley Wickham's efforts to redesign the data manipulation vocabulary from the ground up. Note also that we've made a concession on our requirements: the resulting data structure is a tibble, not a hash, i.e. key lookup time is not constant but depends on the size of the data. Well, just loading the corpus into memory took ~18 minutes. The script then ran for several days , in the course of which I checked every now and then to see how much memory it was using: ~35 GB. I don't suppose anyone has that much RAM on their laptop. Then someone rebooted the server before the program could complete. I think you'll agree the experiment is conclusive even so. You could also offload the decompression to a different thread in the same process, but that complicates the implementation. Piping gives you parallelization basically for free. ↩","tags":"ling","url":"text-munching-in-r.html"},{"title":"Monkey-patching in R","text":"While building a Shiny application with R recently, I've come across the need to invert the filterRange() function in the DT package, which provides a convenient high-level way to add DataTables to your Shiny app. As indicated by its name, this function filters a numeric column in your datatable based on a range, so as it contains only values contained within that range . What I needed was the opposite: include values outside the specified range . The filtering is done server-side and unfortunately, no option is provided out-of-the-box to perform this inversion. One of the solutions is therefore to monkey-patch the filterRange() function in the DT R package, replacing it with a version that filters the outer range instead. Googling for \"monkey patching r\" (currently) yields this blog post , which provides a more complicated though arguably cleaner solution, which introduces a new environment in the search path. My position on this is that if you're worried about cleanliness, you shouldn't be monkey-patching in the first place. Conversely, if you decide monkey-patching is acceptable in your situation, the code required should be as quick and dirty as the thought. Of course, this is R, uncontested king of weird ways of doing anything but the most common data analysis tasks, and even some of those -- so it's never going to be as simple as Python, for instance: import sys sys . stdin = \"foo\" # Aaand done. But it doesn't have to be as complicated as the solution in the blog post above, either. The solution presented here is basically taken from this mailing list post , which has the disadvantage of not containing the key term \"monkey-patch\", which makes it hard to find on Google. It consists in the following steps: Get a handle on the relevant library's namespace with getNamespace() . Make the relevant binding modifiable with unlockBinding() . Define your custom version of the function. Store it in the namespace under the original name. Re-seal everything with lockBinding() . Here's the code for my specific use case with DT::filterRange() : # Monkey patch the filterRange() function in the DT package so that server-side filtering returns # values *outside* the range instead of inside. DT <- getNamespace ( \"DT\" ) unlockBinding ( \"filterRange\" , DT ) #################################################################################################### # This part of the code is deliberately kept as similar to the original as possible, in order to # make potential updates easier. See https://github.com/rstudio/DT/blob/v0.2/R/shiny.R#L474. # filter a numeric/date/time vector using the search string \"lower ... upper\" filterRange = function ( d , string ) { if ( ! grepl ( '[.]{3}' , string ) || length ( r <- strsplit ( string , '[.]{3}' )[[ 1 ]]) > 2 ) stop ( 'The range of a numeric / date / time column must be of length 2' ) if ( length ( r ) == 1 ) r = c ( r , '' ) # lower, r = gsub ( '&#94;\\\\s+|\\\\s+$' , '' , r ) r1 = r [ 1 ]; r2 = r [ 2 ] if ( is.numeric ( d )) { r1 = as.numeric ( r1 ); r2 = as.numeric ( r2 ) } else if ( inherits ( d , 'Date' )) { if ( r1 != '' ) r1 = as.Date ( r1 ) if ( r2 != '' ) r2 = as.Date ( r2 ) } else { if ( r1 != '' ) r1 = as.POSIXct ( r1 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) if ( r2 != '' ) r2 = as.POSIXct ( r2 , tz = 'GMT' , '%Y-%m-%dT%H:%M:%S' ) } if ( r [ 1 ] == '' ) return ( d <= r2 ) if ( r [ 2 ] == '' ) return ( d >= r1 ) d <= r1 | d >= r2 } # End pastiche of original DT code. #################################################################################################### DT $ filterRange <- filterRange lockBinding ( \"filterRange\" , DT ) EDIT, 2021-04-13: As Vincent Wolowski points out in the comments, you can also call the higher-level function assignInNamespace , which does the unlock/relock dance for you behind the scenes: assignInNamespace('filterRange', filterRange, 'DT') . The last piece of the puzzle concerns UX: the user should understand that the filter applies to the outer range, not the inner one. Visually: This is easily achieved with a few lines of CSS: # datatable-id . noUi-background { background : #3FB8AF ; box-shadow : inset 0 0 3 px rgba ( 51 , 51 , 51 , .45 ); transition : background 450 ms ; } # datatable-id . noUi-connect { background : #FAFAFA ; box-shadow : inset 0 1 px 1 px #f0f0f0 ; } In conclusion, monkey-patching is rarely the most elegant, debuggable and maintainable solution to a problem you're having. More often, it's actually the least elegant (etc.) one. But every once in a while, it's the simplest one, the one with the best hassle/reward ratio (until it comes back to bite you once your codebase has grown or assumptions about the monkey-patched code have changed). At any rate, if you need to resort to it, it's nice to have a quick, googlable how-to, hence this post.","tags":"floss","url":"monkey-patching-in-r.html"},{"title":"\"Responsive\" iframes, e.g. for DokuWiki and Shiny","text":"Sometimes, the best way to embed an interactive element into a website is to use an iframe. Obviously, not when your website is a webapp and that element represents the main functionality it's supposed to provide -- that would be gross. But when your website is mostly textual / graphical content, typically authored within a wiki or blogging platform, and you just want to include this one element to liven it up, iframes are actually a decent (and perhaps the only?) solution. Trouble is, you probably want this Frankenstein monster to actually look good, i.e. seamless if at all possible. But iframes don't have automatic vertical resizing according to their content, which means you'll need to take care of that manually. How? By using the JavaScript messaging API for communication between parent and child frames to send information about window resize events (from parent to child) and height updates (from child to parent). Let's imagine you have a DokuWiki article in which you want to embed a small Shiny app. If you just embed it in your dokuwiki code using an iframe, taking care to remove the border and stretch it horizontally... < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > </ html > ... this will happen: Eww, scrollbar. Messaging to the rescue! First of all, you need to teach your embedded web page to send information about its height to the parent at appropriate times. This can be achieved by adding this piece of JavaScript to it: ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // set this to a selector for the element that contains the entire UI // you want to access via the iframe -- for a Shiny app, it might be // a div with Bootstrap's container-fluid class var containerSelector = \".container-fluid\" ; // this should be the root URL of the parent frame (DokuWiki) which you want // to allow to send messages to the child var allowedOrigin = \"https://dokuwiki.example.com\" /////////////////////// // END CONFIGURATION // /////////////////////// function sendHeightOf ( querySelector ) { var container = document . querySelector ( querySelector ); if ( container . scrollHeight !== undefined ) { var h = container . scrollHeight ; parent . postMessage ( h , \"*\" ); } else { console . log ( \"No element corresponding to querySelector \" + querySelector + \" found, or element did not have property scrollHeight.\" ); } } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen for resize message from parent window (see point ② below) eventer ( messageEvent , function ( e ) { if ( e . origin == allowedOrigin ) { sendHeightOf ( containerSelector ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }); window . onload = function () { // inform parent at least once after load (see point ① below) sendHeightOf ( containerSelector ); // monitor self-initiated changes in size (see point ③ below) var mo = new MutationObserver ( function () { sendHeightOf ( containerSelector ); }); mo . observe ( document , { subtree : true , childList : true , characterData : true }); }; })(); What are these \"appropriate times\" mentioned above? The code above implements the following ones, which should be generic enough to cover most situations: on initial page load on window resize (see below, the parent frame has to send a message to the child frame that it has been resized, to which the child responds with a size update message) on any kind of mutation of the DOM inside the child frame (not a full reload of the entire page, that would be handled by point ① above), which might affect the size of the rendered component On the parent (DokuWiki) side, you then need to handle the incoming size update messages from the child frame, and send resize messages when the window is resized. This can be achieved with the following DokuWiki markup: < html > < iframe id = \"embedded-app\" src = \"https://your.shiny.app/url\" frameborder = \"0\" width = \"100%\" ></ iframe > < script > ( function () { //////////////////////////////////////////// // CONFIGURE THESE TO MATCH YOUR USE CASE // //////////////////////////////////////////// // this should be the root URL of the child frame (Shiny app) which you want // to allow to send messages to the parent var allowedOrigin = \"https://your.shiny.app\" /////////////////////// // END CONFIGURATION // /////////////////////// var embeddedApp = document . getElementById ( \"embedded-app\" ); function resizeIframe ( pixels ) { embeddedApp . style . height = pixels + \"px\" ; } // cross-browser compatible infrastructure var eventMethod = window . addEventListener ? \"addEventListener\" : \"attachEvent\" ; var eventer = window [ eventMethod ]; var messageEvent = eventMethod == \"attachEvent\" ? \"onmessage\" : \"message\" ; // listen to message from iframe eventer ( messageEvent , function ( e ) { if ( e . origin === allowedOrigin ) { var key = e . message ? \"message\" : \"data\" ; var data = e [ key ]; resizeIframe ( data ); } else { console . log ( \"Was expecting a message from \" + allowedOrigin + \", got \" + e . origin + \" instead.\" ); } }, false ); // send message to iframe on window resize window . onresize = function () { embeddedApp . contentWindow . postMessage ( \"parentWindowResized\" , \"*\" ); }; })(); </ script > </ html > And the result? Yay! And of course, the iframe gets resized as needed when display conditions change: Cue bittersweet feeling after having figured out a workaround for such a specific use case that you're not quite sure it was worth putting all that effort into it in the first place...","tags":"floss","url":"responsive-iframe.html"},{"title":"The Cathedral and the Bazaar: What is a Useful Notion of \"Language\"?","text":"If you like the essay, then you'll definitely want to take a look at Luc Steels's The Talking Heads Experiment: Origins of Words and Meanings . It's published as an open-access book by Language Science Press, so go grab the free download ! Abstract The essay analyzes why Noam Chomsky's notion of language (both its essence — language as a set of grammatical sentences — and genesis) leads neither to interesting discoveries nor even to useful questions from the point of view of linguistics as a science. A much more fruitful approach to language is to view it as a complex, dynamic, distributed system with emergent properties stemming from its functions, as advocated e.g. by Luc Steels. The argument will be developed against the backdrop of the evolution of Ludwig Wittgenstein's thought, from the Tractatus to the concept of language games, i.e. from an approach to language based on thorough formal analysis but also misconceptions about its functions, to a much keener though less formal grasp of its praxis and purpose. Introduction At least since Thomas Kuhn's The Structure of Scientific Revolutions , it has been a fairly commonplace notion that working within the confines of a particular scientific paradigm conditions to a certain extent the questions one is likely to ask and therefore also the answers that ensue. This effectively limits the range of possible discoveries, because some are not answers to meaningful questions within a given framework while other observations still are taken as given axioms, which means they cannot be the target of further scientific investigation. In contemporary linguistics, one very prominent such paradigm is that of generative grammar , single-handedly established in 1957 by Noam Chomsky in his seminal work Syntactic Structures . While serious criticism has been leveled over time against this initial exposition as well as Chomsky's subsequent elaborations on it (see Pullum 2011; Sampson 2015; and Sampson 2005 for a book-length treatment), the book undeniably attracted significant numbers of brilliant young minds under the wings of its research program, which went from aspiring challenger in the domain of linguistics to established heavyweight in a comparatively short period of time (the transition had been achieved by the mid-1970s at the latest). In the process, it co-opted or spawned various other sub-fields of linguistics, and even rebranded itself, such that Cartesian linguistics , cognitive linguistics and most recently biolinguistics are all labels which suggest a strong generativist presence. One serious competitor to the Chomskyan account of language that has emerged over the years is the field of evolutionary linguistics . It might seem strange at first glance why biolinguistics and evolutionary linguistics should be at odds. As their names indicate, they both aspire to a close relationship with biology, which seems to indicate their research agendas and outlooks should largely overlap. Yet their fundamental assumptions about what constitutes language are so irreconcilable that they might as well be considered to deal with different objects of study. Of the two, it is evolutionary linguistics which leads to questions and investigations which can be conceived of as scientific (in the Popperian sense of involving falsifiable hypotheses instead of being merely speculative), consequently yielding the most useful insights – in the fairly pedestrian sense that these can be intersubjectively replicated without resorting to an argument from authority, which makes them a better foundation to build upon, because the superadded structures are less likely to crumble should said authority ever change their mind, as Chomsky has done several times already. Wittgenstein on language: From logical calculus to language games Let us now take a short détour through the development of Ludwig Wittgenstein's thoughts on language, so that we may couch our later discussion of the differences between generative grammar / biolinguistics and evolutionary linguistics in terms of a contrast that is perhaps more familiar. The imagery in the title of the present essay was borrowed from Eric S. Raymond's book The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary (Raymond 1999). In it, Raymond describes two models of collaborative software development, one of them very rigid, restrictive and hostile to newcomers (the \"cathedral\"), the other overwhelmingly inclusive, open to outside contributions and organic change, an effervescent hive of activity (the \"bazaar\"), whose unexpected but empirically demonstrable virtues he has come to embrace. This architectural metaphor also happens to be very apt when characterizing Wittgenstein's view of language in the two major stages of his thought, as represented by his two books Tractatus Logico-Philosophicus and Philosophical Investigations . In the Tractatus , Wittgenstein has a \"preconceived idea of language as an exact calculus operated according to precise rules\" (McGinn 2006, 12) and formalizing this system of rules leads him to the following dogmatic conclusion: \"what can be said at all can be said clearly, and what we cannot talk about we must pass over in silence\" (Wittgenstein 2001, 3). The deontic force of the final injunction should be taken with a grain of salt; it could perhaps be rephrased in the following less epigraph-worthy manner: there is a sharp logical boundary to be drawn between meaningful and nonsensical propositions, and the purpose of language is to construct meaningful ones, therefore it is futile (rather than strictly forbidden) to engage in nonsensical ones. There have been attempts to read the Tractatus in an ironic mode, as a consciously doomed, self-defeating attempt to circumscribe the limits to the expression of thought, which prefigures the much more subtle attitude towards language that Wittgenstein later exhibits in the Philosophical Investigations (see McGinn 2006, 5–6 and elsewhere for an overview of this so-called \"resolute\" reading). In my opinion, such a stance exhibits a blatant, possibly wilful disregard of his almost penitent tone in the preface to Philosophical Investigations : \"I could not but recognize grave mistakes in what I set out in that first book\" (Wittgenstein 2009, 4e). Where does Wittgenstein think he went wrong then? Arguably, the most serious misconception was conferring a privileged ontological status to language, seeing it as \"the unique correlate, picture, of the world\" (Wittgenstein 2009, 49e), whereas in fact, these referential properties are highly dependent on communicative context. It signifies only insofar as it has an effect on the addressee (another human being, or even myself) which to all practical intents and purposes the speaker can identify as somehow related to what she was trying to achieve with her utterance in the first place. But there is little meaning, in any practical sense, outside these highly particular, localized (in both physical and cultural space and time), embodied, grounded interactions. Wittgenstein calls these interactions \"language-games\" to \"emphasize the fact that the speaking of language is part of an activity, or of a form of life\" (Wittgenstein 2009, 15e, original emphasis). Of course, the notion of game still involves some kind of rules, but by focusing on the activity rather than its regulations, it is much easier to account for \"the case where we play, and make up the rules as we go along […] and even where we alter them – as we go along\" (Wittgenstein 2009, 44e). This is not to say that we cannot construct abstractions, although Wittgenstein himself is clearly in favor of systematically examining particular cases: \"In order to see more clearly, here as in countless similar cases, we must look at what really happens in detail , as it were from close up\" (Wittgenstein 2009, 30e). However, once we do abstract away, it is crucial to approach the resulting theory from a pragmatic standpoint: \"We want to establish an order in our knowledge of language: an order for a particular purpose, one out of many possible orders, not the order\" (Wittgenstein 2009, 56e, original emphasis). Generative grammar In many ways, Chomsky conceives of language as the early Wittgenstein did, i.e. under the cathedral metaphor. This may come across as a surprise because unlike Wittgenstein, he is not concerned with issues of meaningfulness, philosophical or otherwise. Indeed, it is one of his fundamental precepts that grammar can and should be dissociated from meaning, as demonstrated by his famous example sentence \"Colorless green ideas sleep furiously\", which he claims is perfectly grammatical yet meaningless 1 (Chomsky 2002, Chap. 2). Nevertheless, the goal for both is to describe language per se , in the abstract, without regard to its context-grounded use in actual communication. Both strive to give a highly formal definition of the system they think they are uncovering: while Wittgenstein attempts to establish a logical calculus of how propositions can be said to carry meaning in terms of their referential relationship to external reality, Chomsky tries to hint at a calculus which would determine which candidate sentences belong to the language encoded by this calculus, i.e. separate those that are grammatical from those that are not. Another way to put this is that the descriptive part of linguistics can be equated with formal language theory: a language is viewed as a (potentially infinite) set of symbol strings (sentences), and the linguist's task is to find the simplest and most elegant set of rules that would constitute the basis for a procedure to generate (hence generative grammar) all of them and only those, whether observed or potential. Chomsky himself gives the following definition: \"by a generative grammar I mean simply a system of rules that in some explicit and well-defined way assigns structural descriptions to sentences\" (Chomsky 1965, 8). Furthermore, \"Linguistic theory is concerned primarily with an ideal speaker-listener, in a completely homogeneous speech-community, who knows its language perfectly\" (Chomsky 1965, 3). Specifically, it is concerned with his \" competence (the speaker-hearer's [intrinsic] knowledge of his language)\", not his \" performance (the actual use of language in concrete situations)\" (Chomsky 1965, 4). Additionally, no claim is made as to the cognitive or neurophysiological accuracy of the mechanisms described, although to hedge his bets both ways, Chomsky adds that \"No doubt, a reasonable model of language use will incorporate, as a basic component, the generative grammar that expresses the speaker-hearer's knowledge of the language\" (Chomsky 1965, 9). In short, Chomsky consciously sets up the playing field for a thoroughly mentalistic, speculative discipline. At first, it might seem like reasonable approximation and a small concession to make, especially in the face of the sheer daunting complexity of all the intricate mechanisms that conspire to yield the phenomenon we call language, but only until one fully realizes the consequences of such a move. Observe for instance the carefully crafted loophole claiming that linguistics is primarily concerned with an ideal speaker-hearer's competence and that actual usage data is just circumstantial evidence. This effectively allows linguists to dismiss inconvenient edge cases or counterexamples to their theories purely on grounds of their being noisy data or slips of the tongue, which is something they are allowed to determine based on introspection. Mind you, this is not just a theoretical loophole; Chomsky himself has repeatedly relied on it, especially with respect to so-called linguistic universals (see e.g. Sampson 2005, 139 or 160). The net result is rampant, unchecked theorizing. One such example is the postulation of a two-layer linguistic analysis, the observed language data corresponding to a surface structure which provides hints as to an underlying, more regular deep structure to be uncovered. The deep structure is purportedly closer to the universal properties of language; both layers are linked by a system of transformations: We can greatly simplify the description of English and gain new and important insight into its formal structure if we limit the direct description in terms of phrase structure to a kernel of basic sentences (simple, declarative, active, with no complex verb or noun phrases), deriving all other sentences from these (more properly, from the strings that underlie them) by transformation, possibly repeated. (Chomsky 2002, 106–7) Constructing a formal framework for grammar modeling with cognitively unmotivated levels of abstraction might have been a valid goal (though arguably not within linguistics) if the result was indeed, as Chomsky claims it to be, maximally elegant, as simple as can be but no simpler. I was not able to track down a formal definition of this criterion, but simplicity is clearly discursively construed as a desirable quality: \"simple and revealing\" (Chomsky 2002, 11) or \"effective and illuminating\" (Chomsky 2002, 13) are Chomsky's choice epithets for what to look for in a grammar. But that is not true either: transformations are an unnecessary addition, singling them out as a separate category of operations adds nothing to the generative power of his system (Pullum 2011, 290). They are therefore a wart under any reasonable definition of \"simplicity\" and Chomsky thus manages to fall short of even the self-defined, theory-internal standards that are the only ones he allows his enterprise to be held to. Ontogeny and phylogeny As we have seen, generative grammar concerns itself with an ideal speaker-hearer's competence in a perfectly homogeneous community. The trouble is that such an impoverished model eschews any possibility of dynamism. The very dichotomy between grammatical and ungrammatical is intuitively problematic if we consider that judgments are bound to diverge when made in reference to different dialects, sociolects and idiolects, 2 not to mention that binary classification might be too reductive in some cases (how would you categorize, on first encounter, a construction which you passively understand but would never produce actively?). Though Chomsky sometimes mentions in passing the possibility of levels of grammaticalness which would allow a finer-grained analysis (e.g. Chomsky 2002, 16; Chomsky 1965, 11), it seems to be just another instance of hedging his bets, he never makes it a fundamental component of his theory. This would seem to indicate that Chomsky's theory of language has a serious problem in that it is unable to account for any phenomena that involve fluctuations in linguistic ability, including language emergence / diachronic change (phylogeny) and acquisition (ontogeny). Chomsky's response to this is that our language faculty is largely innate: we are genetically endowed with a language-acquisition device in our brains (Chomsky 1965, 31–33) which can supposedly infer the correct grammatical rules even given incomplete, limited and noisy input, which is what Chomsky argues children get (the \"poverty of stimulus\" argument), thanks to strong universal constraints on what a human language can be like. Under this account, the capacity for language, initially \"a language of thought, later externalized and used in many ways\" (Chomsky 2007, 24), appeared as a random mutation in a single individual and progressively spread through the population because it offered a considerable competitive advantage: \"capacities for complex thought, planning, interpretation\" (Chomsky 2007, 22). In his later career, Chomsky increasingly focused on exploring this purported shared genetic basis for human language, hence the aforementioned label \"biolinguistics\". Make no mistake, this in no way entails a turn from mentalism towards empirical neurophysiological or genetic investigation. Quite to the contrary, liberated from the constraints of having to account for individual existing languages in detail, he soars to new heights of abstractness in postulating the formal language underpinnings of human language. The principles and parameters model of Universal Grammar (Chomsky 1986) expands upon the notion of linguistic universals by splitting them up into two sets: principles, which are hardwired and immutable, and parameters, which are hardwired too but can be flipped on or off based on linguistic behavior observed by the child in her particular language community. Since the choices are heavily constrained, the learner can infer correct parameter settings in spite of deficient input. This line of research culminates in the so-called minimalist program (Chomsky 1995), where Chomsky identifies the \"core principle of language, [the operation of] unbounded Merge\" (Chomsky 2007, 22). Under the \"strong\" minimalist hypothesis, this would be the only principle necessary to account for human-like languages (Chomsky 2007, 20), which would paradoxically essentially discard all work (or should I say speculation?) previously done on the parameters side of the Universal Grammar project. All other universal characteristics of language could then be explained by newly introduced \"interface\" conditions, 3 i.e. constraints on how language inter-operates with other systems, including thought and physical language production (Chomsky 2007, 14); 4 all empirically documented variation between the world's languages would be chalked up to lexical differences (Chomsky 2007, 25). The poverty of stimulus and language universals arguments for innateness have been thoroughly debunked, especially in Geoffrey Sampson's book-length diatribe The ‘ Language Instinct ' Debate . In short, it turns out that some of the grammatical constructions which were assumed to be absent from a language learner's input yet acquired nonetheless have since been empirically proven to occur fairly commonly (Sampson 2005, 72–79). Moreover, there is no qualitative difference between a statement like \"the stimulus is too poor to allow language learning without a genetic basis\" and \"the stimulus is just rich enough etc.\", both are unverifiable unless we have already independently proven that language learning occurs one way or the other, so to adduce either of the statements as proof for the hypothesis at stake is misguided (Sampson 2005, 47–48). Finally, the alleged language universals turn out to be either false when checked against additional languages (Sampson 2005, 138–9) or so general as to be meaningless (Sampson 2005, Chap. 5). Irrespective of this, let us suppose for a moment that genetic mutation and subsequent inheritance do play a role in the emergence of language, and work out an account of language emergence consistent with this hypothesis. If language started out through mutation in a single individual as a purely internal advanced conceptualization faculty, then once it started to spread, what was the motivation for the genetically-endowed humans to externalize their thoughts? How did they know to which of their peers they could speak (which had inherited the mutation) and which not? And most importantly, how did they know which parameters of Universal Grammar to flip on and which off, if there was no prior language based on which to decide? Universal Grammar would have had to be fairly detailed in order for intersubjective agreement on the norms for the first ever human language to be reached on the basis of it alone. Yet as we have seen, Chomsky has been moving away from this notion – at the limit, the minimalist program posits only one very general mechanism required for language. The poverty of stimulus argument is turned against its creator as the argument from poverty of the machinery supposed to make up for the poverty of said stimulus. Alternatively, if we fully subscribe to the minimalist program and the notion that all the surface variety exhibited by language comes from the lexicon, then how are individual words created, how do they propagate? One might be tempted to say \"people just invented them\", but consider for a while that in the current setup, there is absolutely no mechanism that would explain how a community of speakers reaches agreement on their lexicon – this theory offers no incentive whatsoever for consensus to be reached; from its point of view, a solution where each speaker ends up with their own private lexicon is equally valid because indistinguishable on the basis of the theory's conceptual apparatus. Chomsky's ideas on phylogenesis appear thoroughly ridiculous when fully carried out to their logical consequences, and this can all be blamed on his sterile, idealized and static view of language which dismisses actual communication as a secondary purpose and therefore a peripheral issue. On a side note, it is hard to say which aspect of Chomsky's theory of language came first – whether innateness accommodated the mentalism and the concomitant quest for formal purity (botched as it may be) of generative grammar, whether it was the other way round, or whether they perhaps co-evolved in his mind. The facts are that Chomsky's initial publications on generative grammar concentrate on the formal language theory part (Chomsky 1956; Chomsky 2002), but he added the innateness argument fairly early on, even tacking a seemingly respectable philosophical lineage onto it in Cartesian Linguistics (Chomsky 2009), which pretends to trace back both innateness and mentalism to Descartes and the Port-Royal grammarians, binding them as two sides of the same coin. It is worth noting that in both formal language theory and history of linguistics / philosophy, Chomsky is more of a dabbler than an expert: he has provably borrowed most of his ideas in the former field from others, sometimes mangling them or extending them in unfortunate ways (Pullum 2011; Sampson 2015), and has thoroughly underresearched (or wilfully twisted?) his understanding of the latter, which has resulted in serious misrepresentations of the history of ideas (Miel 1969; Aarsleff 1970). Evolutionary linguistics There are various sub-fields of linguistics which are in discord with generative grammar, especially over the notion that performance data should be used only as evidence for guiding the speculation and detailed usage and frequency patterns should be disregarded; the primacy of syntax (as advocated by Chomsky) is also disputed. One of these sub-fields is obviously corpus linguistics, which takes a decidedly empiricist stance and starts by assembling a large body of language data (a corpus) from which patterns of language use are inferred. Nevertheless, not all of these compete with generative grammar at the fundamental explanatory level of how language came about phylogenetically and how it is transmitted by ontogenetic acquisition. We have repeatedly encountered Chomsky's emphasis on how communication, actual interactions between speakers, are just an afterthought in the system of language: evolutionary biologist Salvador Luria was the most forceful advocate of the view that communicative needs would not have provided \"any great selective pressure to produce a system such as language,\" with its crucial relation to \"development of abstract or productive thinking.\" His fellow Nobel laureate François Jacob (1977) added later that \"the role of language as a communication system between individuals would have come about only secondarily, as many linguists believe,\" (Chomsky 2007, 23) Part of this vehemence dovetails with the single individual mutation hypothesis of the origins of language – it helps if the significance of communication is downplayed in an account where communication is initially impossible, simply because there is no other language-endowed being to communicate with. If communication were language's killer feature, then the selective pressure for the incriminated gene to propagate would not kick in. The other part can reasonably be attributed to Chomsky's intent to make a clean break from a prior popular theory on language acquisition, epitomized by B. F. Skinner's 1957 monograph Verbal Behavior , which offered a heavily empiricist, behaviorist account of language learning in terms of a stimulus-response cycle. Characteristically, Chomsky's strategy is to trivialize the function of the stimulus, casually implying both that it might not be needed at all, and if it is, then details of the role it plays are of little interest: it would not be at all surprising to find that normal language learning requires use of language in real-life situations, in some way. But this, if true [sic!], would not be sufficient to show that information regarding situational context (in particular, a pairing of signals with structural descriptions that is at least in part prior to assumptions about syntactic structure) plays any role in determining how language is acquired, once the mechanism is put to work and the task of language learning is undertaken by the child. (Chomsky 1965, 33) In retrospect, Skinner's account may be simplistic in many ways, but the basic notion that one has to pay attention to stimuli and responses in the course of particular linguistic interactions is sound. In particular, a theory of language built on this foundation successfully copes with all of the impasses we have explored above regarding Chomsky's approach. One such framework is that of evolutionary linguistics. Evolutionary linguistics views language as a complex adaptive system with emergent properties (Steels 2015, 8–9). A complex adaptive system is a system which is not centrally organized, coordinated or designed: its \"macroscopic\" characteristics are said to \"emerge\" as the result of localized interactions between individual entities (agents) with similar \"microscopic\" characteristics (be they physical, behavioral or motivational). The whole is more than the sum of its parts, and none of the agents can be properly said to have designed the system, nor can they deliberately change it in an arbitrary way; but all are continuously shaping it by taking part in the interactions that constitute its fabric. Examples of complex adaptive systems include the dynamics of insect societies (beehives, ant nests) or patterns of collective motion in large animal groups (flocks of birds or shoals of fish). These and more are discussed in much greater depth in the first chapter of Pierre-Yves Oudeyer's book Self-Organization in the Evolution of Speech . Adaptiveness is a property that these systems acquire by virtue of not being hardwired on the macro level: they are defined functionally instead of structurally. If the conditions in the environment change, the system will adapt to keep fulfilling its function, because the agents are forced to modify their behavior in order to achieve their individual goals. Of course, they may fail to do so, in which case the system breaks down and ceases to exist. If we revert to the metaphor from the title of the present essay, according to Chomsky, language is a cathedral erected by a single unwitting architect, the random genetic mutation that endowed us with the language faculty. Conversely, Luc Steels and fellow evolutionary linguists argue that the apparent macroscopic orderliness of language is the result of a myriad interactions of multiple individual agents, as suggested by the the bazaar image. One form that linguistic research can take under this paradigm is formulating and running computational models which simulate the behavior of agent populations and study the microscopic conditions, i.e. the cognitive and physical abilities, motivations etc. of each agent, necessary for a system like language to emerge within the population and stabilize. By direct inspiration from Wittgenstein's Philosophical Investigations , the interactions between agents are termed \"language games\" (Steels 2015, 167–8); depending on the topic being investigated, the agents can play different types of language games with different rules. It is openly acknowledged that such simulations represent only a limited approximation of a well-defined subspace of the actual uses of language. In the research to date, rules are generally definite and set for the entire experiment, but simulating language games with fuzzy rules remains a perfectly valid research topic within this framework, in the Wittgensteinian spirit of allowing rules to be made up and modified \"as we go along\" (Wittgenstein 2009, 44e). A relatively simple game that agents can play is the so-called Guessing Game (see Chap. 2 of Steels 2015 for more details). In this scenario, a population of agents, embodied in physical robots, tries to establish a shared lexicon and coupled ontology for a simple world consisting of geometrical shapes. Each game is an interaction of two agents picked at random, in the context of a scene consisting of said geometrical shapes. One agent (the speaker) takes the initiative, selects a topic from the scene and names it; the other (the hearer) tries to guess which object the first one had in mind and points to it; the speaker decodes the pointing gesture and the game succeeds if he interprets it as referencing his original topic. If so, he acknowledges the match; otherwise, he points at the intended topic as a repair strategy. At the outset, neither the lexicon nor the ontology are given, only a set of sensors and actuators (which allow the agents to interact with the environment by taking in streams of raw perceptual data or producing sound and pointing gestures) and very general cognitive principles. These include an associative memory and feedback mechanisms to propagate failures and successes in conceptualization and communication to all components of the system and act on them. 5 New distinctions along the perceptual dimensions are introduced in a random fashion, 6 but those that lead to a successful unambiguous selection of a topic and communicative success are strengthened over the course of many interactions, while useless ones are dampened by lateral inhibition and eventually pruned. At the same time, speakers create new words for concepts that are as of yet missing from their lexicon, and hearers may adopt them into theirs for their conceptualization of the topic the speaker points at in case of failure. A similar feedback mechanism then ensures that highly successful words are preferred and come to dominate within the speech community. It is important to realize that at no time do the individual agents share the same ontology or lexicon: newly introduced distinctions and words are random and unique for each agent, agents simply gradually learn which of these are useful in achieving communicative success, which means that they naturally settle on ontologies and lexicons that are close enough to those of others in the population. This barely scratches the surface of how all these notions must be orchestrated for a working computer implementation of this model, not to mention the even more elaborate agent-based language game modeling experiments that are already being conducted, investigating for instance the emergence of grammar (see Part III of Steels 2015 for an overview of recent scholarship). We see that even a seemingly simple task like establishing a shared conceptualization of reality and agreeing on names for these concepts is a complex endeavor which relies on a highly sophisticated (though also highly general) machinery. Another key observation is that while agent-based models can be fully virtual, grounding them in physical reality (cf. the use of robots with sensors and actuators) brings additional challenges that enable researchers to reach vital insights which would otherwise be impossible. In particular, grounding introduces fuzziness on the sensory input channels (by virtue of different points of view for the two robots and analog-to-digital conversion) which the agents must cope with, or else the mechanisms they were endowed with cannot be considered as constituting a plausible, sufficient model of the dynamics of human language. Unlike in generative grammar, anything that is transient, imperfect, is eminently included in the purview of linguistic inquiry. Failures are very much part of the dynamics that steer the evolution of language. How could it adapt to the speakers' changing requirements if it did not include appropriate repair strategies? Indeed, how could it be bootstrapped at all? The reward is a model that successfully simulates not only language emergence, but also transmission: if virgin agents are added into an existing population, they gradually acquire its language (see Fig. 1). Figure 1. Communicative success in a population of agents with a steady influx of virgin agents and outflux of old ones (overall population size remains the same). The game starts in phase 1 with 20 virgin agents; phases 2 and 4 show the behavior of the model at an agent renewal rate of 1/1000 games, whereas phase 3 corresponds to a heavier rate of 1/100. (Figure from Steels 2015, 121.) Crucially, the drive to communicate, to interact, is built into the agents, they just keep playing games as long as they can. But if it was not, the simulation would have to be more complex and somehow elicit this drive by introducing appropriate ecological constraints, e.g. by requiring co-operation as a survival strategy (Steels 2015, 106). Otherwise, the agents would have no motivation to strive for communicative success in their mutual encounters, they would fail to reach intersubjective alignment of their conceptual spaces and lexicons, and language would not emerge. In other words, far from being an afterthought, successful communication with a partner, grounded in an external context, turns out to be a fundamental requirement to establish the kind of dynamics which allow languages to appear. Paradoxically, since it concerns itself with simulations and computational models, this branch of evolutionary linguistics is, like generative grammar, also highly speculative. However, unlike generative grammar, it is a kind of speculation which considers guidance by empirical observations a necessity, not a nuisance. Furthermore, simulations are meant to be tested: if an agent-based model of language emergence fails to converge on the result stipulated for that particular experiment, the model is plain wrong and the dynamics it is trying to put into place (cognitive strategies, feedback propagation etc.) need to be revised. There is thus a clear-cut criterion for validity. Lastly, even if a simulation works, an accompanying debate as to whether the mechanisms involved are actually plausible approximations of reality is considered an integral part of hypothesis evaluation, with evidence from strongly empirically grounded disciplines like biology and neurophysiology a vital element in the process. Conclusion Taking a cue from Wittgenstein's Philosophical Investigations , this essay should not be construed as an attempt to replace one doctrine with another, but to advocate a \"change of attitude\" (cf. McGinn 2013, 33) which allows asking more meaningful questions about language. This being said, on the evidence presented above, it is hard not to conclude that Noam Chomsky is fundamentally mistaken about the corrective that is necessary for language learning to take place. According to Chomsky, the criterion for evaluating linguistic rules lies within a dedicated language organ we are genetically endowed with; the innate structures themselves embody the metric by which conjectures pertaining to linguistic rules will be judged. By contrast, in the evolutionary linguistics perspective, genetics provide innate structures which are capable of random growth, but the feedback (reinforcement and pruning) which results in steering this growth in a particular direction comes from interactions with the environment. This theory presupposes much less specificity in the hardware infrastructure which makes this possible and so should be preferred both on grounds of simplicity and flexibility of the model, not to mention that it is biologically plausible and has been empirically verified to work. In the context of science, Chomsky's rhetorical strategy in and of itself is dishonest: he preaches formal rigor while practicing sleight of hand, and casually retreats to increasingly abstract ground on reaching an impasse. He thus carves out a region in discursive space which has no corresponding equivalent in a logically consistent conceptual space, without which a piece of discourse can hardly constitute a scientific theory. In other words, much like his famous example sentence \"Colorless green ideas sleep furiously\", his discourse is grammatical but largely nonsensical under the requirements on a system of thought which aspires to mirror reality in a coherent fashion. Requirements on scientific discourse notwithstanding, we as linguists should keep in mind that language in general is much more than a system for encoding logical propositions. Even Wittgenstein had to resign himself to the fact – or perhaps knew all along – that the Tractatus , which he framed as the ultimate solution to all metaphysical controversies, could only fan the flames of philosophical debate. It was after all addressed to a diverse community of people bound perhaps exclusively by their penchant for elaborate language games. References Aarsleff, Hans. 1970. \"The History of Linguistics and Professor Chomsky.\" Language 46 (3): 570–85. Chomsky, Noam. 1956. \"Three Models for the Description of Language.\" ———. 1965. Aspects of the Theory of Syntax . Cambridge, MA: The M.I.T. Press. ———. 1986. Knowledge of Language: Its Nature, Origin, and Use . Convergence. New York, Westport, London: Praeger. ———. 1995. The Minimalist Program . Cambridge, MA: The MIT Press. ———. 2002. Syntactic Structures . 2nd ed. Berlin, New York: Mouton de Gruyter. ———. 2007. \"Of Minds and Language.\" Biolinguistics , no. 1: 9–27. ———. 2009. Cartesian Linguistics: A Chapter in the History of Rationalist Thought . 3rd ed. Cambridge: Cambridge University Press. McGinn, Marie. 2006. Elucidating the Tractatus: Wittgenstein's Early Philosophy of Logic and Language . Oxford: Oxford University Press. ———. 2013. The Routledge Guidebook to Wittgenstein's Philosophical Investigations . The Routledge Guides to Great Books. Routledge. Miel, Jan. 1969. \"Pascal, Port-Royal, and Cartesian Linguistics.\" Journal of the History of Ideas 30 (2): 261–71. Oudeyer, Pierre-Yves. 2006. Self-Organization in the Evolution of Speech . Translated by James R. Hurford. Oxford, New York: OUP. Pullum, Geoffrey K. 2011. \"On the Mathematical Foundations of Syntactic Structures .\" Journal of Logic, Language and Information 20: 277–96. Raymond, Eric S. 1999. The Cathedral & the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary . O'Reilly Media. Sampson, Geoffrey. 2005. The \"Language Instinct\" Debate . 3rd ed. London, New York: Continuum. ———. 2015. \"Rigid Strings and Flaky Snowflakes.\" Language and Cognition 10: 1–17. Skinner, B. F. 1957. Verbal Behavior . The Century Psychology Series. New York: Appleton – Century – Crofts. Steels, Luc. 2015. The Talking Heads Experiment: Origins of Words and Meanings . Computational Models of Language Evolution 1. Berlin: Language Science Press. Wittgenstein, Ludwig. 2001. Tractatus Logico-Philosophicus . Routledge Classics. London, New York: Routledge. ———. 2009. Philosophical Investigations . Chichester, United Kingdom: Blackwell Publishing. Let us pretend for a moment that language games like \"poetry\" or the surrealist pastime of cadavre exquis do not exist; in these, the quoted sentence could appear as perfectly valid and meaningful, though perhaps not in the sense that Chomsky intended. \"Meaningful\" in a late-Wittgensteinian perspective could be paraphrased as \"accepted by at least one involved party as a valid turn within the context of a particular language game\". ↩ Arguing that this does not matter because we should be concerned with the ideal speaker-hearer's competence just takes us further down the impasse, because now we have to determine how to delimit the purported \"ideal\". ↩ This is the beauty of building empirically unmotivated, purely speculative theories: at any moment, one can freely accommodate a new element into the existing framework, substituting novelty and amalgamation for critical evaluation. ↩ Cf. also Sampson's riposte : \"If complex properties of some aspect of human behaviour have to be as they are as a matter of conceptual necessity, then there is no reason to postulate complex genetically inherited cognitive machinery determining those behaviour patterns\" (Sampson 2015, 9). ↩ This interconnected architecture stands in stark contrast to Chomsky's deliberately isolationist approach: \"the relation between semantics and syntax […] can only be studied after the syntactic structure has been determined on independent grounds\" (Chomsky 2002, 17). ↩ Wittgenstein only hints at the problem of conceptualization, but he is prescient in realizing it is not a given: \"The primary elements [of the objects which constitute the world in this particular language game] are the coloured squares. ‘But are these simple?' – I wouldn't know what I could more naturally call a ‘simple' in this language-game. But under other circumstances, I'd call a monochrome square, consisting perhaps of two rectangles or of the elements colour and shape, ‘composite'\" (Wittgenstein 2009, 27e). ↩","tags":"ling","url":"cathedral-and-bazaar.html"},{"title":"Configuring Emacs Daemon on Mac OS X","text":"I know I promised this article a loooong time ago (June 2014, when I first got a Mac, to judge by the previous timestamp in the header of this file), but since the historically attested readership of this blog is 2 + a bunch of my facebook friends who I nagged to read my attempt at explaining character encodings to non-technical people , I don't suppose it's as if a legion of fans have been restlessly looking forward to this one ;) Nevertheless, the distinct advantage is that my OS X Emacs setup has had the opportunity to grow more mature and also much simpler in the meantime, which means that if a third reader accidentally stumbles over this note (exploding my ratings...), they might actually find something genuinely useful here. tl;dr This article presents a way to start Emacs Daemon (a persistent Emacs session) from the GUI and subsequently connect to it (creating frames on demand) using an Automator script . The benefit is that you incur startup time lag only once (when you start the daemon) while still being able to close all frames when you're not using Emacs, keeping a clean workspace . This is especially useful if your Emacs is heavily customized and loading it takes a while . Another benefit is that whenever you open a frame connected to an Emacs daemon, all your previously open buffers are still there as you left them (as opposed to opening a fresh instance of Emacs). Skim over the code blocks to get the important gist without the verbose sauce. Tested on OS X 10.11 El Capitan, with Homebrew Emacs and Spacemacs config. Why Emacs Daemon, why this post Installing Emacs on a Mac in and of itself is not that much of a problem -- there are several options, ranging from Homebrew and Macports to Emacs for Mac OS X , Emacs Mac Port and Aquamacs . The last two in this list have some OS X specific tweaks (smooth scrolling, tabs, adapted keyboard shortcuts), which makes them perhaps more appealing out of the box but also less extensible, as some of the information out there about generic Emacs might not apply to them as straightforwardly or indeed at all. With that in mind, if you want to tinker with your Emacs config, it's a good idea to stick with Homebrew's fairly conservative version of Emacs: $ brew update $ brew install emacs --with-cocoa # this step gets you a standard OS X launcher icon $ brew linkapps emacs But now that you've got Emacs, and especially if you're transferring some heavy customization over from say Linux, you might be unhappy that each time you start it from cold, it takes a while, typically a few seconds. That's what emacs --daemon and emacsclient are for: Emacs is run as a daemon in the backround and you connect to it with client frames that spawn almost instantly . This also means that you can close all existing frames to keep your workspace clean if you won't be using Emacs for a while (hard to imagine, right, since you can even read xkcd from inside Emacs ) and then whip up a frame at the speed of a thought when need arises. Now this is all easy to achieve when using the terminal , but since you probably bought that Mac in great part for its shiny pretty elegant ergonomic GUI, you might want Emacs to use GUI frames instead of terminal ones and connect to the Emacs daemon (or start it if it's not running) by just clicking on an app icon in the launcher or finding it from Spotlight. That's where Automator comes in. An Automator script Automator is a built-in OS X app for creating custom automated user workflows for just about any installed app you might have or even OS functionality. Among other things, this means that it allows you to wrap the daemon auto-start functionality available from the terminal (as described in the previous paragraph) into an app launchable from the GUI. Let's get down to business: Launch Automator and create a new document. Select Application as its type. Search the Actions palette on the left for the Run Shell Script action and add it to your Automator document. In the Run Shell Script building block, change the following: set Shell to the shell you're using and whose init files have thus the PATH correctly set to the emacs and emacsclient executables (if you're using Homebrew, it probably told you how to properly set up your PATH as a post-install step) set Pass input to \"as arguments\" (if you then set this Automator app as the default for opening a given type of file , you'll be able to use emacsclient to open files by double-clicking on them in Finder) Finally, paste in the following code snippet and save the app e.g. as EmacsClient.app , preferably in your Applications folder so that it is easily accessible from the launcher. emacsclient --no-wait -c -a emacs \" $@ \" >/dev/null 2 > & 1 & EDIT : An earlier version of this article had nohup prepended to the command above; as pointed out in the comments by MaTres (thanks!), this is unnecessary . At the end of the day, your Automator EmacsClient.app should look something like this: The core of the command that you might want to tweak based on your particular Emacs setup is emacsclient --no-wait -c -a emacs ; mine is optimized to work with mostly stock Spacemacs config (see below). If it doesn't work, you might also want to try a simple emacsclient -c -a \"\" and variations; a good debugging technique is to try these out in the terminal: as soon as you get the line working there, it'll start working in the Automator task as well. \"$@\" is just the list of files (if any) passed to Emacs to open (the aforementioned double-click in Finder use case). The rest is some black magic to ensure that the shell which spawns the Emacs process (because this Automator app is after all, at heart, only a shell script) totally and utterly disowns it, so that the shell script is allowed to return and the Automator task completes as soon as Emacs has started (or the client has spawned a new frame). Otherwise, you'd end up with an irritating spinning cog wheel in your notification area which would stay there until you completely quit Emacs. Which is probably not what you want, since you're undergoing all this hassle in the first place to get a zen, distraction-free Emacs experience. The details of the various incantations are discussed in this Apple forum thread , but let's have a whirlwind tour for the moderately interested (my knowledge of Unix processes is far from perfect, so feel free to correct me on these points!): >/dev/null redirects standard output to oblivion and 2>&1 redirects standard error to standard output (i.e. also to oblivion), which persuades Automator that you're really not expecting to hear from the process via these standard streams ever again, so there's no point in keeping the shell script running. These can be shortened to &>/dev/null . the final & runs the command in the background, which ensures control of the shell is returned to the user as soon as the process is spawned; since there are no additional commands in the shell script and all remaining ties have been severed, Automator finally agrees that the task has probably done all it was expected to do and exits it. Wrapping up Whew! That's it. It's really not that complicated, it's just that my prose is verbose, so it makes it look like there's lots and lots to do. Trust me, there isn't. My first go at solving this usability problem -- the one I originally wanted to post way back in 2014 -- was a lengthy, godawful Applescript prone to subtle breakage. This is much better. And the ability to just use a single GUI app for transparently launching and connecting to the Emacs daemon is pure bliss. While you're at it, for an even better Emacs experience, go fetch the excellent Spacemacs Emacs config distribution , which pulls this venerable piece of software screaming into the 21st century. The best editor is neither Vim nor Emacs, its Vim + Emacs! The addictive icing of Vim modal editing on the outside, a creamy Elisp core -- what more could you want from life? ;) Oh and if, like me, you love Spacemacs' snappy icon with the Evil spaceship over planet Emacs -- or if, like me, you have OCD -- you'll definitely want to switch your Emacs logo to the Spacemacs one !","tags":"macOS","url":"emacs-daemon-osx.html"},{"title":"How computers handle text: a gentle but thorough introduction to Unicode","text":"Or, the absolute minimum every software developer linguist absolutely, positively must know about Unicode and character sets (no excuses!) Note : This text uses the Python programming language to give some hands-on experience of the concepts discussed. If you're not familiar with programming at all, much less with Python, my advice is: either: ignore the code, focus on the comments around it, they should be enough to follow the thread of the explanation; or, if you've got a little more time: Python is pretty intuitive, so you might want to have a look at the code examples anyway. In any case, the code might make more sense to you if you tinker with it in an interactive Python session: And now, without further ado... Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character: with 1 bit you can have 2&#94;1 = 2 characters (one is mapped to 0, the other to 1) with 2 bits you can have 2&#94;2 = 2*2 = 4 characters (mapped to 00, 01, 10 and 11) with 3 bits you can have 2&#94;3 = 2*2*2 = 8 characters etc. Now, a short digression on representing numbers, to make sure we're all on the same page: in the context of computers, you often see the same number represented in three different ways: as a decimal number, using 10 digits 0--9 (e.g. 12) as a binary number, using only 2 digits, 0 and 1 (e.g. 1100, which equals decimal 12) as a hexadecimal number, using 16 digits: 0--9 and a--f (e.g. c, which also equals decimal 12) As a consequence, the need often arises to convert between these different numeral systems . If you don't want to do so by hand, Python has your back! The bin() function gives you a string representation of the binary form of a number: In [1]: bin ( 65 ) Out[1]: '0b1000001' As you can see, in Python, binary numbers are given a 0b prefix to distinguish them from regular (decimal) numbers. The number itself is what follows after (i.e. 1000001). Similarly, hexadecimal numbers are given an 0x prefix: In [2]: hex ( 65 ) Out[2]: '0x41' To convert in the opposite direction, i.e. to decimal, just evaluate the binary or hexadecimal representation of a number: In [3]: 0b1000001 Out[3]: 65 In [4]: 0x41 Out[4]: 65 Numbers using these different bases (base 2 = binary, base 10 = decimal, base 16 = hexadecimal) are mutually compatible, e.g. for comparison purposes: In [5]: 0b1000001 == 0x41 == 65 Out[5]: True Why are hexadecimals useful? They're primarily a more convenient, condensed way of representing sequences of bits: each hexadecimal digit can represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2&#94;4 = 16). In [6]: 0xa == 0b1010 Out[6]: True In [7]: 0xb == 0b1011 Out[7]: True In [8]: # if we paste together hexadecimal a and b, it's the # same as pasting together binary 1010 and 1011 0xab == 0b10101011 Out[8]: True In other words, instead of binary 10101011, we can just write hexadecimal ab and save ourselves some space. Of course, this only works if shorter binary numbers are padded to a 4-bit width : In [9]: 0x2 == 0b10 Out[9]: True In [10]: 0x3 == 0b11 Out[10]: True In [11]: # if we paste together hexadecimal 2 and 3, we have to # paste together binary 0010 and 0011... 0x23 == 0b00100011 Out[11]: True In [12]: # ... not just 10 and 11 0x23 == 0b1011 Out[12]: False The padding has no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers. Now back to text encodings. The oldest encoding still in widespread use is ASCII , which is a 7-bit encoding. What's the number of different sequences of seven 1's and 0's? In [13]: # this is how Python spells 2&#94;7, i.e. 2*2*2*2*2*2*2 2 ** 7 Out[13]: 128 This means ASCII can represent 128 different characters , which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old teletype terminals for which ASCII was designed. For instance, the letter \"A\" corresponds to the number 65 ( 1000001 in binary, see above). \"ASCII\" stands for \" American Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance. Nowadays, ASCII is represented using 8 bits (= 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. That extra bit means that there's room for another 128 characters in addition to the 128 ASCII ones , coming up to a total of 256. In [14]: 2 ** ( 7 + 1 ) Out[14]: 256 What happens in the range [128; 256) is not covered by the ASCII standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO latin-2 encoding, or Microsoft's cp-1250 . Encodings which stick to the same character mappings as ASCII in the range [0; 128) and represent them physically in the same way (as 1 byte) , while potentially adding more character mappings beyond that, are called ASCII -compatible . ASCII compatibility is a good thing™, because when you start reading a character stream in a computer, there's no way to know in advance what encoding it is in (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is ASCII by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this: <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> ... This way, whenever a program wants to read a file like this, it can start off with ASCII , waiting to see if it reaches the charset (i.e. encoding) attribute, and once it does, it can switch from ASCII to that encoding ( UTF-8 here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as ASCII for all practical intents and purposes. Without the charset attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text. Let's take a look at printable characters in the Latin-2 character set . The character set consists of mappings between positive integers (whole numbers) and characters; each one of these is called a codepoint . The Latin-2 encoding then defines how to encode each of these integers as a series of bits (1's and 0's) in the computer's memory. In [15]: latin2_printable_characters = [] # the Latin-2 character set has 256 codepoints, corresponding to # integers from 0 to 255 for codepoint in range ( 256 ): # the Latin-2 encoding is simple: each codepoint is encoded # as the byte corresponding to that integer in binary byte = bytes ([ codepoint ]) character = byte . decode ( encoding = \"latin2\" ) if character . isprintable (): latin2_printable_characters . append (( codepoint , character )) latin2_printable_characters Out[15]: [(32, ' '), (33, '!'), (34, '\"'), (35, '#'), (36, '$'), (37, '%'), (38, '&'), (39, \"'\"), (40, '('), (41, ')'), (42, '*'), (43, '+'), (44, ','), (45, '-'), (46, '.'), (47, '/'), (48, '0'), (49, '1'), (50, '2'), (51, '3'), (52, '4'), (53, '5'), (54, '6'), (55, '7'), (56, '8'), (57, '9'), (58, ':'), (59, ';'), (60, '<'), (61, '='), (62, '>'), (63, '?'), (64, '@'), (65, 'A'), (66, 'B'), (67, 'C'), (68, 'D'), (69, 'E'), (70, 'F'), (71, 'G'), (72, 'H'), (73, 'I'), (74, 'J'), (75, 'K'), (76, 'L'), (77, 'M'), (78, 'N'), (79, 'O'), (80, 'P'), (81, 'Q'), (82, 'R'), (83, 'S'), (84, 'T'), (85, 'U'), (86, 'V'), (87, 'W'), (88, 'X'), (89, 'Y'), (90, 'Z'), (91, '['), (92, '\\\\'), (93, ']'), (94, '&#94;'), (95, '_'), (96, '`'), (97, 'a'), (98, 'b'), (99, 'c'), (100, 'd'), (101, 'e'), (102, 'f'), (103, 'g'), (104, 'h'), (105, 'i'), (106, 'j'), (107, 'k'), (108, 'l'), (109, 'm'), (110, 'n'), (111, 'o'), (112, 'p'), (113, 'q'), (114, 'r'), (115, 's'), (116, 't'), (117, 'u'), (118, 'v'), (119, 'w'), (120, 'x'), (121, 'y'), (122, 'z'), (123, '{'), (124, '|'), (125, '}'), (126, '~'), (161, 'Ą'), (162, '˘'), (163, 'Ł'), (164, '¤'), (165, 'Ľ'), (166, 'Ś'), (167, '§'), (168, '¨'), (169, 'Š'), (170, 'Ş'), (171, 'Ť'), (172, 'Ź'), (174, 'Ž'), (175, 'Ż'), (176, '°'), (177, 'ą'), (178, '˛'), (179, 'ł'), (180, '´'), (181, 'ľ'), (182, 'ś'), (183, 'ˇ'), (184, '¸'), (185, 'š'), (186, 'ş'), (187, 'ť'), (188, 'ź'), (189, '˝'), (190, 'ž'), (191, 'ż'), (192, 'Ŕ'), (193, 'Á'), (194, 'Â'), (195, 'Ă'), (196, 'Ä'), (197, 'Ĺ'), (198, 'Ć'), (199, 'Ç'), (200, 'Č'), (201, 'É'), (202, 'Ę'), (203, 'Ë'), (204, 'Ě'), (205, 'Í'), (206, 'Î'), (207, 'Ď'), (208, 'Đ'), (209, 'Ń'), (210, 'Ň'), (211, 'Ó'), (212, 'Ô'), (213, 'Ő'), (214, 'Ö'), (215, '×'), (216, 'Ř'), (217, 'Ů'), (218, 'Ú'), (219, 'Ű'), (220, 'Ü'), (221, 'Ý'), (222, 'Ţ'), (223, 'ß'), (224, 'ŕ'), (225, 'á'), (226, 'â'), (227, 'ă'), (228, 'ä'), (229, 'ĺ'), (230, 'ć'), (231, 'ç'), (232, 'č'), (233, 'é'), (234, 'ę'), (235, 'ë'), (236, 'ě'), (237, 'í'), (238, 'î'), (239, 'ď'), (240, 'đ'), (241, 'ń'), (242, 'ň'), (243, 'ó'), (244, 'ô'), (245, 'ő'), (246, 'ö'), (247, '÷'), (248, 'ř'), (249, 'ů'), (250, 'ú'), (251, 'ű'), (252, 'ü'), (253, 'ý'), (254, 'ţ'), (255, '˙')] Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include: how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding? how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms? For these purposes, a standard character set known as Unicode was developed which strives for universal coverage of (ultimately) all characters ever used in the history of writing, even adding new ones like emojis . Unicode is much bigger than the character sets we've seen so far -- its most frequently used subset, the Basic Multilingual Plane , has 2&#94;16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more. In [16]: 2 ** 16 Out[16]: 65536 Now, the most straightforward representation for 2&#94;16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called UTF-16 (\"UTF\" stands for \"Unicode Transformation Format\"), but consider the drawbacks: we've lost ASCII compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as 01100001 or 00000000|01100001 , with the | indicating an imaginary boundary between bytes, is not the same thing) encoding a string in a language which is mostly written down using basic letters of the Latin alphabet now takes up twice as much space (which is probably not a good idea, given the general dominance of English in electronic communication) Looks like we'll have to think outside the box. The box in question here is called fixed-width encodings -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for UTF-16 , because it is something more than just a \"16-bit ASCII \": it has ways of handling characters beyond 2&#94;16 using so-called surrogate sequences -- but you get the gist.) The smart idea that some bright people have come up with was to use a variable-width encoding . The most ubiquitous one currently is UTF-8 , which we've already met in the HTML example above. UTF-8 is ASCII -compatible, i.e. the 1's and 0's used to encode text containing only ASCII characters are the same regardless of whether you use ASCII or UTF-8 : it's a sequence of 8-bit bytes. But UTF-8 can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits. In [17]: def print_utf8_bytes ( char ): \"\"\"Prints binary representation of character as encoded by UTF-8. \"\"\" # encode the string as UTF-8 and iterate over the bytes; # iterating over a sequence of bytes yields integers in the # range [0; 256); the formatting directive \"{:08b}\" does two # things: # - \"b\" prints the integer in its binary representation # - \"08\" left-pads the binary representation with 0's to a total # width of 8, which is the width of a byte binary_bytes = [ f \" { byte : 08b } \" for byte in char . encode ( \"utf8\" )] print ( f \" { char !r} encoded in UTF-8 is: { binary_bytes } \" ) print_utf8_bytes ( \"A\" ) # the representations... print_utf8_bytes ( \"č\" ) # ... keep... print_utf8_bytes ( \"字\" ) # ... getting longer. 'A' encoded in UTF-8 is: ['01000001'] 'č' encoded in UTF-8 is: ['11000100', '10001101'] '字' encoded in UTF-8 is: ['11100101', '10101101', '10010111'] How does that even work? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits? Essentially, the trick is to use some of the bits in the representation of a codepoint to store information not about which character it is (whether it's an \"A\" or a \"字\"), but how many bits it occupies . In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. Take the following example: In [18]: for char in \"Básník 李白\" : print_utf8_bytes ( char ) 'B' encoded in UTF-8 is: ['01000010'] 'á' encoded in UTF-8 is: ['11000011', '10100001'] 's' encoded in UTF-8 is: ['01110011'] 'n' encoded in UTF-8 is: ['01101110'] 'í' encoded in UTF-8 is: ['11000011', '10101101'] 'k' encoded in UTF-8 is: ['01101011'] ' ' encoded in UTF-8 is: ['00100000'] '李' encoded in UTF-8 is: ['11100110', '10011101', '10001110'] '白' encoded in UTF-8 is: ['11100111', '10011001', '10111101'] Notice the initial bits in each byte of a character follow a pattern depending on how many bytes in total that character has: if it's a 1-byte character, that byte starts with 0 if it's a 2-byte character, the first byte starts with 11 and the following one with 10 if it's a 3-byte character, the first byte starts with 111 and the following ones with 10 This makes it possible to find out which bytes belong to which characters, and also to spot invalid strings, as the leading byte in a multi-byte sequence always \"announces\" how many continuation bytes (= starting with 10) should follow. So much for a quick introduction to UTF-8 (= the encoding), but there's much more to Unicode (= the character set). While UTF-8 defines only how integer numbers corresponding to codepoints are to be represented as 1's and 0's in a computer's memory, Unicode specifies how those numbers are to be interpreted as characters, what their properties and mutual relationships are, what conversions (i.e. mappings between (sequences of) codepoints) they can undergo, etc. Consider for instance the various ways diacritics are handled: \"č\" can be represented either as a single codepoint ( LATIN SMALL LETTER C WITH CARON -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark ( COMBINING CARON ). You can search for the codepoints corresponding to Unicode characters e.g. here and play with them in Python using the chr(0xXXXX) built-in function or with the special string escape sequence \\uXXXX (where XXXX is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint: In [19]: # \"č\" as LATIN SMALL LETTER C WITH CARON, codepoint 010d print ( chr ( 0x010d )) print ( \" \\u010d \" ) č č In [20]: # \"č\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and # COMBINING CARON, codepoint 030c print ( chr ( 0x0063 ) + chr ( 0x030c )) print ( \" \\u0063\\u030c \" ) č č In [21]: # of course, chr() also works with decimal numbers chr ( 269 ) Out[21]: 'č' This means you have to be careful when working with languages that use accents, because to a computer, the two possible representations are of course different strings , even though to you, they're conceptually the same: In [22]: s1 = \" \\u010d \" s2 = \" \\u0063\\u030c \" # s1 and s2 look the same to the naked eye... print ( s1 , s2 ) č č In [23]: # ... but they're not s1 == s2 Out[23]: False Watch out, they even have different lengths ! This might come to bite you if you're trying to compute the length of a word in letters. In [24]: print ( \"s1 is\" , len ( s1 ), \"character(s) long.\" ) print ( \"s2 is\" , len ( s2 ), \"character(s) long.\" ) s1 is 1 character(s) long. s2 is 2 character(s) long. For this reason, even though we've been informally calling these Unicode entities \"characters\", it is more accurate and less confusing to use the technical term \"codepoints\". Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a normalization : In [25]: from unicodedata import normalize # NFC stands for Normal Form C; this normalization applies a canonical # decomposition (into a multi-codepoint representation) followed by a # canonical composition (into a single-codepoint representation) s1 = normalize ( \"NFC\" , s1 ) s2 = normalize ( \"NFC\" , s2 ) s1 == s2 Out[25]: True Let's wrap things up by saying that Python itself uses Unicode internally, but the encoding it defaults to when opening an external file depends on the locale of the system (broadly speaking, the set of region, language and character-encoding related settings of the operating system). On most modern Linux and macOS systems, this will probably be a UTF-8 locale and Python will therefore assume UTF-8 as the encoding by default. Unfortunately, Windows is different. To be on the safe side, whenever opening files in Python, you can specify the encoding explicitly: In [26]: with open ( \"unicode.ipynb\" , encoding = \"utf-8\" ) as file : pass In fact, it's always a good idea to specify the encoding explicitly, using UTF-8 as a default if you don't know, for at least two reasons -- it makes your code more: portable -- it will work the same across different operating systems which assume different default encodings; and resistant to data corruption -- UTF-8 is more restrictive than fixed-width encodings, in the sense that not all sequences of bytes are valid UTF-8 . E.g. if one byte starts with 11, then the following one must start with 10 (see above). If it starts with anything else, it's an error. By contrast, in a fixed-width encoding, any sequence of bytes is valid. Decoding will always succeed, but if you use the wrong fixed-width encoding, the result will be garbage, which you might not notice. Therefore, it makes sense to default to UTF-8 : if it works, then there's a good chance that the file actually was encoded in UTF-8 and you've read the data in correctly; if it fails, you get an explicit error which prompts you to investigate further. Another good idea, when dealing with Unicode text from an unknown and unreliable source, is to look at the set of codepoints contained in it and eliminate or replace those that look suspicious. Here's a function to help with that: In [27]: import unicodedata as ud from collections import Counter import pandas as pd def inspect_codepoints ( string ): \"\"\"Create a frequency distribution of the codepoints in a string. \"\"\" char_frequencies = Counter ( string ) df = pd . DataFrame . from_records ( ( freq , char , f \"U+ { ord ( char ) : 04x } \" , ud . name ( char ), ud . category ( char ) ) for char , freq in char_frequencies . most_common () ) df . columns = ( \"freq\" , \"char\" , \"codepoint\" , \"name\" , \"category\" ) return df Depending on your font configuration, it may be very hard to spot the two intruders in the sentence below. The frequency table shows the string contains regular LATIN SMALL LETTER T and LATIN SMALL LETTER G , but also their specialized but visually similar variants MATHEMATICAL SANS-SERIF SMALL T and LATIN SMALL LETTER SCRIPT G . You might want to replace such codepoints before doing further text processing... In [28]: inspect_codepoints ( \"Intruders here, good 𝗍hinɡ I checked.\" ) Out[28]: freq char codepoint name category 0 5 e U+0065 LATIN SMALL LETTER E Ll 1 5 U+0020 SPACE Zs 2 3 r U+0072 LATIN SMALL LETTER R Ll 3 3 d U+0064 LATIN SMALL LETTER D Ll 4 3 h U+0068 LATIN SMALL LETTER H Ll 5 2 I U+0049 LATIN CAPITAL LETTER I Lu 6 2 n U+006e LATIN SMALL LETTER N Ll 7 2 o U+006f LATIN SMALL LETTER O Ll 8 2 c U+0063 LATIN SMALL LETTER C Ll 9 1 t U+0074 LATIN SMALL LETTER T Ll 10 1 u U+0075 LATIN SMALL LETTER U Ll 11 1 s U+0073 LATIN SMALL LETTER S Ll 12 1 , U+002c COMMA Po 13 1 g U+0067 LATIN SMALL LETTER G Ll 14 1 𝗍 U+1d5cd MATHEMATICAL SANS-SERIF SMALL T Ll 15 1 i U+0069 LATIN SMALL LETTER I Ll 16 1 ɡ U+0261 LATIN SMALL LETTER SCRIPT G Ll 17 1 k U+006b LATIN SMALL LETTER K Ll 18 1 . U+002e FULL STOP Po ... because of course, for a computer, the word \"thing\" written with two different variants of \"g\" is really just two different words, which is probably not what you want: In [29]: \"thing\" == \"thinɡ\" Out[29]: False Finally, to put things into perspective, here's a diagram what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is not UTF-8 nor UTF-16 ): (Image shamelessly hotlinked from / courtesy of the NLTK Book . Go check it out, it's an awesome intro to Python programming for linguists!) A terminological postscript: we've been using some terms a bit informally, but now that we have a practical intuition for what they mean, it's good to get the definitions straight in one's head. So, a character set is a mapping between codepoints (integers) and characters . We may for instance say that in our character set, the integer 99 corresponds to the character \"c\". On the other hand, an encoding is a mapping between a codepoint (an integer) and a physical sequence of 1's and 0's that represent it in memory . With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, which have to explicitly encode information about how many bits are spanned by each codepoint, this straightforward correspondence breaks down. A comparison might be helpful here: as encodings, UTF-8 and UTF-16 both use the same character set -- the same integers corresponding to the same characters. But since they're different encodings , when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one. For more on Unicode, a great read already hinted at above is Joel Spolsky's The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . Another great piece of material is the Characters, Symbols and the Unicode Miracle video by the Computerphile channel on YouTube. To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)","tags":"ling","url":"unicode.html"},{"title":"Úprava rozhraní konkordanceru KonText -- vylepšená verze","text":"Před nějakou dobou jsem zde vyvěsil skript , jehož pomocí lze lehce \"přeskládat\" a upravit rozhraní korpusového konkordanceru KonText : menu je umístěné po straně místo nahoře a permanentně rozbalené nad vyhledanou konkordancí je umístěn rychlý hledací box, v němž lze předchozí dotaz pohodlně upravit Víc o motivaci těchto úprav se dočtete v původním článku . Stále platí, že ČNK nemá v plánu tyto změny začlenit přímo do oficiální verze KonTextu, zejména proto, že rychlý hledací box sice v jistých situacích může být užitečný, nicméně oproti standardnímu formuláři Nový dotaz výrazně omezuje možnosti pro zadání dotazu. Vylepšená verze, která je k dispozici níže, odstraňuje některé předchozí nedostatky skriptu: rychlý hledací box nad konkordancí je větší, ukazuje vždy CQL podobu posledního zadaného dotazu 1 , a především zůstává zobrazený i během listování konkordancí (tj. není k dispozici jen na její první stránce). Dotaz lze nyní navíc pro větší přehlednost rozdělit do více řádků, takže opětovné vyhledávání se nově spouští stiskem kombinace kláves Ctrl+Enter (místo jen Enteru). Výsledné upravené rozhraní KonText vypadá stále podobně: Postup instalace skriptu Nová verze skriptu je k dispozici zde: Kroky k jeho zprovoznění zůstávají stejné: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. V předchozí verzi se po aplikaci libovolného filtru změnil obsah hledacího boxu na parametry filtrování. ↩","tags":"ling","url":"kontext-interface-tweak-update.html"},{"title":"Úprava rozhraní konkordanceru KonText","text":"!POZOR! K dispozici je nyní vylepšená verze níže popsaného skriptu . Hledání v korpusech ČNK Český národní korpus je sbírka jazykových korpusů částečně vytvářených Ústavem Českého národního korpusu a částečně jinými institucemi. Všechny jsou hostované na jednom serveru a dostupné skrz různá vyhledávací rozhraní (tzv. konkordancery ), např. NoSke , Bonito či nejnověji KonText . Koncem března 2015 ovšem bude podpora starších rozhraní ukončena a nadále půjde k datům v ČNK přistupovat primárně pouze přes KonText. (Pokud vám odstavec výše nedává příliš smysl, s jazykovými korpusy se setkáváte poprvé, ale chcete se dozvědět víc, raději si místo tohoto postu přečtěte, k čemu je takový korpus dobrý , a zkuste si v něm něco pro zajímavost vyhledat . Pokud se vám při vzpomínce na Bonito či NoSke naopak zaskvěla slza v oku, čtěte dál!) KonText vs. Bonito / NoSke KonText má oproti starším rozhraním řadu výhod -- bohatší funkcionalitu, mnohé pomůcky, které vám pomohou se zadáním složitějších dotazů (sestavení morfologického tagu či podmínky within ), a v neposlední řadě mnohem lépe vypadá, což kupříkladu mně při práci působí jako balzám na duši. Nicméně dlouholetí uživatelé ČNK byli jednoduše zvyklí na některé aspekty Bonita a NoSke, které jim teď v KonTextu chybí. Onehdy při rozhovoru s jedním z nich vyplavaly na povrch jako hodně důležité dvě stížnosti: Vrchní menu v KonTextu je zákeřné, schovává se, člověk nemá přehled nad dostupnými funkcemi. Oproti tomu NoSke má menu po straně a je permanentně rozvinuté, takže uživatel má všechny možnosti interakce s konkordancí soustavně jako na dlani. Po zadání dotazu člověk často na základě konkordance zjistí, že jej potřebuje ještě trochu upravit / zjemnit. KonText si sice předchozí dotazy pamatuje, je ale potřeba se k nim doklikat; šikovnější by bylo, kdyby tato možnost byla dostupná přímo ze stránky konkordance v podobě nějakého zjednodušeného hledacího boxu. (NoSke tohle vlastně taky neumí, v Bonitu je to jednodušší.) V obou případech jde o smysluplné požadavky, jenže KonText je poměrně velká a složitá aplikace, takže i pokud se ČNK rozhodne do ní tyto podněty v nějaké podobě zapracovat (např. jako možnost přepnutí zobrazení menu), bude nějakou chvíli trvat, než se implementace navrhne, vytvoří, řádně otestuje a konečně dostane k uživatelům. Nicméně aby bylo možné alespoň vyzkoušet, jak by zmíněné změny vypadaly v praxi, dal jsem dohromady krátký skript, který již v prohlížeči nahraný KonText trochu \"přestaví\" a upraví. Výsledek vypadá následovně: Rovnou předesílám: ten skript je nevzhledný bastl přilepený na KonText zvnějšku; proto taky bylo možné jej dát dohromady poměrně rychle, protože si neklade nárok na spolehlivost, která se vyžaduje od oficiální verze KonTextu. Je to spíš prototyp, jehož účelem je otestovat výše popsané změny v praxi a získat představu o tom, zda a do jaké míry jsou přínosné. (Vlastní zkušenost: po chvíli používání mi přijde přídatný hledací box nad konkordancí hodně šikovný a užitečný.) Teď k jádru pudla: pokud máte zájem, můžete si KonText takto k obrazu svému (resp. k obrázku o odstavec výš) upravit také a vyzkoušet, jak vám takové nastavení vyhovuje. Když se vám jedna z úprav bude líbit (nebo vás u toho napadne jiná, kterou by si KonText zasloužil), můžete pak zadat požadavek na nový feature . Návod, jak si KonText upravit, následuje níže. Postup instalace skriptu Skript samotný je k dispozici zde: K jeho zprovoznění jsou potřeba následující kroky: Nainstalovat si do svého prohlížeče plugin Tampermonkey , pokud používáte Chrome, nebo Greasemonkey , pokud používáte Firefox. (Pokud používáte Internet Explorer, budete muset dočasně přesedlat na Chrome nebo Firefox.) Testovaný je skript zatím jen na Chromu. Založit v daném pluginu nový skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nového skriptu a nahradit ji skriptem, který si zkopírujete výše. Skript uložit. Používat KonText jako normálně -- skript už by podle adresy měl sám poznat, že se má spustit. Pokud se tak nestane, nejspíš to znamená, že je prohlížečový plugin (Tampermonkey nebo Greasemonkey) deaktivovaný a je potřeba jej znovu aktivovat. Omezení Skript má pravděpodobně hromadu drobných much, na které se mi zatím nepodařilo přijít -- budu se je snažit průběžně opravovat, když na ně padnu, nebo když mi o nich dáte vědět . Krom toho má i některé mouchy, o nichž už vím, ale bohužel toho s nimi nejde moc dělat. Asi nejnápadnější je, že přidaný hledací box funguje jen na těch stránkách, kde je původní dotaz i součástí adresy URL (což nejsou všechny -- třeba když začnete listovat konkordancí na druhou stránku a dál, dotaz je z adresy vyjmut a pomocný hledací box tedy zmizí ). Ale vzhledem k tomu, že jeho hlavní účel má být možnost lehce upravit dotaz po prvním rychlém nahlédnutí do konkordance, snad to nebude takový problém. Pokud někdy bude podobný box řádně přidán přímo do KonTextu, takovými nedostatky samozřejmě trpět nebude. A ještě k používání přidaného hledacího boxu : Typ dotazu, který je do něj potřeba zadat, je stejný jako ten, který jste při prvotním vyhledání konkordance zadali na stránce Nový dotaz . Pokud tento prvotní dotaz byl Základní dotaz, můžete pomocí rychlého boxu zadat jiný Základní dotaz; pokud to byl CQL dotaz, můžete ho upravit zas jen na další CQL dotaz. Důvodem je, že smyslem tohoto pomocného boxu není nahradit plnohodnotný formulář pro zadání dotazu, jen poskytnout rychlou možnost, jak již zadaný dotaz upravit . Pomocný hledací box se objeví i poté, co na konkordanci provedete filtrování. V takové situaci se dá použít k tomu, abyste pozměnili zadání aktuálního filtru , tj. filtrování se provede znovu na původní konkordanci, ne na této již filtrované. Pokud chcete opakovaně filtrovat tu samou konkordanci a postupně podle daných kritérií vyřazovat / přidávat řádky, je potřeba místo hledacího boxu opakovaně použít menu Filtr . Komu si stěžovat, když to nebude fungovat Skript je volně šiřitelný pod licencí GNU GPL v3 , takže se na něj neváže žádná záruka. Když se vám ale nebude dařit jej zprovoznit, rád se pokusím pomoct! Stačí se ozvat na adresu uvedenou zde .","tags":"ling","url":"kontext-interface-tweak.html"},{"title":"Beyond semantic versioning? (cross-post)","text":"Background Ever since I first read about semantic versioning , I've thought of it as a neat idea. But only recently did it occur to me that what I liked about the idea was its goal, much less its execution (more on that below). What made it obvious was this lengthy discussion about breaking changes introduced in v1.7 of underscore.js without an accompanying major version bump. Even though I still think sticking to semver is the right thing to do if your community of users expects you to (even if you don't personally like the system), I am convinced there are fundamentally better ways of dealing with the problem of safely and consistently updating dependencies. It made me want to add my two cents to the discussion , as someone who's more of a dabbler in programming and not really part of the community, so feel free to ignore me :) I attach my commentary below for reference (it's virtually the same text as in the link above). tl;dr semver is trying to do the right thing, but doing it wrong -- instead of implicitly encoding severity of change information in version numbers , explicit keywords like :patch, :potentially-breaking or :major-api-change would make much more sense. More verbosely I've always found the goals of semver worthy, but this thread has made me realize that while its aims are commendable, its methods are kind of broken: semver tries to take an existing semiotic system (= version numbers), which has developed informally and is therefore a loose convention rather than an exact spec, and reinterpret it in terms of an exact spec (or impose that spec on it). trouble is, the prior informal meaning won't go away so easily (why should it?), especially for projects that have been around longer than semver. the problem then is, since the two systems (the informal one and semver) look the same in terms of their symbolic representation, it's hard to guess which one you're dealing with by just eyeballing the version number of a library (or project in general). it's like if someone decided that \"f*ck\" should mean \"orchid\" from now on, because it's nicer -- on hearing the word, you'd never know if it's being used as the original profanity, or in its new meaning. homonymy is a pain to deal with when it's accidental (cf. NLP), so why introduce it on purpose? the job that semver set out to do should be fulfilled by a new formal means which is instantly recognizable, not by hijacking an existing one and overlaying additional interpretation on it and thus making it ambiguous . even if version numbers hadn't existed before semver, they're terribly inadequate for the purpose of conveying information about the severity of changes introduced by an update (though I understand their appeal to mathematically-minded people). they're inadequate because they're implicit -- it's a bit like if someone decided they don't need hash maps because they can make do with arrays by remembering the order in which they're adding in the key-val pairs. if I remember the order, then I know which key the given index implicitly refers to, and the result is as good as a hash map, isn't it? except it isn't. keys are useful because they have explicit semantics , making it instantly clear what kind of value you're retrieving. in the same way, encoding the information about the severity of changes into version numbers makes it implicit (in addition to being ambiguous, as stated previously). why not use explicit keyword tags along with the version number (which can be romantic, semantic -- whichever floats the dev team's boat and best reflects the progress of the project) to give a heads up as to the nature of the update? e.g. :patch, :potentially-breaking, :major-api-change etc. granted, even language is a code which needs to be learned, like semver (gross oversimplification here, but let's not get into the details of language acquisition), but since it's widely established and conventionalized for conveying the kinds of meanings semver is trying to convey, why not just use it when it's available ? why use a system (version numbers) which is less well-suited to the purpose and ambiguous to boot? (on the other hand, numbers are eminently well-suited for keeping track of which version is newer than which and how much so -- the original purpose of version numbering -- because they are designed to have orderings defined on them. by contrast, words would do a terrible job at this. if you care to indicate the evolution of your codebase, you might introduce your own disciplined romantic or sentimental versioning scheme, which ironically is a more meaningful and ergo semantic way of doing versioning than semver, because it sticks to the conventional semantics of numbers (the closer the numbers, the more similar the versions). if you don't care about this, which is perfectly fine, you might as well use dates for version numbers.) keyword tags have the advantage that they're instantly human-readable by anyone who has a basic command of English. if there is sufficient will in the community, a useful subset can be frozen in a binding spec, so that they are machine-readable as well. I'm not sure whether these keywords should be an appendix to the version number (like v2.3.4-:potentially-breaking), or whether the information they provide should be more extensive and included in a formalized preamble to the changelog (finally forcing people to at least take a glance at it ;) ). using the latter approach, the information provided could be (optionally) even more targeted, e.g. detailing explicitly which parts of the API are affected in a non-backwards compatible manner by the update. anyways, just a few ideas :) I am not primarily a coder, so there may be obvious drawbacks to this scheme that I can't see or which have already been discussed by the community on multiple occasions which have escaped my attention. in which case, please bear with me and excuse my lack of sophistication.","tags":"floss","url":"beyond-semver.html"},{"title":"Filling (hardwrapping) paragraphs in Airmail with `par`","text":"tl;dr Jump directly to the proposed solution . Tested on OS X 10.9 (Mavericks). Back story Airmail is a great application -- being very happy with Gmail's in-browser UI, it's honestly the first e-mail desktop client that I ever felt even remotely tempted to use. It has: a sleek, functional design almost flawless integration with Gmail (except for categories -- but there's a not-too-hackish way to deal with those) a Markdown compose mode (yay!) -- and tons of other good stuff. Especially that last feature almost got me sold -- you see, I like my e-mail hardwrapped (what Emacs calls \"filling paragraphs\"), because most of the time, I view it on monitors that are too wide for soft line wrapping to achieve a comfortable text width. (By the way, Airmail's layout deals with this issue very elegantly, but I know I won't be using only Airmail. Plus there are the obvious netiquette issues -- lines \"should be\" wrapped at 72 characters etc.) In Gmail, I therefore use plain-text compose, which is fine for the purposes described above, but frustrating whenever you want to apply formatting (obviously, you can't -- it's plain text). I tried using the usual replacements for formatting like stars & co., and I don't know about your grandma, but mine certainly doesn't take *...* to mean emphasis. I thought the Markdown compose mode in Airmail would solve my problems -- I could apply formatting if and when I wanted (using the frankly more streamlined process of typing it in rather than fumbling around for the right button in the GUI) and fill my paragraphs, because I somehow automatically assumed there'd by a hard-wrap feature like in any decent editor (read: emacs or vi). Markdown is plain text after all, isn't it? Long story short, as of yet, there isn't . There isn't even one for the plain-text compose mode, as far as I'm aware. So I added my two cents to this feature request thread and went back to the Gmail in-browser UI. Solution But then I realized (it took me a while, I'm still very much an OS X newbie): in OS X, you can define custom actions with shortcuts 1 for any application using Automator Services these actions can be easily set to receive text selected in the application as input these actions can also involve shell scripts there already is a great (command line) program for filling paragraphs -- it's called par , and as much as I admire what Airmail's developers have achieved, it's unlikely that they'd come up with a more sophisticated hard-wrapping algorithm than par 's simply as a side project for Airmail (see the EXAMPLES section in man par ) With that in mind, you can have hard-wrapping in Markdown or plain-text Airmail compose at your fingertips in no time flat. If you don't have homebrew , start by installing that (or any other ports manager that will allow you to install par ; I'll assume homebrew below) by pasting ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" at a Terminal prompt. Then: install par with brew install par at a Terminal prompt open Automator (e.g. by typing \"Automator\" into Spotlight) and create a new Service select the applications for which you want the service to be active (for me, that's just Airmail) and tick the \"Output replaces selected text\" box drag the \"Run Shell Script\" action onto the workflow canvas, and as the shell script, paste in PARINIT = \"rTbgqR B=.,?_A_a Q=_s>|\" /usr/local/bin/par 79 the $PARINIT environment variable contains the default recommended settings for par (if you want to customize its behavior, you can -- good luck wrapping your head around par 's manpage, though) you should set the full path to the par executable, the shell spawned by the Service might not inherit your $PATH -- for par installed via homebrew , it's /usr/local/bin/par the parameter at the end is the max number of characters per line -- mailing list etiquette stipulates 72, I personally prefer the pythonesque 79, but it's your choice At this point, your service should look something like in the screenshot below: Save it, open Keyboard preferences (type \"Keyboard\" into Spotlight), navigate to Shortcuts → Services → Text and set a keyboard shortcut for your newly created Service, e.g. Cmd+Opt+P. Next time you compose an e-mail in Airmail, just select the entire text when you're done (Cmd+A), press Cmd+Opt+P, and voilà! Your lines have been hardwrapped, your paragraphs filled :) (Same thing, I know.) If the shortcut doesn't appear to work 1 , try fiddling around with it, resetting it (maybe the one you've chosen conflicts with a pre-existing one?), restarting Airmail, logging out and back in, rebooting... The custom shortcut part is unfortunately the least reliable aspect of this whole setup. Automator is a great idea, I was pleasantly surprised by it when I started using OS X a few days back, but it could seriously use some bug-squashing. If you fail miserably at getting the shortcut to work, you can still access your fill paragraph service via the menu (select the text you want to hard-wrap, then navigate to Airmail → Services → <name of your fill paragraph service>). Clicking around in a GUI is tedious (though hey -- it's the Apple way after all, isn't it?), but it shouldn't be too much of a bother since you need to do it only once per e-mail. Bottom line : I am now officially completely sold on Airmail (even bought the released version instead of using the free beta) and look forward to the joy of using it! EDIT: In order to have the least trouble possible getting the shell script up and running as a Service , two rules of thumb: Leave it completely up to OS X where it stores the Service (.workflow) file. This will probably be in ~/Library/Services , and I learnt the hard way not to tinker with it -- if Services is a symlink instead of a real directory, the OS won't discover new Service files (though old ones will still be accessible). If the Service doesn't show up in the keyboard shortcuts menu after creation, try refreshing the service list with /System/Library/CoreServices/pbs -update . Those shortcuts are in fact quite buggy, especially those that you want to be global (not specific to a concrete app) -- at least on Mavericks (OS X 10.9). They tend to get disabled on a whim, especially if you tinker with them, and are a pain to get working again (login, logout, reboot -- anything goes). If anyone knows why, please let me know! ↩ ↩","tags":"macOS","url":"fill-par-in-airmail.html"}]}