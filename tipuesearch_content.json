{"pages":[{"url":"http://dlukes.github.io/pages/about.html","title":"about me","text":"Hi! My name is David Luke≈° and I'm primarily a phonetician/linguist interested in programming and NLP applications, an avid fan of Linux and a beginning user of OS X. I like Python and Clojure and would like to learn me a Haskell for great good ; I find R useful (though convoluted on occasion) and avoid Perl as much as I can. I work at the Institute of the Czech National Corpus , so if you're interested in Czech and/or corpus linguistics , check us out! We have a nice and sleek web interface called KonText (with the manatee corpus query engine as a back-end) for all your Czech-related queries ;) I hail from Prague, Czech Republic, the most beautiful city in the world, and work and live there, though I've lived in France and Belgium for a while when I was a kid. I play badminton and football (of the soccer persuasion -- but it's still football) and enjoy American Jewish literature (to the point of writing my BA thesis on Philip Roth), as well as Jon Stewart, Ricky Gervais, Stephen Fry and several other smart people who can make me laugh. I used to play the saxophone reasonably well, but nowadays mostly enjoy playing the guitar atrociously bad and singing along. As you can tell from the title of this blog (if you're a fellow aficionado), I spent most of my teen years listening to the great music of one Frank Zappa . If you're interested in my academic (and more broadly, professional) credentials, they're available for inspection here .","tags":"pages"},{"url":"http://dlukes.github.io/pages/work.html","title":"work","text":"Current position I'm coordinating phonetic transcription for the ORTOFON corpus project at the Spoken Corpora Section of the Institute of the Czech National Corpus , Faculty of Arts , Charles University in Prague . I also help out with various data processing tasks and functionality prototyping (mainly using Python, Perl and R). Projects For more, check out my GitHub profile . KonText Layout Switcher ‚Ä¢ a Tampermonkey/Greasemonkey script to customize the interface of the KonText corpus manager (see link for a screenshot) MluvKonk ‚Ä¢ an experimental viewer for spoken corpus concordances as exported from KonText , which tries to give more visual cues as to the structure of the dialogue; similar functionality in KonText itself is hopefully coming up soon! CNC Maps ‚Ä¢ an interactive web interface for displaying and manipulating concordances from the ORAL series corpora on a map. also features a component for browsing dialect recordings along with descriptions. TransVer ‚Ä¢ a transcription verifier for our current spoken data collection project at the Czech National Corpus. written in Clojure, using the wonderful seesaw wrapper library around the horrible swing GUI toolkit. my MA thesis (in czech) -- Perceptual sensitivity to music and speech stimuli in the frequency and temporal domains ‚Ä¢ if you're interested in the affinities between the ways humans process language and music, check it out! EXMARaLDA/EXAKT tutorial ‚Ä¢ a short tutorial on using the EXAKT corpus concordancer tool. mainly intended for internal use, but you might find something useful in there. if you speak czech, that is :) suggestions for improvement welcome! PraatEdit ‚Ä¢ a code editor for scripting the Praat speech analysis software environment, with syntax highlighting, written as a Java-learning project -- so expect bugs :) CV pdf (in czech; update: september 2014)","tags":"pages"},{"url":"http://dlukes.github.io/emacs-daemon-osx.html","title":"Configuring Emacs Daemon on Mac OS X","text":"I know I promised this article a loooong time ago (June 2014, when I first got a Mac, to judge by the previous timestamp in the header of this file), but since the historically attested readership of this blog is 2 + a bunch of my facebook friends who I nagged to read my attempt at explaining character encodings to non-technical people , I don't suppose it's as if a legion of fans have been restlessly looking forward to this one ;) Nevertheless, the distinct advantage is that my OS X Emacs setup has had the opportunity to grow more mature and also much simpler in the meantime, which means that if a third reader accidentally stumbles over this note (exploding my ratings...), they might actually find something genuinely useful here. tl;dr This article presents a way to start Emacs Daemon (a persistent Emacs session) from the GUI and subsequently connect to it (creating frames on demand) using an Automator script . The benefit is that you incur startup time lag only once (when you start the daemon) while still being able to close all frames when you're not using Emacs, keeping a clean workspace . This is especially useful if your Emacs is heavily customized and loading it takes a while . Another benefit is that whenever you open a frame connected to an Emacs daemon, all your previously open buffers are still there as you left them (as opposed to opening a fresh instance of Emacs). Skim over the code blocks to get the important gist without the verbose sauce. Tested on OS X 10.11 El Capitan, with Homebrew Emacs and Spacemacs config. Why Emacs Daemon, why this post Installing Emacs on a Mac in and of itself is not that much of a problem -- there are several options, ranging from Homebrew and Macports to Emacs for Mac OS X , Emacs Mac Port and Aquamacs . The last two in this list have some OS X specific tweaks (smooth scrolling, tabs, adapted keyboard shortcuts), which makes them perhaps more appealing out of the box but also less extensible, as some of the information out there about generic Emacs might not apply to them as straightforwardly or indeed at all. With that in mind, if you want to tinker with your Emacs config, it's a good idea to stick with Homebrew's fairly conservative version of Emacs: $ brew update $ brew install emacs --with-cocoa # this step gets you a standard OS X launcher icon $ brew linkapps emacs But now that you've got Emacs, and especially if you're transferring some heavy customization over from say Linux, you might be unhappy that each time you start it from cold, it takes a while, typically a few seconds. That's what emacs --daemon and emacsclient are for: Emacs is run as a daemon in the backround and you connect to it with client frames that spawn almost instantly . This also means that you can close all existing frames to keep your workspace clean if you won't be using Emacs for a while (hard to imagine, right, since you can even read xkcd from inside Emacs ) and then whip up a frame at the speed of a thought when need arises. Now this is all easy to achieve when using the terminal , but since you probably bought that Mac in great part for its shiny pretty elegant ergonomic GUI, you might want Emacs to use GUI frames instead of terminal ones and connect to the Emacs daemon (or start it if it's not running) by just clicking on an app icon in the launcher or finding it from Spotlight. That's where Automator comes in. An Automator script Automator is a built-in OS X app for creating custom automated user workflows for just about any installed app you might have or even OS functionality. Among other things, this means that it allows you to wrap the daemon auto-start functionality available from the terminal (as described in the previous paragraph) into an app launchable from the GUI. Let's get down to business: Launch Automator and create a new document. Select Application as its type. Search the Actions palette on the left for the Run Shell Script action and add it to your Automator document. In the Run Shell Script building block, change the following: set Shell to the shell you're using and whose init files have thus the PATH correctly set to the emacs and emacsclient executables (if you're using Homebrew, it probably told you how to properly set up your PATH as a post-install step) set Pass input to \"as arguments\" (if you then set this Automator app as the default for opening a given type of file , you'll be able to use emacsclient to open files by double-clicking on them in Finder) Finally, paste in the following code snippet and save the app e.g. as EmacsClient.app , preferably in your Applications folder so that it is easily accessible from the launcher. nohup emacsclient --no-wait -c -a emacs \" $@ \" >/dev/null 2> & 1 & At the end of the day, your Automator EmacsClient.app should look something like this: The core of the command that you might want to tweak based on your particular Emacs setup is emacsclient --no-wait -c -a emacs ; mine is optimized to work with mostly stock Spacemacs config (see below). If it doesn't work, you might also want to try a simple emacsclient -c -a \"\" and variations; a good debugging technique is to try these out in the terminal: as soon as you get the line working there, it'll start working in the Automator task as well. \"$@\" is just the list of files (if any) passed to Emacs to open (the aforementioned double-click in Finder use case). The rest is some black magic to ensure that the shell which spawns the Emacs process (because this Automator app is after all, at heart, only a shell script) totally and utterly disowns it, so that the shell script is allowed to return and the Automator task completes as soon as Emacs has started (or the client has spawned a new frame). Otherwise, you'd end up with an irritating spinning cog wheel in your notification area which would stay there until you completely quit Emacs. Which is probably not what you want, since you're undergoing all this hassle in the first place to get a zen, distraction-free Emacs experience. The details of the various incantations are discussed in this Apple forum thread , but let's have a whirlwind tour for the moderately interested (my knowledge of Unix processes is far from perfect, so feel free to correct me on these points!): nohup makes the parent process (the shell) disown the child process (Emacs), so that the shell can exit without killing Emacs as well >/dev/null redirects standard output to oblivion and 2>&1 redirects standard error to standard output (i.e. also to oblivion), which persuades Automator that you're really not expecting to hear from the process via these standard streams ever again, so there's no point in keeping the shell script running. the final & runs the command in the background, which ensures control of the shell is returned to the user as soon as the process is spawned; since there are no additional commands in the shell script and all remaining ties have been severed (see points 1 and 2 above), Automator finally agrees that the task has probably done all it was expected to do and exits it. Wrapping up Whew! That's it. It's really not that complicated, it's just that my prose is verbose, so it makes it look like there's lots and lots to do. Trust me, there isn't. My first go at solving this usability problem -- the one I originally wanted to post way back in 2014 -- was a lengthy, godawful Applescript prone to subtle breakage. This is much better. And the ability to just use a single GUI app for transparently launching and connecting to the Emacs daemon is pure bliss. While you're at it, for an even better Emacs experience, go fetch the excellent Spacemacs Emacs config distribution , which pulls this venerable piece of software screaming into the 21st century. The best editor is neither Vim nor Emacs, its Vim + Emacs! The addictive icing of Vim modal editing on the outside, a creamy Elisp core -- what more could you want from life? ;) Oh and if, like me, you love Spacemacs' snappy icon with the Evil spaceship over planet Emacs -- or if, like me, you have OCD -- you'll definitely want to switch your Emacs logo to the Spacemacs one !","tags":"os x tips"},{"url":"http://dlukes.github.io/unicode.html","title":"How computers handle text: a gentle but thorough introduction to Unicode","text":"Or, the absolute minimum every software developer linguist absolutely, positively must know about Unicode and character sets (no excuses!) Note : This text was written as part of a larger programming tutorial in Python, and the code samples are taken from an interactive session using the Jupyter notebook . As a consequence, there are digressions here and there about playing with text data in Python. These might seem: useless if what you came for is just the part about text encoding; long-winded if you already know some Python; or confusing if, on the contrary, you're not familiar with programming at all, much less with Python. If any of these is your case, my advice is: ignore the code, focus on the comments around it, they're more than enough to follow the thread of the explanation. Though if you've got a little more time, why not try some of these out in an interactive Python session ? ;) And now, without further ado... Much like any other piece of data inside a digital computer, text is represented as a series of binary digits (bits), i.e. 0's and 1's. A mapping between sequences of bits and characters is called an encoding. How many different characters your encoding can handle depends on how many bits you allow per character: with 1 bit you can have 2&#94;1 = 2 characters (one is represented by 0, the other by 1) with 2 bits you can have 2&#94;2 = 4 characters(represented by 00, 01, 10 and 11) etc. The oldest encoding still in widespread use (it's what makes the Internet and the web tick) is ASCII , which is a 7-bit encoding: In [1]: 2 ** 7 Out[1]: 128 This means it can represent 128 different characters , which comfortably fits the basic Latin alphabet (both lowercase and uppercase), Arabic numerals, punctuation and some \"control characters\" which were primarily useful on the old teletype terminals for which ASCII was designed. For instance, the letter \"A\" corresponds to the number 65 ( 1000001 in binary, see below). \"ASCII\" stands for \" American Standard Code for Information Interchange\" -- which explains why there are no accented characters, for instance. Nowadays, ASCII is represented using 8 bits (== 1 byte), because that's the unit of computer memory which has become ubiquitous (in terms of both hardware and software assumptions), but still uses only 7 bits' worth of information. In [2]: 2 ** 8 Out[2]: 256 In [3]: # how to find out the binary representation of a decimal number? \"{:b}\" . format ( 65 ) Out[3]: &apos;1000001&apos; In [4]: # Digression/explanation: the format() method # # the format() string method inserts its arguments into the string # wherever there is a \"{}\" \"{} {} {}\" . format ( \"foo\" , \"bar\" , \"baz\" ) Out[4]: &apos;foo bar baz&apos; In [5]: # you can also specify a different order by using (zero-based) # positional indices -- or even repeating them \"{1} {0} {1}\" . format ( \"foo\" , \"bar\" ) Out[5]: &apos;bar foo bar&apos; In [6]: # for long strings with many insertions, where you might mess up the # order of arguments, keyword arguments are also available \"{foo_arg} {bar_arg}\" . format ( bar_arg = \"bar\" , foo_arg = \"foo\" ) Out[6]: &apos;foo bar&apos; In [7]: # and you can also request various formatting adjustments or conversions # to be made by specifying them after a \":\" -- e.g. \"b\" prints a given # number in its binary representation \"{:b}\" . format ( 45 ) Out[7]: &apos;101101&apos; In [8]: # or simply bin ( 45 ) # but that has an ugly \"0b\" in front, and we would've missed out on # format() if we'd used that directly! Out[8]: &apos;0b101101&apos; What happens in the range [128; 256) is not covered by the ASCII standard. In the 1990s, many encodings were standardized which used this range for their own purposes, usually representing additional accented characters used in a particular region. E.g. Czech (and Slovak, Polish...) alphabets can be represented using the ISO latin-2 encoding, or Microsoft's cp-1250 . Encodings which stick with the same character mappings as ASCII in the range [0; 128) and represent them physically in the same way (as 1 byte) , while potentially adding more character mappings beyond that, are called ASCII -compatible . ASCII compatibility is a good thing‚Ñ¢, because when you start reading a character stream in a computer, there's no way to know in advance what encoding it is in (unless it's a file you've encoded yourself). So in practice, a heuristic has been established to start reading the stream assuming it is ASCII by default, and switch to a different encoding if evidence becomes available that motivates it. For instance, HTML files should all start something like this: <!DOCTYPE html> < html > < head > < meta charset = \"utf-8\" /> ... This way, whenever a program wants to read a file like this, it can start off with ASCII , waiting to see if it reaches the charset (i.e. encoding) attribute, and once it does, it can switch from ASCII to that encoding ( UTF-8 here) and restart reading the file, now fairly sure that it's using the correct encoding. This trick works only if we can assume that whatever encoding the rest of the file is in, the first few lines can be considered as ASCII for all practical intents and purposes. Without the charset attribute, the only way to know if the encoding is right would be for you to look at the rendered text and see if it makes sense; if it did not, you'd have to resort to trial and error, manually switching the encodings and looking for the one in which the numbers behind the characters stop coming out as gibberish and are actually translated into intelligible text. In [9]: # Let's take a look at printable characters in the latin-2 character # set. Each mapping is called a \"codepoint\": it is a correspondence # between an integer and a character. import codecs latin2 = [] for codepoint in range ( 256 ): byte = bytes ([ codepoint ]) character = codecs . decode ( byte , encoding = \"latin2\" ) if character . isprintable (): latin2 . append (( codepoint , character )) latin2 Out[9]: [(32, &apos; &apos;), (33, &apos;!&apos;), (34, &apos;\"&apos;), (35, &apos;#&apos;), (36, &apos;$&apos;), (37, &apos;%&apos;), (38, &apos;&&apos;), (39, \"&apos;\"), (40, &apos;(&apos;), (41, &apos;)&apos;), (42, &apos;*&apos;), (43, &apos;+&apos;), (44, &apos;,&apos;), (45, &apos;-&apos;), (46, &apos;.&apos;), (47, &apos;/&apos;), (48, &apos;0&apos;), (49, &apos;1&apos;), (50, &apos;2&apos;), (51, &apos;3&apos;), (52, &apos;4&apos;), (53, &apos;5&apos;), (54, &apos;6&apos;), (55, &apos;7&apos;), (56, &apos;8&apos;), (57, &apos;9&apos;), (58, &apos;:&apos;), (59, &apos;;&apos;), (60, &apos;<&apos;), (61, &apos;=&apos;), (62, &apos;>&apos;), (63, &apos;?&apos;), (64, &apos;@&apos;), (65, &apos;A&apos;), (66, &apos;B&apos;), (67, &apos;C&apos;), (68, &apos;D&apos;), (69, &apos;E&apos;), (70, &apos;F&apos;), (71, &apos;G&apos;), (72, &apos;H&apos;), (73, &apos;I&apos;), (74, &apos;J&apos;), (75, &apos;K&apos;), (76, &apos;L&apos;), (77, &apos;M&apos;), (78, &apos;N&apos;), (79, &apos;O&apos;), (80, &apos;P&apos;), (81, &apos;Q&apos;), (82, &apos;R&apos;), (83, &apos;S&apos;), (84, &apos;T&apos;), (85, &apos;U&apos;), (86, &apos;V&apos;), (87, &apos;W&apos;), (88, &apos;X&apos;), (89, &apos;Y&apos;), (90, &apos;Z&apos;), (91, &apos;[&apos;), (92, &apos;\\\\&apos;), (93, &apos;]&apos;), (94, &apos;&#94;&apos;), (95, &apos;_&apos;), (96, &apos;`&apos;), (97, &apos;a&apos;), (98, &apos;b&apos;), (99, &apos;c&apos;), (100, &apos;d&apos;), (101, &apos;e&apos;), (102, &apos;f&apos;), (103, &apos;g&apos;), (104, &apos;h&apos;), (105, &apos;i&apos;), (106, &apos;j&apos;), (107, &apos;k&apos;), (108, &apos;l&apos;), (109, &apos;m&apos;), (110, &apos;n&apos;), (111, &apos;o&apos;), (112, &apos;p&apos;), (113, &apos;q&apos;), (114, &apos;r&apos;), (115, &apos;s&apos;), (116, &apos;t&apos;), (117, &apos;u&apos;), (118, &apos;v&apos;), (119, &apos;w&apos;), (120, &apos;x&apos;), (121, &apos;y&apos;), (122, &apos;z&apos;), (123, &apos;{&apos;), (124, &apos;|&apos;), (125, &apos;}&apos;), (126, &apos;~&apos;), (161, &apos;ƒÑ&apos;), (162, &apos;Àò&apos;), (163, &apos;≈Å&apos;), (164, &apos;¬§&apos;), (165, &apos;ƒΩ&apos;), (166, &apos;≈ö&apos;), (167, &apos;¬ß&apos;), (168, &apos;¬®&apos;), (169, &apos;≈†&apos;), (170, &apos;≈û&apos;), (171, &apos;≈§&apos;), (172, &apos;≈π&apos;), (174, &apos;≈Ω&apos;), (175, &apos;≈ª&apos;), (176, &apos;¬∞&apos;), (177, &apos;ƒÖ&apos;), (178, &apos;Àõ&apos;), (179, &apos;≈Ç&apos;), (180, &apos;¬¥&apos;), (181, &apos;ƒæ&apos;), (182, &apos;≈õ&apos;), (183, &apos;Àá&apos;), (184, &apos;¬∏&apos;), (185, &apos;≈°&apos;), (186, &apos;≈ü&apos;), (187, &apos;≈•&apos;), (188, &apos;≈∫&apos;), (189, &apos;Àù&apos;), (190, &apos;≈æ&apos;), (191, &apos;≈º&apos;), (192, &apos;≈î&apos;), (193, &apos;√Å&apos;), (194, &apos;√Ç&apos;), (195, &apos;ƒÇ&apos;), (196, &apos;√Ñ&apos;), (197, &apos;ƒπ&apos;), (198, &apos;ƒÜ&apos;), (199, &apos;√á&apos;), (200, &apos;ƒå&apos;), (201, &apos;√â&apos;), (202, &apos;ƒò&apos;), (203, &apos;√ã&apos;), (204, &apos;ƒö&apos;), (205, &apos;√ç&apos;), (206, &apos;√é&apos;), (207, &apos;ƒé&apos;), (208, &apos;ƒê&apos;), (209, &apos;≈É&apos;), (210, &apos;≈á&apos;), (211, &apos;√ì&apos;), (212, &apos;√î&apos;), (213, &apos;≈ê&apos;), (214, &apos;√ñ&apos;), (215, &apos;√ó&apos;), (216, &apos;≈ò&apos;), (217, &apos;≈Æ&apos;), (218, &apos;√ö&apos;), (219, &apos;≈∞&apos;), (220, &apos;√ú&apos;), (221, &apos;√ù&apos;), (222, &apos;≈¢&apos;), (223, &apos;√ü&apos;), (224, &apos;≈ï&apos;), (225, &apos;√°&apos;), (226, &apos;√¢&apos;), (227, &apos;ƒÉ&apos;), (228, &apos;√§&apos;), (229, &apos;ƒ∫&apos;), (230, &apos;ƒá&apos;), (231, &apos;√ß&apos;), (232, &apos;ƒç&apos;), (233, &apos;√©&apos;), (234, &apos;ƒô&apos;), (235, &apos;√´&apos;), (236, &apos;ƒõ&apos;), (237, &apos;√≠&apos;), (238, &apos;√Æ&apos;), (239, &apos;ƒè&apos;), (240, &apos;ƒë&apos;), (241, &apos;≈Ñ&apos;), (242, &apos;≈à&apos;), (243, &apos;√≥&apos;), (244, &apos;√¥&apos;), (245, &apos;≈ë&apos;), (246, &apos;√∂&apos;), (247, &apos;√∑&apos;), (248, &apos;≈ô&apos;), (249, &apos;≈Ø&apos;), (250, &apos;√∫&apos;), (251, &apos;≈±&apos;), (252, &apos;√º&apos;), (253, &apos;√Ω&apos;), (254, &apos;≈£&apos;), (255, &apos;Àô&apos;)] Using the 8th bit (and thus the codepoint range [128; 256)) solves the problem of handling languages with character sets different than that of American English, but introduces a lot of complexity -- whenever you come across a text file with an unknown encoding, it might be in one of literally dozens of encodings. Additional drawbacks include: how to handle multilingual text with characters from many different alphabets, which are not part of the same 8-bit encoding? how to handle writing systems which have way more than 256 \"characters\", e.g. Chinese, Japanese and Korean (CJK) ideograms? For these purposes, a standard encoding known as Unicode was developed which strives for universal coverage of all possible character sets. Unicode is much bigger than the encodings we've seen so far -- its most frequently used subset, the Basic Multilingual Plane , has 2&#94;16 codepoints, but overall the number of codepoints is past 1M and there's room to accommodate many more. In [10]: 2 ** 16 Out[10]: 65536 Now, the most straightforward representation for 2&#94;16 codepoints is what? Well, it's simply using 16 bits per character, i.e. 2 bytes. That encoding exists, it's called UTF-16 , but consider the drawbacks: we've lost ASCII compatibility by the simple fact of using 2 bytes per character instead of 1 (encoding \"a\" as 01100001 or 01100001|00000000 , with the | indicating an imaginary boundary between bytes, is not the same thing) encoding a string in a character set which uses a \"reasonable\" number of characters (like any European language) now takes twice as much space without any added benefit (which is probably not a good idea, given the general dominance of English -- one of those \"reasonable character set size\" languages -- in electronic communication) Looks like we'll have to think outside the box. The box in question here is called fixed-width encodings -- all of the encoding schemes we've encountered so far were fixed-width, meaning that each character was represented by either 7, 8 or 16 bits. In other word, you could jump around the string in multiples of 7, 8 or 16 and always land at the beginning of a character. (Not exactly true for UTF-16 , because it is something more than just a \"16-bit ASCII \": it has ways of handling characters beyond 2&#94;16 using so-called surrogate sequences -- but you get the gist.) \"UTF\" stands for \"Unicode Transformation Format\". The smart idea that some bright people have come up with was to use a variable-width encoding . The most ubiquitous one currently is UTF-8 , which we've already met in the HTML example above. UTF-8 is ASCII -compatible, i.e. the 1's and 0's used to encode text containing only ASCII characters are the same regardless of whether you use ASCII or UTF-8 : it's a sequence of 8-bit bytes. But UTF-8 can also handle many more additional characters, as defined by the Unicode standard, by using progressively longer and longer sequences of bits. In [11]: def print_as_binary_utf8 ( string ): \"\"\"Prints binary representation of string as encoded by UTF-8. \"\"\" # encode the string as UTF-8 byte = string . encode ( \"utf-8\" ) # get the hexadecimal representation of the bytes hexa = byte . hex () # convert the hexadecimal representation to a decimal integer inte = int ( hexa , base = 16 ) # create a string containing the integer, formatted as binary binary_str = \"'{}' encoded in UTF-8 is: {:b}\" . format ( string , inte ) # print that binary representation. whew! if there's an easier # way (e.g. at least skipping the intermediate hexadecimal # representation), please let me know! print ( binary_str ) print_as_binary_utf8 ( \"A\" ) # the representations... print_as_binary_utf8 ( \"ƒç\" ) # ... keep... print_as_binary_utf8 ( \"Â≠ó\" ) # ... getting longer. &apos;A&apos; encoded in UTF-8 is: 1000001 &apos;ƒç&apos; encoded in UTF-8 is: 1100010010001101 &apos;Â≠ó&apos; encoded in UTF-8 is: 111001011010110110010111 How does it achieve that? The obvious problem here is that with a fixed-width encoding, you just chop up the string at regular intervals (7, 8, 16 bits) and you know that each interval represents one character. So how do you know where to chop up a variable width-encoded string, if each character can take up a different number of bits? We won't go into the details, but essentially, the trick is to use some of the bits in the representation of a codepoint to store information not about which character it is (whether it's an \"A\" or a \"Â≠ó\"), but how many bits it occupies . In other words, if you want to skip ahead 10 characters in a string encoded with a variable width-encoding, you can't just skip 10 * 7 or 8 or 16 bits; you have to read all the intervening characters to figure out how much space they take up. There's much more to Unicode than this simple introduction, for instance the various ways diacritics are handled: \"ƒç\" can be represented either as a single codepoint ( LATIN SMALL LETTER C WITH CARON -- all Unicode codepoints have cute names like this) or a sequence of two codepoints, the character \"c\" and a combining diacritic mark ( COMBINING CARON ). You can search for the codepoints corresponding to Unicode characters e.g. here and play with them in Python using the chr(0xXXXX) built-in function or with the special string escape sequence \\uXXXX (where XXXX is the hexadecimal representation of the codepoint) -- both are ways to get the character corresponding to the given codepoint: In [12]: # \"ƒç\" as LATIN SMALL LETTER C WITH CARON, codepoint 010D print ( chr ( 0x010D )) print ( \" \\u010D \" ) ƒç ƒç In [13]: # \"ƒç\" as a sequence of LATIN SMALL LETTER C, codepoint 0063, and # COMBINING CARON, codepoint 030c print ( chr ( 0x0063 ) + chr ( 0x030c )) print ( \" \\u0063\\u030c \" ) cÃå cÃå Hexadecimal is just a more convenient way of representing sequences of bits, where each of the X 's can be a number between 0 and 15 (10--15 are represented by the letters A--F). Each hexadecimal number can thus represent 16 different values, and therefore it can stand in for a sequence of 4 bits (2&#94;4 == 16). Without worrying too much about the details right now, our old friend ASCII uppercase \"A\" can be thought of equivalently either as decimal 65, binary 1000001 , or hexadecimal 0x41 (the \"0x\" prefix is there just to say \"this is a hexadecimal number\"). > Binary and hexadecimal numbers are often written padded with leading zeros to some number of bytes, but these have no effect on the value, much like decimal 42 and 00000042 are effectively the same numbers. In [14]: # use hex() to find out the hexadecimal representation of a decimal # integer... hex ( 99 ) Out[14]: &apos;0x63&apos; In [15]: # ... and int() to go back int ( 0x63 ) Out[15]: 99 This means you have to be careful when working with languages that use accents, because for a computer, the two possible representations are of course different strings , even though for you, they're conceptually the same: In [16]: s1 = \" \\u010D \" s2 = \" \\u0063\\u030c \" # s1 and s2 look the same to the naked eye... print ( s1 , s2 ) ƒç cÃå In [17]: # ... but in the eternal realm of Plato's Ideas, they're not s1 == s2 Out[17]: False Watch out, they even have different lengths ! This might come to bite you if you're trying to compute the length of a word in letters. In [18]: print ( \"s1 is\" , len ( s1 ), \"character(s) long.\" ) print ( \"s2 is\" , len ( s2 ), \"character(s) long.\" ) s1 is 1 character(s) long. s2 is 2 character(s) long. Generally, most text out there will use the first, single-codepoint approach whenever possible, and pre-packaged linguistic corpora will try to be consistent about this (unless they come from the web, which always warrants being suspicious and defensive about your material). If you're worried about inconsistencies in your data, you can perform a normalization : In [19]: from unicodedata import normalize # NFC stands for Normal Form C; this normalization applies a canonical # decomposition (into a multi-codepoint representation) followed by a # canonical composition (into a single-codepoint representation) s1 = normalize ( \"NFC\" , s1 ) s2 = normalize ( \"NFC\" , s2 ) s1 == s2 Out[19]: True Let's wrap things up by saying that Python itself uses Unicode internally and (mostly?) assumes UTF-8 when reading files. So if you're using UTF-8 as is increasingly the case (and you should be), you won't have to worry too much about encodings, except perhaps for normalization. In [20]: # a good idea when dealing with Unicode text from an unknown and # unreliable source is to look at the set of codepoints contained # in it and eliminate or replace those that shouldn't be there import unicodedata def inspect_codepoints ( text ): charset = set () for char in text : charset . add ( char ) for char in sorted ( charset ): info = r\"{} (\\u{:04x}): {} (category: {})\" . format ( char , ord ( char ), unicodedata . name ( char ), unicodedata . category ( char )) print ( info ) # depending on your font configuration, it may be very hard to spot # the two intruders in the sentence below that look like regular # letters but really are specialized variants; you might want # to replace them before doing further text processing... inspect_codepoints ( \"Intruders here, good ùóçhin…° I checked.\" ) (\\u0020): SPACE (category: Zs) , (\\u002c): COMMA (category: Po) . (\\u002e): FULL STOP (category: Po) I (\\u0049): LATIN CAPITAL LETTER I (category: Lu) c (\\u0063): LATIN SMALL LETTER C (category: Ll) d (\\u0064): LATIN SMALL LETTER D (category: Ll) e (\\u0065): LATIN SMALL LETTER E (category: Ll) g (\\u0067): LATIN SMALL LETTER G (category: Ll) h (\\u0068): LATIN SMALL LETTER H (category: Ll) i (\\u0069): LATIN SMALL LETTER I (category: Ll) k (\\u006b): LATIN SMALL LETTER K (category: Ll) n (\\u006e): LATIN SMALL LETTER N (category: Ll) o (\\u006f): LATIN SMALL LETTER O (category: Ll) r (\\u0072): LATIN SMALL LETTER R (category: Ll) s (\\u0073): LATIN SMALL LETTER S (category: Ll) t (\\u0074): LATIN SMALL LETTER T (category: Ll) u (\\u0075): LATIN SMALL LETTER U (category: Ll) …° (\\u0261): LATIN SMALL LETTER SCRIPT G (category: Ll) ùóç (\\u1d5cd): MATHEMATICAL SANS-SERIF SMALL T (category: Ll) In [21]: # ... because of course, for a computer, the word \"thing\" written with # two different variants of \"g\" is really just two different words, which # is probably not what you want \"thing\" == \"thin…°\" Out[21]: False In any case, here's what happens when processing text with Python (\"Unicode\" in the central box stands for Python's internal representation of Unicode, which is not UTF-8 nor UTF-16 ): (Image shamelessly hotlinked from / courtesy of the NLTK Book . Go check it out, it's an awesome intro to Python programming for linguists!) A terminological postscript: we've been using some terms a bit informally and for the most part it's okay, but it's good to get the distinctions straight in one's head at least once. So, a character set is a mapping between codepoints (integers) and characters . We may for instance say that in our character set, the integer 99 corresponds to the character \"c\". On the other hand, an encoding is a mapping between a codepoint (an integer) and a physical sequence of 1's and 0's that represent it in memory . With fixed-width encodings, this mapping is generally straightforward -- the 1's and 0's directly represent the given integer, only in binary and padded with zeros to fit the desired width. With variable-width encodings, as the necessity creeps in to include the information about how many bits are spanned by the current character, this straightforward correspondence breaks down. A comparison might be helpful here: as encodings, UTF-8 and UTF-16 both use the same character set -- the same integers corresponding to the same characters. But since they're different encodings , when the time comes to turn these integers into sequences of bits to store in a computer's memory, each of them generates a different one. For more on Unicode, a great read already hinted at above is Joel Spolsky's The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) . To make the discussion digestible for newcomers, I sometimes slightly distorted facts about how things are \"really really\" done. And some inaccuracies may be genuine mistakes. In any case, please let me know in the comments! I'm grateful for feedback and looking to improve this material; I'll fix the mistakes and consider ditching some of the simplifications if they prove untenable :)","tags":"ling"},{"url":"http://dlukes.github.io/kontext-interface-tweak-update.html","title":"√öprava rozhran√≠ konkordanceru KonText -- vylep≈°en√° verze","text":"P≈ôed nƒõjakou dobou jsem zde vyvƒõsil skript , jeho≈æ pomoc√≠ lze lehce \"p≈ôeskl√°dat\" a upravit rozhran√≠ korpusov√©ho konkordanceru KonText : menu je um√≠stƒõn√© po stranƒõ m√≠sto naho≈ôe a permanentnƒõ rozbalen√© nad vyhledanou konkordanc√≠ je um√≠stƒõn rychl√Ω hledac√≠ box, v nƒõm≈æ lze p≈ôedchoz√≠ dotaz pohodlnƒõ upravit V√≠c o motivaci tƒõchto √∫prav se doƒçtete v p≈Øvodn√≠m ƒçl√°nku . St√°le plat√≠, ≈æe ƒåNK nem√° v pl√°nu tyto zmƒõny zaƒçlenit p≈ô√≠mo do ofici√°ln√≠ verze KonTextu, zejm√©na proto, ≈æe rychl√Ω hledac√≠ box sice v jist√Ωch situac√≠ch m≈Ø≈æe b√Ωt u≈æiteƒçn√Ω, nicm√©nƒõ oproti standardn√≠mu formul√°≈ôi Nov√Ω dotaz v√Ωraznƒõ omezuje mo≈ænosti pro zad√°n√≠ dotazu. Vylep≈°en√° verze, kter√° je k dispozici n√≠≈æe, odstra≈àuje nƒõkter√© p≈ôedchoz√≠ nedostatky skriptu: rychl√Ω hledac√≠ box nad konkordanc√≠ je vƒõt≈°√≠, ukazuje v≈ædy CQL podobu posledn√≠ho zadan√©ho dotazu 1 , a p≈ôedev≈°√≠m z≈Øst√°v√° zobrazen√Ω i bƒõhem listov√°n√≠ konkordanc√≠ (tj. nen√≠ k dispozici jen na jej√≠ prvn√≠ str√°nce). Dotaz lze nyn√≠ nav√≠c pro vƒõt≈°√≠ p≈ôehlednost rozdƒõlit do v√≠ce ≈ô√°dk≈Ø, tak≈æe opƒõtovn√© vyhled√°v√°n√≠ se novƒõ spou≈°t√≠ stiskem kombinace kl√°ves Ctrl+Enter (m√≠sto jen Enteru). V√Ωsledn√© upraven√© rozhran√≠ KonText vypad√° st√°le podobnƒõ: Postup instalace skriptu Nov√° verze skriptu je k dispozici zde: Kroky k jeho zprovoznƒõn√≠ z≈Øst√°vaj√≠ stejn√©: Nainstalovat si do sv√©ho prohl√≠≈æeƒçe plugin Tampermonkey , pokud pou≈æ√≠v√°te Chrome, nebo Greasemonkey , pokud pou≈æ√≠v√°te Firefox. (Pokud pou≈æ√≠v√°te Internet Explorer, budete muset doƒçasnƒõ p≈ôesedlat na Chrome nebo Firefox.) Testovan√Ω je skript zat√≠m jen na Chromu. Zalo≈æit v dan√©m pluginu nov√Ω skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nov√©ho skriptu a nahradit ji skriptem, kter√Ω si zkop√≠rujete v√Ω≈°e. Skript ulo≈æit. Pou≈æ√≠vat KonText jako norm√°lnƒõ -- skript u≈æ by podle adresy mƒõl s√°m poznat, ≈æe se m√° spustit. Pokud se tak nestane, nejsp√≠≈° to znamen√°, ≈æe je prohl√≠≈æeƒçov√Ω plugin (Tampermonkey nebo Greasemonkey) deaktivovan√Ω a je pot≈ôeba jej znovu aktivovat. V p≈ôedchoz√≠ verzi se po aplikaci libovoln√©ho filtru zmƒõnil obsah hledac√≠ho boxu na parametry filtrov√°n√≠. ‚Ü©","tags":"ling"},{"url":"http://dlukes.github.io/kontext-interface-tweak.html","title":"√öprava rozhran√≠ konkordanceru KonText","text":"!POZOR! K dispozici je nyn√≠ vylep≈°en√° verze n√≠≈æe popsan√©ho skriptu . Hled√°n√≠ v korpusech ƒåNK ƒåesk√Ω n√°rodn√≠ korpus je sb√≠rka jazykov√Ωch korpus≈Ø ƒç√°steƒçnƒõ vytv√°≈ôen√Ωch √östavem ƒåesk√©ho n√°rodn√≠ho korpusu a ƒç√°steƒçnƒõ jin√Ωmi institucemi. V≈°echny jsou hostovan√© na jednom serveru a dostupn√© skrz r≈Øzn√° vyhled√°vac√≠ rozhran√≠ (tzv. konkordancery ), nap≈ô. NoSke , Bonito ƒçi nejnovƒõji KonText . Koncem b≈ôezna 2015 ov≈°em bude podpora star≈°√≠ch rozhran√≠ ukonƒçena a nad√°le p≈Øjde k dat≈Øm v ƒåNK p≈ôistupovat prim√°rnƒõ pouze p≈ôes KonText. (Pokud v√°m odstavec v√Ω≈°e ned√°v√° p≈ô√≠li≈° smysl, s jazykov√Ωmi korpusy se setk√°v√°te poprv√©, ale chcete se dozvƒõdƒõt v√≠c, radƒõji si m√≠sto tohoto postu p≈ôeƒçtƒõte, k ƒçemu je takov√Ω korpus dobr√Ω , a zkuste si v nƒõm nƒõco pro zaj√≠mavost vyhledat . Pokud se v√°m p≈ôi vzpom√≠nce na Bonito ƒçi NoSke naopak zaskvƒõla slza v oku, ƒçtƒõte d√°l!) KonText vs. Bonito / NoSke KonText m√° oproti star≈°√≠m rozhran√≠m ≈ôadu v√Ωhod -- bohat≈°√≠ funkcionalitu, mnoh√© pom≈Øcky, kter√© v√°m pomohou se zad√°n√≠m slo≈æitƒõj≈°√≠ch dotaz≈Ø (sestaven√≠ morfologick√©ho tagu ƒçi podm√≠nky within ), a v neposledn√≠ ≈ôadƒõ mnohem l√©pe vypad√°, co≈æ kup≈ô√≠kladu mnƒõ p≈ôi pr√°ci p≈Øsob√≠ jako balz√°m na du≈°i. Nicm√©nƒõ dlouholet√≠ u≈æivatel√© ƒåNK byli jednodu≈°e zvykl√≠ na nƒõkter√© aspekty Bonita a NoSke, kter√© jim teƒè v KonTextu chyb√≠. Onehdy p≈ôi rozhovoru s jedn√≠m z nich vyplavaly na povrch jako hodnƒõ d≈Øle≈æit√© dvƒõ st√≠≈ænosti: Vrchn√≠ menu v KonTextu je z√°ke≈ôn√©, schov√°v√° se, ƒçlovƒõk nem√° p≈ôehled nad dostupn√Ωmi funkcemi. Oproti tomu NoSke m√° menu po stranƒõ a je permanentnƒõ rozvinut√©, tak≈æe u≈æivatel m√° v≈°echny mo≈ænosti interakce s konkordanc√≠ soustavnƒõ jako na dlani. Po zad√°n√≠ dotazu ƒçlovƒõk ƒçasto na z√°kladƒõ konkordance zjist√≠, ≈æe jej pot≈ôebuje je≈°tƒõ trochu upravit / zjemnit. KonText si sice p≈ôedchoz√≠ dotazy pamatuje, je ale pot≈ôeba se k nim doklikat; ≈°ikovnƒõj≈°√≠ by bylo, kdyby tato mo≈ænost byla dostupn√° p≈ô√≠mo ze str√°nky konkordance v podobƒõ nƒõjak√©ho zjednodu≈°en√©ho hledac√≠ho boxu. (NoSke tohle vlastnƒõ taky neum√≠, v Bonitu je to jednodu≈°≈°√≠.) V obou p≈ô√≠padech jde o smyslupln√© po≈æadavky, jen≈æe KonText je pomƒõrnƒõ velk√° a slo≈æit√° aplikace, tak≈æe i pokud se ƒåNK rozhodne do n√≠ tyto podnƒõty v nƒõjak√© podobƒõ zapracovat (nap≈ô. jako mo≈ænost p≈ôepnut√≠ zobrazen√≠ menu), bude nƒõjakou chv√≠li trvat, ne≈æ se implementace navrhne, vytvo≈ô√≠, ≈ô√°dnƒõ otestuje a koneƒçnƒõ dostane k u≈æivatel≈Øm. Nicm√©nƒõ aby bylo mo≈æn√© alespo≈à vyzkou≈°et, jak by zm√≠nƒõn√© zmƒõny vypadaly v praxi, dal jsem dohromady kr√°tk√Ω skript, kter√Ω ji≈æ v prohl√≠≈æeƒçi nahran√Ω KonText trochu \"p≈ôestav√≠\" a uprav√≠. V√Ωsledek vypad√° n√°sledovnƒõ: Rovnou p≈ôedes√≠l√°m: ten skript je nevzhledn√Ω bastl p≈ôilepen√Ω na KonText zvnƒõj≈°ku; proto taky bylo mo≈æn√© jej d√°t dohromady pomƒõrnƒõ rychle, proto≈æe si neklade n√°rok na spolehlivost, kter√° se vy≈æaduje od ofici√°ln√≠ verze KonTextu. Je to sp√≠≈° prototyp, jeho≈æ √∫ƒçelem je otestovat v√Ω≈°e popsan√© zmƒõny v praxi a z√≠skat p≈ôedstavu o tom, zda a do jak√© m√≠ry jsou p≈ô√≠nosn√©. (Vlastn√≠ zku≈°enost: po chv√≠li pou≈æ√≠v√°n√≠ mi p≈ôijde p≈ô√≠datn√Ω hledac√≠ box nad konkordanc√≠ hodnƒõ ≈°ikovn√Ω a u≈æiteƒçn√Ω.) Teƒè k j√°dru pudla: pokud m√°te z√°jem, m≈Ø≈æete si KonText takto k obrazu sv√©mu (resp. k obr√°zku o odstavec v√Ω≈°) upravit tak√© a vyzkou≈°et, jak v√°m takov√© nastaven√≠ vyhovuje. Kdy≈æ se v√°m jedna z √∫prav bude l√≠bit (nebo v√°s u toho napadne jin√°, kterou by si KonText zaslou≈æil), m≈Ø≈æete pak zadat po≈æadavek na nov√Ω feature . N√°vod, jak si KonText upravit, n√°sleduje n√≠≈æe. Postup instalace skriptu Skript samotn√Ω je k dispozici zde: K jeho zprovoznƒõn√≠ jsou pot≈ôeba n√°sleduj√≠c√≠ kroky: Nainstalovat si do sv√©ho prohl√≠≈æeƒçe plugin Tampermonkey , pokud pou≈æ√≠v√°te Chrome, nebo Greasemonkey , pokud pou≈æ√≠v√°te Firefox. (Pokud pou≈æ√≠v√°te Internet Explorer, budete muset doƒçasnƒõ p≈ôesedlat na Chrome nebo Firefox.) Testovan√Ω je skript zat√≠m jen na Chromu. Zalo≈æit v dan√©m pluginu nov√Ω skript (pro Chrome je tutorial zde , pro Firefox zde ). Smazat kostru nov√©ho skriptu a nahradit ji skriptem, kter√Ω si zkop√≠rujete v√Ω≈°e. Skript ulo≈æit. Pou≈æ√≠vat KonText jako norm√°lnƒõ -- skript u≈æ by podle adresy mƒõl s√°m poznat, ≈æe se m√° spustit. Pokud se tak nestane, nejsp√≠≈° to znamen√°, ≈æe je prohl√≠≈æeƒçov√Ω plugin (Tampermonkey nebo Greasemonkey) deaktivovan√Ω a je pot≈ôeba jej znovu aktivovat. Omezen√≠ Skript m√° pravdƒõpodobnƒõ hromadu drobn√Ωch much, na kter√© se mi zat√≠m nepoda≈ôilo p≈ôij√≠t -- budu se je sna≈æit pr≈Øbƒõ≈ænƒõ opravovat, kdy≈æ na nƒõ padnu, nebo kdy≈æ mi o nich d√°te vƒõdƒõt . Krom toho m√° i nƒõkter√© mouchy, o nich≈æ u≈æ v√≠m, ale bohu≈æel toho s nimi nejde moc dƒõlat. Asi nejn√°padnƒõj≈°√≠ je, ≈æe p≈ôidan√Ω hledac√≠ box funguje jen na tƒõch str√°nk√°ch, kde je p≈Øvodn√≠ dotaz i souƒç√°st√≠ adresy URL (co≈æ nejsou v≈°echny -- t≈ôeba kdy≈æ zaƒçnete listovat konkordanc√≠ na druhou str√°nku a d√°l, dotaz je z adresy vyjmut a pomocn√Ω hledac√≠ box tedy zmiz√≠ ). Ale vzhledem k tomu, ≈æe jeho hlavn√≠ √∫ƒçel m√° b√Ωt mo≈ænost lehce upravit dotaz po prvn√≠m rychl√©m nahl√©dnut√≠ do konkordance, snad to nebude takov√Ω probl√©m. Pokud nƒõkdy bude podobn√Ω box ≈ô√°dnƒõ p≈ôid√°n p≈ô√≠mo do KonTextu, takov√Ωmi nedostatky samoz≈ôejmƒõ trpƒõt nebude. A je≈°tƒõ k pou≈æ√≠v√°n√≠ p≈ôidan√©ho hledac√≠ho boxu : Typ dotazu, kter√Ω je do nƒõj pot≈ôeba zadat, je stejn√Ω jako ten, kter√Ω jste p≈ôi prvotn√≠m vyhled√°n√≠ konkordance zadali na str√°nce Nov√Ω dotaz . Pokud tento prvotn√≠ dotaz byl Z√°kladn√≠ dotaz, m≈Ø≈æete pomoc√≠ rychl√©ho boxu zadat jin√Ω Z√°kladn√≠ dotaz; pokud to byl CQL dotaz, m≈Ø≈æete ho upravit zas jen na dal≈°√≠ CQL dotaz. D≈Øvodem je, ≈æe smyslem tohoto pomocn√©ho boxu nen√≠ nahradit plnohodnotn√Ω formul√°≈ô pro zad√°n√≠ dotazu, jen poskytnout rychlou mo≈ænost, jak ji≈æ zadan√Ω dotaz upravit . Pomocn√Ω hledac√≠ box se objev√≠ i pot√©, co na konkordanci provedete filtrov√°n√≠. V takov√© situaci se d√° pou≈æ√≠t k tomu, abyste pozmƒõnili zad√°n√≠ aktu√°ln√≠ho filtru , tj. filtrov√°n√≠ se provede znovu na p≈Øvodn√≠ konkordanci, ne na t√©to ji≈æ filtrovan√©. Pokud chcete opakovanƒõ filtrovat tu samou konkordanci a postupnƒõ podle dan√Ωch krit√©ri√≠ vy≈ôazovat / p≈ôid√°vat ≈ô√°dky, je pot≈ôeba m√≠sto hledac√≠ho boxu opakovanƒõ pou≈æ√≠t menu Filtr . Komu si stƒõ≈æovat, kdy≈æ to nebude fungovat Skript je volnƒõ ≈°i≈ôiteln√Ω pod licenc√≠ GNU GPL v3 , tak≈æe se na nƒõj nev√°≈æe ≈æ√°dn√° z√°ruka. Kdy≈æ se v√°m ale nebude da≈ôit jej zprovoznit, r√°d se pokus√≠m pomoct! Staƒç√≠ se ozvat na adresu uvedenou zde .","tags":"ling"},{"url":"http://dlukes.github.io/beyond-semver.html","title":"Beyond semantic versioning? (cross-post)","text":"Background Ever since I first read about semantic versioning , I've thought of it as a neat idea. But only recently did it occur to me that what I liked about the idea was its goal, much less its execution (more on that below). What made it obvious was this lengthy discussion about breaking changes introduced in v1.7 of underscore.js without an accompanying major version bump. Even though I still think sticking to semver is the right thing to do if your community of users expects you to (even if you don't personally like the system), I am convinced there are fundamentally better ways of dealing with the problem of safely and consistently updating dependencies. It made me want to add my two cents to the discussion , as someone who's more of a dabbler in programming and not really part of the community, so feel free to ignore me :) I attach my commentary below for reference (it's virtually the same text as in the link above). tl;dr semver is trying to do the right thing, but doing it wrong -- instead of implicitly encoding severity of change information in version numbers , explicit keywords like :patch, :potentially-breaking or :major-api-change would make much more sense. More verbosely I've always found the goals of semver worthy, but this thread has made me realize that while its aims are commendable, its methods are kind of broken: semver tries to take an existing semiotic system (= version numbers), which has developed informally and is therefore a loose convention rather than an exact spec, and reinterpret it in terms of an exact spec (or impose that spec on it). trouble is, the prior informal meaning won't go away so easily (why should it?), especially for projects that have been around longer than semver. the problem then is, since the two systems (the informal one and semver) look the same in terms of their symbolic representation, it's hard to guess which one you're dealing with by just eyeballing the version number of a library (or project in general). it's like if someone decided that \"f*ck\" should mean \"orchid\" from now on, because it's nicer -- on hearing the word, you'd never know if it's being used as the original profanity, or in its new meaning. homonymy is a pain to deal with when it's accidental (cf. NLP), so why introduce it on purpose? the job that semver set out to do should be fulfilled by a new formal means which is instantly recognizable, not by hijacking an existing one and overlaying additional interpretation on it and thus making it ambiguous . even if version numbers hadn't existed before semver, they're terribly inadequate for the purpose of conveying information about the severity of changes introduced by an update (though I understand their appeal to mathematically-minded people). they're inadequate because they're implicit -- it's a bit like if someone decided they don't need hash maps because they can make do with arrays by remembering the order in which they're adding in the key-val pairs. if I remember the order, then I know which key the given index implicitly refers to, and the result is as good as a hash map, isn't it? except it isn't. keys are useful because they have explicit semantics , making it instantly clear what kind of value you're retrieving. in the same way, encoding the information about the severity of changes into version numbers makes it implicit (in addition to being ambiguous, as stated previously). why not use explicit keyword tags along with the version number (which can be romantic, semantic -- whichever floats the dev team's boat and best reflects the progress of the project) to give a heads up as to the nature of the update? e.g. :patch, :potentially-breaking, :major-api-change etc. granted, even language is a code which needs to be learned, like semver (gross oversimplification here, but let's not get into the details of language acquisition), but since it's widely established and conventionalized for conveying the kinds of meanings semver is trying to convey, why not just use it when it's available ? why use a system (version numbers) which is less well-suited to the purpose and ambiguous to boot? (on the other hand, numbers are eminently well-suited for keeping track of which version is newer than which and how much so -- the original purpose of version numbering -- because they are designed to have orderings defined on them. by contrast, words would do a terrible job at this. if you care to indicate the evolution of your codebase, you might introduce your own disciplined romantic or sentimental versioning scheme, which ironically is a more meaningful and ergo semantic way of doing versioning than semver, because it sticks to the conventional semantics of numbers (the closer the numbers, the more similar the versions). if you don't care about this, which is perfectly fine, you might as well use dates for version numbers.) keyword tags have the advantage that they're instantly human-readable by anyone who has a basic command of English. if there is sufficient will in the community, a useful subset can be frozen in a binding spec, so that they are machine-readable as well. I'm not sure whether these keywords should be an appendix to the version number (like v2.3.4-:potentially-breaking), or whether the information they provide should be more extensive and included in a formalized preamble to the changelog (finally forcing people to at least take a glance at it ;) ). using the latter approach, the information provided could be (optionally) even more targeted, e.g. detailing explicitly which parts of the API are affected in a non-backwards compatible manner by the update. anyways, just a few ideas :) I am not primarily a coder, so there may be obvious drawbacks to this scheme that I can't see or which have already been discussed by the community on multiple occasions which have escaped my attention. in which case, please bear with me and excuse my lack of sophistication.","tags":"floss"},{"url":"http://dlukes.github.io/fill-par-in-airmail.html","title":"Filling (hardwrapping) paragraphs in Airmail with `par`","text":"tl;dr Jump directly to the proposed solution . Tested on OS X 10.9 (Mavericks). Back story Airmail is a great application -- being very happy with Gmail's in-browser UI, it's honestly the first e-mail desktop client that I ever felt even remotely tempted to use. It has: a sleek, functional design almost flawless integration with Gmail (except for categories -- but there's a not-too-hackish way to deal with those) a Markdown compose mode (yay!) -- and tons of other good stuff. Especially that last feature almost got me sold -- you see, I like my e-mail hardwrapped (what Emacs calls \"filling paragraphs\"), because most of the time, I view it on monitors that are too wide for soft line wrapping to achieve a comfortable text width. (By the way, Airmail's layout deals with this issue very elegantly, but I know I won't be using only Airmail. Plus there are the obvious netiquette issues -- lines \"should be\" wrapped at 72 characters etc.) In Gmail, I therefore use plain-text compose, which is fine for the purposes described above, but frustrating whenever you want to apply formatting (obviously, you can't -- it's plain text). I tried using the usual replacements for formatting like stars & co., and I don't know about your grandma, but mine certainly doesn't take *...* to mean emphasis. I thought the Markdown compose mode in Airmail would solve my problems -- I could apply formatting if and when I wanted (using the frankly more streamlined process of typing it in rather than fumbling around for the right button in the GUI) and fill my paragraphs, because I somehow automatically assumed there'd by a hard-wrap feature like in any decent editor (read: emacs or vi). Markdown is plain text after all, isn't it? Long story short, as of yet, there isn't . There isn't even one for the plain-text compose mode, as far as I'm aware. So I added my two cents to this feature request thread and went back to the Gmail in-browser UI. Solution But then I realized (it took me a while, I'm still very much an OS X newbie): in OS X, you can define custom actions with shortcuts 1 for any application using Automator Services these actions can be easily set to receive text selected in the application as input these actions can also involve shell scripts there already is a great (command line) program for filling paragraphs -- it's called par , and as much as I admire what Airmail's developers have achieved, it's unlikely that they'd come up with a more sophisticated hard-wrapping algorithm than par 's simply as a side project for Airmail (see the EXAMPLES section in man par ) With that in mind, you can have hard-wrapping in Markdown or plain-text Airmail compose at your fingertips in no time flat. If you don't have homebrew , start by installing that (or any other ports manager that will allow you to install par ; I'll assume homebrew below) by pasting ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" at a Terminal prompt. Then: install par with brew install par at a Terminal prompt open Automator (e.g. by typing \"Automator\" into Spotlight) and create a new Service select the applications for which you want the service to be active (for me, that's just Airmail) and tick the \"Output replaces selected text\" box drag the \"Run Shell Script\" action onto the workflow canvas, and as the shell script, paste in PARINIT = \"rTbgqR B=.,?_A_a Q=_s>|\" /usr/local/bin/par 79 the $PARINIT environment variable contains the default recommended settings for par (if you want to customize its behavior, you can -- good luck wrapping your head around par 's manpage, though) you should set the full path to the par executable, the shell spawned by the Service might not inherit your $PATH -- for par installed via homebrew , it's /usr/local/bin/par the parameter at the end is the max number of characters per line -- mailing list etiquette stipulates 72, I personally prefer the pythonesque 79, but it's your choice At this point, your service should look something like in the screenshot below: Save it, open Keyboard preferences (type \"Keyboard\" into Spotlight), navigate to Shortcuts ‚Üí Services ‚Üí Text and set a keyboard shortcut for your newly created Service, e.g. Cmd+Opt+P. Next time you compose an e-mail in Airmail, just select the entire text when you're done (Cmd+A), press Cmd+Opt+P, and voil√†! Your lines have been hardwrapped, your paragraphs filled :) (Same thing, I know.) If the shortcut doesn't appear to work 1 , try fiddling around with it, resetting it (maybe the one you've chosen conflicts with a pre-existing one?), restarting Airmail, logging out and back in, rebooting... The custom shortcut part is unfortunately the least reliable aspect of this whole setup. Automator is a great idea, I was pleasantly surprised by it when I started using OS X a few days back, but it could seriously use some bug-squashing. If you fail miserably at getting the shortcut to work, you can still access your fill paragraph service via the menu (select the text you want to hard-wrap, then navigate to Airmail ‚Üí Services ‚Üí <name of your fill paragraph service>). Clicking around in a GUI is tedious (though hey -- it's the Apple way after all, isn't it?), but it shouldn't be too much of a bother since you need to do it only once per e-mail. Bottom line : I am now officially completely sold on Airmail (even bought the released version instead of using the free beta) and look forward to the joy of using it! EDIT: In order to have the least trouble possible getting the shell script up and running as a Service , two rules of thumb: Leave it completely up to OS X where it stores the Service (.workflow) file. This will probably be in ~/Library/Services , and I learnt the hard way not to tinker with it -- if Services is a symlink instead of a real directory, the OS won't discover new Service files (though old ones will still be accessible). If the Service doesn't show up in the keyboard shortcuts menu after creation, try refreshing the service list with /System/Library/CoreServices/pbs -update . Those shortcuts are in fact quite buggy, especially those that you want to be global (not specific to a concrete app) -- at least on Mavericks (OS X 10.9). They tend to get disabled on a whim, especially if you tinker with them, and are a pain to get working again (login, logout, reboot -- anything goes). If anyone knows why, please let me know! ‚Ü©","tags":"os x tips"}]}